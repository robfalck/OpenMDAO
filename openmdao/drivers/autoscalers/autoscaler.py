from typing import TYPE_CHECKING

import numpy as np

from openmdao.core.constants import INF_BOUND
from openmdao.vectors.optimizer_vector import OptimizerVector

if TYPE_CHECKING:
    from openmdao.core.driver import Driver


class Autoscaler:
    """
    Base class of autoscalers that transform optimizer variables between model and optimizer spaces.

    Autoscalers apply scaling transformations to **continuous** design variables, constraints,
    and objectives, converting between physical (model) space and optimizer (scaled) space
    They also handle transformation of Lagrange multipliers from optimizer space back to physical
    (model) space and of jacobians from model space to optimizer space.

    Discrete design variables, constraints, and objectives are not scaled/unscaled by autoscalers.

    This autoscaler performs an affine scaling:
        x_scaled = (x_model' + adder) * scaler

    Here x_model' is assumed to be the value of the variable in the model but converted
    to the units specified for the variable of interest (design variable, constraint, or objective).
    If the units of the VOI are `None`, then x_model' is the same as the value of the associated
    output in the model.

    If implementing specialized autoscaler algorithms that derive from this class,
    the developer could choose to utilize or ignore the scaling applied with the optimizer
    variables via scaler/adder/ref0/ref. However, the unit conversions (which are part of
    the combined scaling) will need to be accounted for to maintain physical correctness.

    Subclasses must implement the following methods:

    Methods
    -------
    setup(driver)
        Initialize the autoscaler with driver metadata.
        Called once during driver setup.
    
    update(driver)
        Update any scaling parameters after the model has been executed.
        This can be used to assess the current values in the model,
        the current jacobian, etc.

    apply_vec_scaling(vec)
        Scale a vector from model space to optimizer space.
        Modifies vec in-place.

    apply_vec_unscaling(vec, name)
        Return an unscaled copy of a single variable from optimizer space to model space.
        Does not modify vec; returns a new array.

    apply_mult_unscaling(desvar_multipliers, con_multipliers)
        Unscale Lagrange multipliers from optimizer space to physical space.
        Modifies the input dictionaries in-place.
    
    apply_jac_scaling
        TBD

    This flexible interface allows autoscalers to choose any implementation strategy,
    from simple loops to complex matrix operations.

    """

    def setup(self, driver: 'Driver'):
        """
        Perform setup of autoscaler during final setup of the problem.

        Parameters
        ----------
        driver : Driver
            The driver associated with this autoscaler.
        """
        self._var_meta : dict[str, dict[str, dict]] = {
            'design_var': driver._designvars,
            'constraint': driver._cons,
            'objective': driver._objs
        }

        # Compute and cache combined scalers for all variables
        self._combined_scalers = {}

        self._has_scaling = False

        for voi_type in ['design_var', 'constraint', 'objective']:
            self._combined_scalers[voi_type] = {}
            for name, meta in self._var_meta[voi_type].items():
                scaler, adder = meta['total_scaler'], meta['total_adder']
                self._has_scaling = self._has_scaling \
                    or (scaler is not None) \
                    or (adder is not None)

        # Compute and cache scaled bounds vectors for design vars and constraints
        self._scaled_lower = {}
        self._scaled_upper = {}
        self._scaled_equals = {}
        for voi_type in ['design_var', 'constraint']:
            self._scaled_lower[voi_type], \
                self._scaled_upper[voi_type], \
                self._scaled_equals[voi_type] = self._compute_scaled_bounds(voi_type)

    @property
    def has_scaling(self):
        return self._has_scaling

    def update(self, driver: 'Driver'):
        """
        Perform any last minute setup of the autoscaler at the start of the driver's execution.

        This method is called during driver.run when the model has been executed. It can be used
        to configure the scaling based on values in the model, the current model jacobian, etc.

        Parameters
        ----------
        driver : Driver
            The driver that is running this autoscaler.
        """
        pass

    def _scale_bound(self, val, adder, scaler, size, is_lower):
        """
        Apply scaling to a single bound value, preserving infinite bounds.

        Parameters
        ----------
        val : float or ndarray
            Bound value in physical (model) units.
        adder : float, ndarray, or None
            Combined additive scaling factor.
        scaler : float, ndarray, or None
            Combined multiplicative scaling factor.
        size : int
            Number of elements for the variable.
        is_lower : bool
            True if this is a lower bound; controls which infinity sentinel is used.

        Returns
        -------
        ndarray
            Scaled bound array of length `size`.
        """
        if np.isscalar(val):
            val_arr = np.full(size, val, dtype=float)
        else:
            if val is None:
                if is_lower:
                    val = -INF_BOUND
                else:
                    val = INF_BOUND

            val_arr = np.asarray(val, dtype=float)
                
            if val_arr.size != size:
                val_arr = np.broadcast_to(val_arr, (size,)).copy()
            else:
                val_arr = val_arr.copy()

        # Identify unbounded (infinite) elements before scaling
        inf_mask = (val_arr <= -INF_BOUND) if is_lower else (val_arr >= INF_BOUND)

        if not inf_mask.all():
            finite = ~inf_mask
            if adder is not None:
                val_arr[finite] += adder if np.isscalar(adder) else np.asarray(adder)[finite]
            if scaler is not None:
                val_arr[finite] *= scaler if np.isscalar(scaler) else np.asarray(scaler)[finite]

        # Restore sentinel for unbounded elements (scaling may have perturbed them)
        val_arr[inf_mask] = -INF_BOUND if is_lower else INF_BOUND

        val_arr = val_arr.ravel()

        return val_arr

    def _compute_scaled_bounds(self, voi_type):
        """
        Compute scaled bounds OptimizerVectors for design variables or constraints.

        Called once during setup() to build and cache scaled bounds. Bounds are read
        from metadata in physical (model) units and transformed to driver (optimizer)
        units using the combined scaler and adder for each variable.

        Parameters
        ----------
        voi_type : str
            One of 'design_var' or 'constraint'.

        Returns
        -------
        lower : OptimizerVector
            Scaled lower bounds. Unbounded entries contain -INF_BOUND.
        upper : OptimizerVector
            Scaled upper bounds. Unbounded entries contain INF_BOUND.
        equals : OptimizerVector or None
            Scaled equality values. Non-equality constraint entries contain np.nan.
            None when voi_type='design_var'.
        """
        vecmeta = {}
        total_size = 0

        for name, meta in self._var_meta[voi_type].items():
            if meta.get('discrete', False):
                continue
            size = meta.get('global_size', meta.get('size', 0)) \
                if meta.get('distributed', False) else meta.get('size', 0)
            vecmeta[name] = {
                'start_idx': total_size,
                'end_idx': total_size + size,
                'size': size,
            }
            total_size += size

        lower_data = np.empty(total_size)
        upper_data = np.empty(total_size)
        equals_data = np.full(total_size, np.nan) if voi_type == 'constraint' else None

        for name, vmeta in vecmeta.items():
            meta = self._var_meta[voi_type][name]
            if meta.get('discrete', False):
                continue
            size = vmeta['size']
            start = vmeta['start_idx']
            end = vmeta['end_idx']
            adder = self._var_meta[voi_type][name]['total_adder']
            scaler = self._var_meta[voi_type][name]['total_scaler']

            lower_data[start:end] = self._scale_bound(
                meta.get('lower', -INF_BOUND), adder, scaler, size, is_lower=True)
            upper_data[start:end] = self._scale_bound(
                meta.get('upper', INF_BOUND), adder, scaler, size, is_lower=False)

            if voi_type == 'constraint':
                eq = meta.get('equals')
                if eq is not None:
                    equals_data[start:end] = self._scale_bound(
                        eq, adder, scaler, size, is_lower=False)

        lower_vec = OptimizerVector(voi_type, lower_data, vecmeta)
        upper_vec = OptimizerVector(voi_type, upper_data, vecmeta)
        equals_vec = OptimizerVector(voi_type, equals_data, vecmeta) \
            if voi_type == 'constraint' else None

        return lower_vec, upper_vec, equals_vec

    def get_bounds_scaling(self, voi_type):
        """
        Return pre-computed scaled bounds vectors for the given variable type.

        Returns bounds cached during setup() in driver (optimizer) units. The original
        metadata bounds remain in physical (model) units and are not modified.

        Infinite bounds (abs value >= INF_BOUND in model space) are returned as ±INF_BOUND.

        If scalers change after setup (e.g. in an adaptive autoscaler subclass), call
        _compute_scaled_bounds() again for each affected voi_type to refresh the cache.

        Parameters
        ----------
        voi_type : str
            One of 'design_var' or 'constraint'.

        Returns
        -------
        lower : OptimizerVector
            Scaled lower bounds. Unbounded entries contain -INF_BOUND.
        upper : OptimizerVector
            Scaled upper bounds. Unbounded entries contain INF_BOUND.
        equals : OptimizerVector or None
            Scaled equality values. Non-equality constraint entries contain np.nan as
            a sentinel. None when voi_type='design_var'.
        """
        return (self._scaled_lower[voi_type],
                self._scaled_upper[voi_type],
                self._scaled_equals[voi_type])

    def refresh_bounds_cache(self, voi_type):
        """
        Re-compute and cache scaled bounds for the given variable type.

        Call this after modifying the constraint or design variable metadata post-setup,
        for example when a driver adds entries to _cons dynamically during _setup_driver.

        Parameters
        ----------
        voi_type : str
            One of 'design_var' or 'constraint'.
        """
        (self._scaled_lower[voi_type],
         self._scaled_upper[voi_type],
         self._scaled_equals[voi_type]) = self._compute_scaled_bounds(voi_type)

    def apply_vec_unscaling(self, vec: 'OptimizerVector'):
        """
        Unscale the optmization variables from the optimizer space to the model space, in place.

        This method will generally be applied to each design variable at every iteration.

        Parameters
        ----------
        vec : OptimizerVector
            A vector of the scaled optimization variables.

        Returns
        -------
        OptimizerVector
            The unscaled optimization vector.
        """
        if not vec.driver_scaling:
            return vec
        
        for name in vec:
            # Use cached combined scaler/adder - includes both unit conversion and user scaling
            scaler = self._var_meta[vec.voi_type][name]['total_scaler']
            adder = self._var_meta[vec.voi_type][name]['total_adder']

            # Unscale: x_model = x_optimizer / scaler - adder
            if scaler is not None:
                vec[name] /= scaler
            if adder is not None:
                vec[name] -= adder
        vec._driver_scaling = False

        return vec
    
    def apply_vec_scaling(self, vec: 'OptimizerVector'):
        """
        Scale the vector from the model space to the optimizer space.

        Scaling is applied to the optimizer vector in-place.
        """
        if vec.driver_scaling:
            return vec
        for name in vec:
            # Use cached combined scaler/adder - includes both unit conversion and user scaling
            scaler = self._var_meta[vec.voi_type][name]['total_scaler']
            adder = self._var_meta[vec.voi_type][name]['total_adder']

            # Scale: x_optimizer = (x_model + adder) * scaler
            if adder is not None:
                vec[name] += adder
            if scaler is not None:
                vec[name] *= scaler
        vec._driver_scaling = True

    def apply_mult_unscaling(self, desvar_multipliers, con_multipliers):
        """
        Unscale the Lagrange multipliers from optimizer space to model space.
        
        This method transforms Lagrange multipliers of active constraints (including
        active design variable bounds) from the scaled optimization space back to 
        physical (model) space.
        
        At optimality, we assume the KKT stationarity condition holds:
        
            ∇ₓf(x) + ∇ₓg(x)^T λ = 0
        
        where:
            - ∇ₓf is the gradient of the objective
            - ∇ₓg(x)^T is the Jacobian of all active constraints (each row is ∇ₓg_i^T)
            - λ is the vector of Lagrange multipliers (in optimizer-scaled)
        
        The constraint vector g(x) includes:
            - Active design variables (on their bounds, to within some tolerance)
            - Equality constraints (always active)
            - Active inequality constraints (on their bounds, to within some tolerance)
        
        Scaling Transformations
        -----------------------
        Define scaling transformations that map from unscaled (physical) space to
        scaled (optimizer) space:
        
            x_scaled = T_x(x)         (design variables)
            g_scaled = T_g(g(x))      (constraints)
            f_scaled = T_f(f(x))      (objective)
        
        Applying the chain rule to the scaled stationarity condition:
        
            ∇ₓ_scaled f_scaled + ∇ₓ_scaled g_scaled^T λ_scaled = 0
        
        The gradients in scaled space are:
        
            ∇ₓ_scaled f_scaled = (dTf/df) * ∇ₓf * (dTₓ/dx)^(-1)
            ∇ₓ_scaled g_scaled = (dTg/dg) * ∇ₓg * (dTₓ/dx)^(-1)
        
        Substituting into the scaled stationarity condition and multiplying by (dTₓ/dx)^T:
        
            (dTf/df) * ∇ₓf + (dTg/dg) * ∇ₓg^T * λ_scaled = 0
        
        Dividing by (dTf/df) and comparing with the unscaled condition ∇ₓf + ∇ₓg^T λ = 0:
        
            λ = (dTg/dg) / (dTf/df) * λ_scaled
        
        For the Default autoscaler, we have
        
            T_x(x) = (x + adder_x) * scaler_x
            T_g(g) = (g + adder_g) * scaler_g
            T_f(f) = (f + adder_f) * scaler_f
        
        The derivatives are:
        
            dT_x/dx = scaler_x
            dT_g/dg = scaler_g
            dT_f/df = scaler_f
        
        Therefore:
        
            λ_constraint = (scaler_g / scaler_f) * λ_constraint_scaled
    
            λ_bound = (scaler_x / scaler_f) * λ_bound_scaled
        
        The adder terms do not appear in the multiplier transformation
        because they are constant offsets that vanish under differentiation.
        
        Parameters
        ----------
        desvar_multipliers : dict[str, np.ndarray]
            A dict of optimizer-scaled Lagrange multipliers keyed by each active design variable.
        con_multipliers : dict[str, np.ndarray]
            A dict of optimizer-scaled Lagrange multipliers keyed by each active constraint.
        
        Returns
        -------
        desvar_multipliers : dict[str, np.ndarray]
            A reference to the desvar_multipliers given on input. The values of the multipliers
            were unscaled in-place.
        con_multipliers : dict[str, np.ndarray]
            A reference to the con_multipliers given on input. The values of the multipliers
            were unscaled in-place.
        """
        if not self._has_scaling:
            return desvar_multipliers, con_multipliers

        # Get the objective scaler from cached combined scalers
        obj_meta = self._var_meta['objective']
        obj_name = list(obj_meta.keys())[0]
        obj_scaler = obj_meta[obj_name]['total_scaler'] or 1.0

        if desvar_multipliers:
            for name, mult in desvar_multipliers.items():
                # Get the design variable scaler from cached combined scalers
                scaler = self._var_meta['design_var'][name]['total_scaler'] or 1.0
                mult *= scaler / obj_scaler

        if con_multipliers:
            for name, mult in con_multipliers.items():
                # Get the constraint scaler from cached combined scalers
                scaler = self._var_meta['constraint'][name]['total_scaler'] or 1.0
                mult *= scaler / obj_scaler

        return desvar_multipliers, con_multipliers

    def apply_jac_scaling(self, jac_dict):
        """
        Scale a Jacobian dictionary from model space to optimizer space.

        Applies the scaling transformation to convert a Jacobian computed in the model's
        coordinate system to the optimizer's scaled coordinate system.

        The scaling transformation for the Jacobian is:
            J_scaled = (dT_f/df) * J_model * (dT_x/dx)^-1
                     = scaler_f * J_model / scaler_x

        This accounts for how the scaling transformations affect the derivatives.

        Parameters
        ----------
        jac_dict : dict
            Dictionary of Jacobian blocks. Can be either:
            - Nested dict where jac_dict[output_name][input_name] = array
            - Flat dict where jac_dict[(output_name, input_name)] = array

        Notes
        -----
        The method modifies the Jacobian dictionary in-place, scaling each partial
        derivative block according to the output and input scalers.

        When a scaler is None (identity transformation), it's treated as 1.0 for
        multiplication and division.
        """
        if not self._has_scaling:
            return

        for key, jac_block in jac_dict.items():
            # Handle both nested dict and flat dict formats
            if isinstance(key, tuple):
                # Flat dict format: key is (output_name, input_name)
                out_name, in_name = key
            else:
                # Nested dict format: key is output_name, need to iterate inner dicts
                out_name = key
                for in_name, block in jac_block.items():
                    # Determine output scaler
                    if out_name in self._var_meta['objective']:
                        out_scaler = self._var_meta['objective'][out_name]['total_scaler']
                    elif out_name in self._var_meta['constraint']:
                        out_scaler = self._var_meta['constraint'][out_name]['total_scaler']
                    else:
                        # Unknown output, skip scaling this row
                        continue

                    # Determine input scaler
                    if in_name in self._var_meta['design_var']:
                        in_scaler = self._var_meta['design_var'][in_name]['total_scaler']
                    else:
                        # Unknown input, skip scaling this entry
                        continue

                    # Scale the Jacobian block in-place: J_scaled = J_model * out_scaler / in_scaler
                    # Use in-place operations to preserve view relationship with underlying array
                    if out_scaler is not None:
                        block[...] = (out_scaler * block.T).T
                    if in_scaler is not None:
                        block *= 1.0 / in_scaler
                continue

            # Handle flat dict format (key is a tuple)
            # Determine output scaler
            if out_name in self._var_meta['objective']:
                out_scaler = self._var_meta['objective'][out_name]['total_scaler']
            elif out_name in self._var_meta['constraint']:
                out_scaler = self._var_meta['constraint'][out_name]['total_scaler']
            else:
                # Unknown output, skip scaling this entry
                continue

            # Determine input scaler
            if in_name in self._var_meta['design_var']:
                in_scaler = self._var_meta['design_var'][in_name]['total_scaler']
            else:
                # Unknown input, skip scaling this entry
                continue

            # Scale the Jacobian block in-place: J_scaled = J_model * out_scaler / in_scaler
            # Must use in-place operations to preserve view relationship with underlying array
            if out_scaler is not None:
                jac_block[...] = (out_scaler * jac_block.T).T
            if in_scaler is not None:
                jac_block *= 1.0 / in_scaler
