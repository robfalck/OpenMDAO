{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Post-Optimality Sensitivities of a Constrained Optimization Problem\n",
    "\n",
    "Lets consider a problem such that we have an active bound and an active inequality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta_0,\\, \\theta_1} \\quad & f(\\theta_0, \\theta_1; \\mathbf{p}) = (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\\\\n",
    "\\text{where} \\quad \\mathbf{p} &= \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "\\text{bounds:} \\quad \\theta_0 &\\le 6 \\\\\n",
    "\\text{equality constraints:} \\quad \\theta_0 + \\theta_1 &= 0\n",
    "\\end{align*}\n",
    "\n",
    "We want to know the sensitivities of the optimization outputs with respect to the optimization inputs.\n",
    "\n",
    "In this context, consider the outputs of the optimization to be the objective and any other functions of interest, $f$.\n",
    "\n",
    "The design variables $\\theta$ and Lagrange multipliers $\\lambda$ are effectively the implicit outputs of the optimization.\n",
    "\n",
    "The _inputs_ to the optimization process consists of:\n",
    "- any independent parameters, $\\bar{p}$\n",
    "- the bounding values of any **active** design variables, $\\bar{b}_{\\theta}$\n",
    "- the bounding values of any **active** constraints, $\\bar{b}o_{g}$\n",
    "\n",
    "In our case we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{p} &= \\begin{bmatrix} p_0 \\\\ p_1 \\\\ p_2 \\end{bmatrix} \\\\\n",
    "    \\bar{b}_{\\theta} &= \\begin{bmatrix} \\theta_0^{ub} \\end{bmatrix} \\\\\n",
    "    \\bar{b}_g &= \\begin{bmatrix} g_0^{eq} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<!-- If active, we can treat the bound on $\\theta_0$ as just another equality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{\\mathcal{G}}(\\bar{\\theta}, \\bar{p}) &= \\begin{bmatrix}\n",
    "                                   \\theta_0 + \\theta_1 \\\\\n",
    "                                   \\theta_0 - p_3\n",
    "                                \\end{bmatrix} = \\bar 0\n",
    "\\end{align*}\n",
    "\n",
    "**How will my system design ($\\bar{\\theta}^*$) respond to changes in my assumptions and system inputs ($\\bar{p}$)?** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Derivatives Equation\n",
    "\n",
    "The UDE is:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right] \\left[ \\frac{d u}{d \\mathcal{R}} \\right]\n",
    "  &=\n",
    "  \\left[ I \\right]\n",
    "  =\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right]^T \\left[ \\frac{d u}{d \\mathcal{R}} \\right]^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, the residuals are the primal and dual residuals of the optimization process, given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the UDE to solving post-optimality sensitivities\n",
    "\n",
    "In our case, the unknowns vector consists of\n",
    "- the optimization parameters ($\\bar{p}$)\n",
    "- the bounding values of any active design variables ($\\bar{b}_{\\theta}$)\n",
    "- the bounding values of any active constraints ($\\bar{b}_{g}$)\n",
    "- the design variables of the optimization ($\\bar{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active design variables ($\\bar{\\lambda}_{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active constraints ($\\bar{\\lambda}_{g}$)\n",
    "- the objective value **as well as** any other outputs for which we want the sensitivities ($f$)\n",
    "\n",
    "The total size of the unknowns vector is $N_p + N_{\\theta} + 2N_{\\lambda \\theta} + 2N_{\\lambda g} + N_{f}$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{u} &=\n",
    "  \\begin{bmatrix}\n",
    "    \\bar{p} \\\\\n",
    "    \\bar{b}_{\\theta} \\\\\n",
    "    \\bar{b}_{g} \\\\\n",
    "    \\bar{\\theta} \\\\\n",
    "    \\bar{\\lambda_{\\theta}} \\\\\n",
    "    \\bar{\\lambda_{g}} \\\\\n",
    "    \\bar{f}\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Under the UDE, the corresponding residual equations for these unknowns are\n",
    "- the implicit form of the parameter values\n",
    "- the implicit form of the active design variable values\n",
    "- the implicit form of the active constraint values\n",
    "- the stationarity condition\n",
    "- the active design variable residuals\n",
    "- the active constraint residuals\n",
    "- the implicit form of the explicit calculations of $f$\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathcal{R}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathcal{R}}_p \\\\\n",
    "\\bar{\\mathcal{R}}_{b \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{b g} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda g} \\\\\n",
    "\\bar{\\mathcal{R}}_{f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\check{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{\\theta} - \\check{b}_{\\theta} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{g} - \\check{b}_g \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ \\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_g + \\nabla_{\\bar{\\theta}} \\check{\\theta}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda \\theta} - \\left[ \\check{\\theta}_{\\mathcal{A}} \\left( \\bar{\\theta} \\right) - \\bar{b}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda g} - \\left[ \\check{g}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_g \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{f} - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\check{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{\\theta} - \\check{b}_{\\theta} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{g} - \\check{b}_g \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\nabla_{\\theta} \\check{\\mathcal{L}} \\left( \\bar{\\theta}, \\bar{p} \\right) \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda \\theta} - \\left[ \\check{\\theta}_{\\mathcal{A}} \\left( \\bar{\\theta} \\right) - \\bar{b}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda g} - \\left[ \\check{g}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_g \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{f} - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\bar 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the total derivatives that we seek ($\\frac{d f^*}{d \\bar{p}}$ and $\\frac{d \\bar{\\theta}^*}{d \\bar{p}}$), we need $\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}$.\n",
    "\n",
    "The optimizer has served as the nonlinear solver in this case which has computed the values in the unknowns vector: $\\bar{\\theta}$, $\\bar{\\lambda}$, and $\\bar{f}$ such that the residuals are satisfied.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_p}}{\\partial \\bar{p}} & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} \\theta}}}{\\partial \\bar{b}_{\\theta}} & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} g}}}{\\partial \\bar{b}_{g}} & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_{\\theta}}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_g}} & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{b}_{\\theta}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{\\theta}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{p}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{b}_g} & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{\\theta}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}} & 0 & 0 & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}} & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}} & 0 & \\left[ I_{bg} \\right] & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{f}}{\\partial \\bar{p}} & 0 & 0 & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This nomenclature can be a bit confusing.\n",
    "\n",
    "**The _partial_ derivatives of the post-optimality residuals are the _total_ derivatives of the analysis.**\n",
    "\n",
    "In this case of the stationarity residuals $\\mathcal{R}_{\\bar{\\theta}}$, which already include _total_ derivatives of the analysis for the objective and constraint gradients, second derivatives are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding total sensitivities derivaties which we need to solve for are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\bar{u}}{d \\bar{\\mathcal{R}}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\left[ I_p \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  \\left[ I_{b\\theta} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  0 &\n",
    "  \\left[ I_{bg} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we can solve them with four linear solves of the forward system, or three solves of the reverse system.\n",
    "\n",
    "TODO: Need to explain how du/dRf becomes du/df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDE for this case, in forward form, is\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}} & 0 & 0 & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}} & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}} & 0 & \\left[ I_{bg} \\right] & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{f}}{\\partial \\bar{p}} & 0 & 0 & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  \\left[ I_p \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  \\left[ I_{b\\theta} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  0 &\n",
    "  \\left[ I_{bg} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_{\\theta} \\right] & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & \\left[ I_{\\lambda \\theta} \\right] & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & \\left[ I_{\\lambda g} \\right] & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "The linear solve of this system can proceed one column of $\\left[ \\frac{d \\bar{u}}{d \\mathcal{R}} \\right]$ at a time (the forward solve).\n",
    "\n",
    "In this case we would need one solve for each column ($N_p$ + $N_{b\\theta}$ + $N_b{g}$).\n",
    "\n",
    "In our example optimization with three parameters, an active bound and an active equality constraint, this would be five solves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could transpose this system and solve it in reverse mode. Solving for one column at a time in the transposed system would mean solving once for each design variable and each output of interest. In our example optimziation this would be three solves.\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                    & 0                    & -\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}}^T & 0                    & -\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check{f}}{\\partial \\bar{p}}^T \\\\[1.1ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right] & 0                    & 0                                                                & \\left[ I_{b\\theta} \\right] & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & \\left[ I_{bg} \\right]      & 0                                                                & 0                    & \\left[ I_{bg} \\right]                                      & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}}                        & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}  & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}}                & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T               & 0                    & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T                    & 0                    & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                         & 0                       & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}}^T                    & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}}^T              & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}}^T                 & \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}}^T \\\\[1.1ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right]      & 0                       & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}}^T           & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}}^T     & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}}^T        & \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & \\left[ I_{bg} \\right]         & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}}^T                  & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}}^T            & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}}^T                & \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}}^T          & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}}^T    & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}}^T       & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T  & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T       & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T    & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T       & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & 0                                                                & 0                                                       & 0                                                         & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_{\\theta} \\right] & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & \\left[ I_{\\lambda \\theta} \\right] & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & \\left[ I_{\\lambda g} \\right] & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The All-JVP Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "with JVPs - Forward Mode Sensitivity Computation\n",
      "============================================================\n",
      "\n",
      "Solving for sensitivities w.r.t. p[0]...\n",
      "  df/dp[0] = -6.000000\n",
      "  dθ₀/dp[0] = +0.000000\n",
      "  dθ₁/dp[0] = -0.000000\n",
      "\n",
      "Solving for sensitivities w.r.t. p[1]...\n",
      "  df/dp[1] = -4.000000\n",
      "  dθ₀/dp[1] = -0.000000\n",
      "  dθ₁/dp[1] = +0.000000\n",
      "\n",
      "Solving for sensitivities w.r.t. p[2]...\n",
      "  df/dp[2] = -1.000000\n",
      "  dθ₀/dp[2] = +0.000000\n",
      "  dθ₁/dp[2] = +0.000000\n",
      "\n",
      "Solving for sensitivities w.r.t. b_theta...\n",
      "  df/db_θ₀ = -2.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "\n",
      "Solving for sensitivities w.r.t. b_g...\n",
      "  df/db_g₀ = +2.000000\n",
      "  dθ₀/db_g₀ = +0.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "dθ₁/db_θ₀:\n",
      "  UDE:    -1.000000\n",
      "  FD:     -1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results\n",
      "============================================================\n",
      "\n",
      "Note: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\n",
      "- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\n",
      "- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\n",
      "- The objective is most sensitive to p₀ (df/dp₀ = -6)\n",
      "\n",
      "Forward mode approach:\n",
      "- Solves ∂R/∂u @ (du/dp) = -∂R/∂p directly\n",
      "- Uses JVPs for all matrix-vector operations\n",
      "- Requires one solve per parameter (less efficient for many parameters)\n",
      "- More intuitive: directly computes how variables change with parameters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with Jacobian-Vector Products (JVPs) - forward mode sensitivity computation.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    Uses Jacobian-Vector Products (JVPs) for forward mode sensitivity computation.\n",
    "    Solves: ∂R/∂u @ (du/dp) = -∂R/∂p\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_f\n",
    "\n",
    "    # Define Lagrangian and its gradient\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Lagrangian function\"\"\"\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    def lagrangian_grad(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Gradient of Lagrangian w.r.t. Θ\"\"\"\n",
    "        return jax.grad(lagrangian, argnums=0)(Θ, p, λ_theta, λ_g)\n",
    "\n",
    "    # Function to compute Hessian-vector product using finite differences\n",
    "    def hessian_vector_product(v_theta, h=1e-8):\n",
    "        \"\"\"\n",
    "        Compute H @ v where H is the Hessian of the Lagrangian w.r.t. Θ\n",
    "        Uses finite differences: H @ v ≈ (∇L(Θ + h*v) - ∇L(Θ - h*v)) / (2*h)\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Compute gradients at perturbed points\n",
    "        grad_plus = lagrangian_grad(Θ_opt + h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "        grad_minus = lagrangian_grad(Θ_opt - h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "\n",
    "        # Finite difference approximation of Hessian-vector product\n",
    "        hvp = (grad_plus - grad_minus) / (2 * h)\n",
    "\n",
    "        return np.array(hvp)\n",
    "\n",
    "    # JVP functions for matrix-vector products\n",
    "    def jvp_d2L_dtheta_dp(v_p):\n",
    "        \"\"\"\n",
    "        Compute [∂²L/∂θ∂p] @ v_p using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_p):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_p_jax = jnp.array(v_p)\n",
    "\n",
    "        # JVP of the gradient function w.r.t. p\n",
    "        _, jvp_result = jax.jvp(lambda p: lagrangian_grad(Θ_opt, p, λ_theta_opt, λ_g_opt),\n",
    "                                (p_opt,), (v_p_jax,))\n",
    "\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dg_active_dp(v_p):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂p] @ v_p using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_p):\n",
    "            return np.zeros(n_lambda_g)\n",
    "\n",
    "        v_p_jax = jnp.array(v_p)\n",
    "\n",
    "        # JVP of g_active w.r.t. p\n",
    "        _, jvp_result = jax.jvp(lambda p: g_active(Θ_opt, p), (p_opt,), (v_p_jax,))\n",
    "\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_df_dp(v_p):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂p] @ v_p using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_p):\n",
    "            return np.zeros(n_f)\n",
    "\n",
    "        v_p_jax = jnp.array(v_p)\n",
    "\n",
    "        # JVP of f w.r.t. p\n",
    "        _, jvp_result = jax.jvp(lambda p: f(Θ_opt, p), (p_opt,), (v_p_jax,))\n",
    "\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dtheta_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(Θ_active, (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dg_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_g)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(lambda theta: g_active(theta, p_opt), (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_df_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_f)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(lambda theta: f(theta, p_opt), (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def matvec_forward(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u] @ v\n",
    "        This implements the forward UDE Jacobian matrix.\n",
    "        Uses JVPs for all matrix-vector operations.\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_f = v[idx:idx+n_f]\n",
    "\n",
    "        # print(f\"Input vector norm: {np.linalg.norm(v):.6f}\")\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Parameter residual equation\n",
    "        # R_p = p - (actual parameters) = 0\n",
    "        # ∂R_p/∂u @ v = I @ v_p = v_p\n",
    "        result[:n_p] = v_p\n",
    "\n",
    "        # Block 2: Active bound residual equation\n",
    "        # R_btheta = b_theta - (actual bounds) = 0\n",
    "        # ∂R_btheta/∂u @ v = I @ v_btheta = v_btheta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta\n",
    "\n",
    "        # Block 3: Active constraint bound residual equation\n",
    "        # R_bg = b_g - (actual constraint bounds) = 0\n",
    "        # ∂R_bg/∂u @ v = I @ v_bg = v_bg\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg\n",
    "\n",
    "        # Block 4: Optimality condition (stationarity)\n",
    "        # R_theta = ∇_θ L = 0\n",
    "        # ∂R_theta/∂u @ v = ∂²L/∂θ∂p @ v_p + H @ v_theta + [∂Θ_active/∂θ]^T @ v_lambda_theta + [∂g_active/∂θ]^T @ v_lambda_g\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "        result_theta += jvp_d2L_dtheta_dp(v_p)\n",
    "        if np.any(v_theta):\n",
    "            # print('using hessian-vector product!')\n",
    "            result_theta += hessian_vector_product(v_theta)\n",
    "        # For transpose operations, we use the fact that [A]^T @ v = A^T @ v\n",
    "        # But we need to be careful about the shapes\n",
    "        if np.any(v_lambda_theta):\n",
    "            # [∂Θ_active/∂θ]^T is (n_theta x n_lambda_theta), so we need to handle this properly\n",
    "            # This is equivalent to computing the VJP, but we'll use the transpose relationship\n",
    "            _, vjp_fun = jax.vjp(Θ_active, Θ_opt)\n",
    "            result_theta += np.array(vjp_fun(jnp.array(v_lambda_theta))[0])\n",
    "        if np.any(v_lambda_g):\n",
    "            # [∂g_active/∂θ]^T @ v_lambda_g\n",
    "            _, vjp_fun = jax.vjp(lambda theta: g_active(theta, p_opt), Θ_opt)\n",
    "            result_theta += np.array(vjp_fun(jnp.array(v_lambda_g))[0])\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Active bound constraints\n",
    "        # R_lambda_theta = Θ_active(θ) - b_theta = 0\n",
    "        # ∂R_lambda_theta/∂u @ v = [∂Θ_active/∂θ] @ v_theta - I @ v_btheta\n",
    "        idx += n_theta\n",
    "        result[idx:idx+n_lambda_theta] = jvp_dtheta_active_dtheta(v_theta) - v_btheta\n",
    "\n",
    "        # Block 6: Active equality constraints\n",
    "        # R_lambda_g = g_active(θ, p) - b_g = 0\n",
    "        # ∂R_lambda_g/∂u @ v = [∂g_active/∂θ] @ v_theta + [∂g_active/∂p] @ v_p - I @ v_bg\n",
    "        idx += n_lambda_theta\n",
    "        result[idx:idx+n_lambda_g] = (jvp_dg_active_dtheta(v_theta) +\n",
    "                                      jvp_dg_active_dp(v_p) - v_bg)\n",
    "\n",
    "        # Block 7: Objective function\n",
    "        # R_f = f(θ, p) - f_target = 0  (where f_target is just a dummy)\n",
    "        # ∂R_f/∂u @ v = [∂f/∂θ] @ v_theta + [∂f/∂p] @ v_p - I @ v_f\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_f] = (jvp_df_dtheta(v_theta) +\n",
    "                               jvp_df_dp(v_p) - v_f)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator for the forward system\n",
    "    A_forward = LinearOperator((total_size, total_size), matvec=matvec_forward)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # For each parameter, solve the forward system\n",
    "    for i in range(n_p):\n",
    "        # Create RHS vector: -∂R/∂p_i\n",
    "        rhs = np.zeros(total_size)\n",
    "\n",
    "        # Only the constraint and objective residuals depend on parameters\n",
    "        # Block 4 (theta): -∂²L/∂θ∂p_i\n",
    "        rhs[n_p + n_btheta + n_bg:n_p + n_btheta + n_bg + n_theta] = -jvp_d2L_dtheta_dp(np.eye(n_p)[i])\n",
    "\n",
    "        # Block 6 (lambda_g): -∂g_active/∂p_i\n",
    "        rhs[n_p + n_btheta + n_bg + n_theta + n_lambda_theta:\n",
    "            n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g] = -jvp_dg_active_dp(np.eye(n_p)[i])\n",
    "\n",
    "        # Block 7 (f): -∂f/∂p_i\n",
    "        rhs[n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g:\n",
    "            n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_f] = -jvp_df_dp(np.eye(n_p)[i])\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities w.r.t. p[{i}]...\")\n",
    "        solution, info = gmres(A_forward, rhs, rtol=1e-10, maxiter=1000)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities from the solution\n",
    "            dp_dp = solution[:n_p]\n",
    "            dbtheta_dp = solution[n_p:n_p+n_btheta]\n",
    "            dbg_dp = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "            dtheta_dp = solution[n_p+n_btheta+n_bg:n_p+n_btheta+n_bg+n_theta]\n",
    "            dlambda_theta_dp = solution[n_p+n_btheta+n_bg+n_theta:n_p+n_btheta+n_bg+n_theta+n_lambda_theta]\n",
    "            dlambda_g_dp = solution[n_p+n_btheta+n_bg+n_theta+n_lambda_theta:n_p+n_btheta+n_bg+n_theta+n_lambda_theta+n_lambda_g]\n",
    "            df_dp = solution[n_p+n_btheta+n_bg+n_theta+n_lambda_theta+n_lambda_g:]\n",
    "\n",
    "            print(f\"  df/dp[{i}] = {df_dp[0]:+.6f}\")\n",
    "            print(f\"  dθ₀/dp[{i}] = {dtheta_dp[0]:+.6f}\")\n",
    "            print(f\"  dθ₁/dp[{i}] = {dtheta_dp[1]:+.6f}\")\n",
    "\n",
    "            # Store results\n",
    "            if i == 0:\n",
    "                sensitivities['f'] = {'wrt_p': np.zeros(n_p), 'wrt_btheta': np.zeros(n_btheta), 'wrt_bg': np.zeros(n_bg)}\n",
    "                sensitivities['θ₀'] = {'wrt_p': np.zeros(n_p), 'wrt_btheta': np.zeros(n_btheta), 'wrt_bg': np.zeros(n_bg)}\n",
    "                sensitivities['θ₁'] = {'wrt_p': np.zeros(n_p), 'wrt_btheta': np.zeros(n_btheta), 'wrt_bg': np.zeros(n_bg)}\n",
    "\n",
    "            sensitivities['f']['wrt_p'][i] = df_dp[0]\n",
    "            sensitivities['θ₀']['wrt_p'][i] = dtheta_dp[0]\n",
    "            sensitivities['θ₁']['wrt_p'][i] = dtheta_dp[1]\n",
    "\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    # Solve for sensitivities w.r.t. b_theta\n",
    "    print(f\"\\nSolving for sensitivities w.r.t. b_theta...\")\n",
    "    rhs = np.zeros(total_size)\n",
    "    # Block 5 (lambda_theta): -(-I) = +I\n",
    "    rhs[n_p + n_btheta + n_bg + n_theta:n_p + n_btheta + n_bg + n_theta + n_lambda_theta] = 1.0\n",
    "\n",
    "    solution, info = gmres(A_forward, rhs, rtol=1e-10, maxiter=1000)\n",
    "    if info == 0:\n",
    "        dtheta_dbtheta = solution[n_p+n_btheta+n_bg:n_p+n_btheta+n_bg+n_theta]\n",
    "        df_dbtheta = solution[n_p+n_btheta+n_bg+n_theta+n_lambda_theta+n_lambda_g:]\n",
    "\n",
    "        sensitivities['f']['wrt_btheta'][0] = df_dbtheta[0]\n",
    "        sensitivities['θ₀']['wrt_btheta'][0] = dtheta_dbtheta[0]\n",
    "        sensitivities['θ₁']['wrt_btheta'][0] = dtheta_dbtheta[1]\n",
    "\n",
    "        print(f\"  df/db_θ₀ = {df_dbtheta[0]:+.6f}\")\n",
    "        print(f\"  dθ₀/db_θ₀ = {dtheta_dbtheta[0]:+.6f}\")\n",
    "        print(f\"  dθ₁/db_θ₀ = {dtheta_dbtheta[1]:+.6f}\")\n",
    "\n",
    "    # Solve for sensitivities w.r.t. b_g\n",
    "    print(f\"\\nSolving for sensitivities w.r.t. b_g...\")\n",
    "    rhs = np.zeros(total_size)\n",
    "    # Block 6 (lambda_g): -(-I) = +I\n",
    "    rhs[n_p + n_btheta + n_bg + n_theta + n_lambda_theta:n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g] = 1.0\n",
    "\n",
    "    solution, info = gmres(A_forward, rhs, rtol=1e-10, maxiter=1000)\n",
    "    if info == 0:\n",
    "        dtheta_dbg = solution[n_p+n_btheta+n_bg:n_p+n_btheta+n_bg+n_theta]\n",
    "        df_dbg = solution[n_p+n_btheta+n_bg+n_theta+n_lambda_theta+n_lambda_g:]\n",
    "\n",
    "        sensitivities['f']['wrt_bg'][0] = df_dbg[0]\n",
    "        sensitivities['θ₀']['wrt_bg'][0] = dtheta_dbg[0]\n",
    "        sensitivities['θ₁']['wrt_bg'][0] = dtheta_dbg[1]\n",
    "\n",
    "        print(f\"  df/db_g₀ = {df_dbg[0]:+.6f}\")\n",
    "        print(f\"  dθ₀/db_g₀ = {dtheta_dbg[0]:+.6f}\")\n",
    "        print(f\"  dθ₁/db_g₀ = {dtheta_dbg[1]:+.6f}\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "    # dθ₁/db_θ₀ by finite differences (should be -1.0 due to constraint θ₀ + θ₁ = 0)\n",
    "    dtheta1_dbtheta_fd = (theta_plus[1] - theta_minus[1]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₁/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₁']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta1_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₁']['wrt_btheta'][0] - dtheta1_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"with JVPs - Forward Mode Sensitivity Computation\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary of Key Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\")\n",
    "    print(\"- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\")\n",
    "    print(\"- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\")\n",
    "    print(\"- The objective is most sensitive to p₀ (df/dp₀ = -6)\")\n",
    "    print(\"\\nForward mode approach:\")\n",
    "    print(\"- Solves ∂R/∂u @ (du/dp) = -∂R/∂p directly\")\n",
    "    print(\"- Uses JVPs for all matrix-vector operations\")\n",
    "    print(\"- Requires one solve per parameter (less efficient for many parameters)\")\n",
    "    print(\"- More intuitive: directly computes how variables change with parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The All-VJP way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "with VJPs - Direct Theta Sensitivity Computation\n",
      "============================================================\n",
      "\n",
      "Solving for sensitivities of f (objective)...\n",
      "  df/dp₀ = -6.000000\n",
      "  df/dp₁ = -4.000000\n",
      "  df/dp₂ = -1.000000\n",
      "  df/db_θ₀ = -2.000000\n",
      "  df/db_g₀ = +2.000000\n",
      "\n",
      "Solving for sensitivities of θ₀ (design variable 0)...\n",
      "  dθ₀/dp₀ = -0.000000\n",
      "  dθ₀/dp₁ = +0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₀/db_g₀ = -0.000000\n",
      "\n",
      "Solving for sensitivities of θ₁ (design variable 1)...\n",
      "  dθ₁/dp₀ = -0.000000\n",
      "  dθ₁/dp₁ = -0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "dθ₁/db_θ₀:\n",
      "  UDE:    -1.000000\n",
      "  FD:     -1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results\n",
      "============================================================\n",
      "\n",
      "Note: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\n",
      "- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\n",
      "- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\n",
      "- The objective is most sensitive to p₀ (df/dp₀ = -6)\n",
      "\n",
      "This approach:\n",
      "- Solves for actual design variable sensitivities dθ/dp\n",
      "- Uses the Hessian for proper second-order effects\n",
      "- Generalizes to real optimization problems\n",
      "- Maintains computational efficiency with VJPs/JVPs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with Vector-Jacobian Products (VJPs) - solving for theta sensitivities directly.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    Uses Vector-Jacobian Products (VJPs) and solves for theta sensitivities directly.\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_f\n",
    "\n",
    "    # Define Lagrangian and its gradient\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Lagrangian function\"\"\"\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    def lagrangian_grad(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Gradient of Lagrangian w.r.t. Θ\"\"\"\n",
    "        return jax.grad(lagrangian, argnums=0)(Θ, p, λ_theta, λ_g)\n",
    "\n",
    "    # Function to compute Hessian-vector product using finite differences\n",
    "    def hessian_vector_product(v_theta, h=1e-8):\n",
    "        \"\"\"\n",
    "        Compute H @ v where H is the Hessian of the Lagrangian w.r.t. Θ\n",
    "        Uses finite differences: H @ v ≈ (∇L(Θ + h*v) - ∇L(Θ - h*v)) / (2*h)\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Compute gradients at perturbed points\n",
    "        grad_plus = lagrangian_grad(Θ_opt + h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "        grad_minus = lagrangian_grad(Θ_opt - h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "\n",
    "        # Finite difference approximation of Hessian-vector product\n",
    "        hvp = (grad_plus - grad_minus) / (2 * h)\n",
    "\n",
    "        return np.array(hvp)\n",
    "\n",
    "    # VJP functions for matrix-transpose-vector products\n",
    "    def vjp_d2L_dtheta_dp_T(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂²L/∂θ∂p]^T @ v_theta using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Get VJP function for the gradient w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda p: lagrangian_grad(Θ_opt, p, λ_theta_opt, λ_g_opt), p_opt)\n",
    "\n",
    "        # Apply VJP with v_theta\n",
    "        result = vjp_fun(v_theta_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dg_active_dp_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂p]^T @ v_lambda_g using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        # Get VJP function for g_active w.r.t. p\n",
    "        _, vjp_fun = jax.vjp(lambda p: g_active(Θ_opt, p), p_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_g\n",
    "        result = vjp_fun(v_lambda_g_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_df_dp_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂p]^T @ v_f using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        # Get VJP function for f w.r.t. p\n",
    "        _, vjp_fun = jax.vjp(lambda p: f(Θ_opt, p), p_opt)\n",
    "\n",
    "        # Apply VJP with v_f\n",
    "        result = vjp_fun(v_f_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dtheta_active_dtheta_T(v_lambda_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ]^T @ v_lambda_theta using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_theta_jax = jnp.array(v_lambda_theta)\n",
    "\n",
    "        # Get VJP function for Θ_active w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(Θ_active, Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_theta\n",
    "        result = vjp_fun(v_lambda_theta_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dg_active_dtheta_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ]^T @ v_lambda_g using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        # Get VJP function for g_active w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda theta: g_active(theta, p_opt), Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_g\n",
    "        result = vjp_fun(v_lambda_g_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_df_dtheta_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂θ]^T @ v_f using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        # Get VJP function for f w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda theta: f(theta, p_opt), Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_f\n",
    "        result = vjp_fun(v_f_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    # Regular JVP functions for forward matrix-vector products\n",
    "    def jvp_dtheta_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(Θ_active, (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dg_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_g)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(lambda theta: g_active(theta, p_opt), (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def matvec_transpose(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u]^T @ v\n",
    "        This implements the transpose of the UDE Jacobian matrix.\n",
    "        Uses VJPs for transpose operations and JVPs for forward operations.\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_f = v[idx:idx+n_f]\n",
    "\n",
    "        # print(f\"Input vector norm: {np.linalg.norm(v):.6f}\")\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Effect on p\n",
    "        result[:n_p] = v_p\n",
    "        result[:n_p] -= vjp_d2L_dtheta_dp_T(v_theta)\n",
    "        result[:n_p] -= vjp_dg_active_dp_T(v_lambda_g)\n",
    "        result[:n_p] -= vjp_df_dp_T(v_f)\n",
    "\n",
    "        # Block 2: Effect on b_theta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta + v_lambda_theta\n",
    "\n",
    "        # Block 3: Effect on b_g\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg + v_lambda_g\n",
    "\n",
    "        # Block 4: Effect on theta\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "        if np.any(v_theta):\n",
    "            # print('using hessian-vector product!')\n",
    "            # Use Hessian-vector product for second derivatives\n",
    "            result_theta -= hessian_vector_product(v_theta)\n",
    "        result_theta -= vjp_dtheta_active_dtheta_T(v_lambda_theta)\n",
    "        result_theta -= vjp_dg_active_dtheta_T(v_lambda_g)\n",
    "        result_theta -= vjp_df_dtheta_T(v_f)\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Effect on lambda_theta\n",
    "        idx += n_theta\n",
    "        result[idx:idx+n_lambda_theta] = -jvp_dtheta_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 6: Effect on lambda_g\n",
    "        idx += n_lambda_theta\n",
    "        result[idx:idx+n_lambda_g] = -jvp_dg_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 7: Effect on f\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_f] = v_f\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator\n",
    "    A_transpose = LinearOperator((total_size, total_size), matvec=matvec_transpose)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # Solve for each output: f, θ₀, θ₁\n",
    "    output_info = [\n",
    "        ('f', n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g, 'objective'),\n",
    "        ('θ₀', n_p + n_btheta + n_bg, 'design variable 0'),\n",
    "        ('θ₁', n_p + n_btheta + n_bg + 1, 'design variable 1')\n",
    "    ]\n",
    "\n",
    "    for name, rhs_idx, description in output_info:\n",
    "        # Create RHS vector with 1 in appropriate position\n",
    "        rhs = np.zeros(total_size)\n",
    "        rhs[rhs_idx] = 1.0\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities of {name} ({description})...\")\n",
    "        solution, info = gmres(A_transpose, rhs, rtol=1e-10, maxiter=1000)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities\n",
    "            sens_p = solution[:n_p]\n",
    "            sens_btheta = solution[n_p:n_p+n_btheta]\n",
    "            sens_bg = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "\n",
    "            sensitivities[name] = {\n",
    "                'wrt_p': sens_p,\n",
    "                'wrt_btheta': sens_btheta,\n",
    "                'wrt_bg': sens_bg\n",
    "            }\n",
    "\n",
    "            print(f\"  d{name}/dp₀ = {sens_p[0]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₁ = {sens_p[1]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₂ = {sens_p[2]:+.6f}\")\n",
    "            print(f\"  d{name}/db_θ₀ = {sens_btheta[0]:+.6f}\")\n",
    "            print(f\"  d{name}/db_g₀ = {sens_bg[0]:+.6f}\")\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    # Verify df/dp₀ and dθ₀/db_θ₀\n",
    "\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "    # dθ₁/db_θ₀ by finite differences (should be -1.0 due to constraint θ₀ + θ₁ = 0)\n",
    "    dtheta1_dbtheta_fd = (theta_plus[1] - theta_minus[1]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₁/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₁']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta1_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₁']['wrt_btheta'][0] - dtheta1_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"with VJPs - Direct Theta Sensitivity Computation\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary of Key Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\")\n",
    "    print(\"- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\")\n",
    "    print(\"- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\")\n",
    "    print(\"- The objective is most sensitive to p₀ (df/dp₀ = -6)\")\n",
    "    print(\"\\nThis approach:\")\n",
    "    print(\"- Solves for actual design variable sensitivities dθ/dp\")\n",
    "    print(\"- Uses the Hessian for proper second-order effects\")\n",
    "    print(\"- Generalizes to real optimization problems\")\n",
    "    print(\"- Maintains computational efficiency with VJPs/JVPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different optimization problem\n",
    "\n",
    "### The G13 problem from _Evolutionary Computation with Biogeography-based Optimization_ by Ma and Simon\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmdao.api as om\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class G13Comp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('x1')\n",
    "        self.add_input('x2')\n",
    "        self.add_input('x3')\n",
    "        self.add_input('x4')\n",
    "        self.add_input('x5')\n",
    "        self.add_input('a')\n",
    "        self.add_input('b')\n",
    "        self.add_input('c')\n",
    "        self.add_output('f')\n",
    "        self.add_output('h1')\n",
    "        self.add_output('h2')\n",
    "        self.add_output('h3')\n",
    "\n",
    "    def compute_primal(self, x1, x2, x3, x4, x5, a=10., b=5., c=1.):\n",
    "        f = jnp.exp(x1 * x2 * x3 * x4 * x5)\n",
    "        h1 = x1 ** 2 + x2 ** 2 + x3 ** 2 + x4 ** 2 + x5 ** 2 - a\n",
    "        h2 = x2 * x3 - b * x4 * x5\n",
    "        h3 = x1 ** 3 + x2 ** 3 + c\n",
    "        return f, h1, h2, h3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmdao.api as om\n",
    "\n",
    "prob = om.Problem()\n",
    "\n",
    "prob.model.add_subsystem('g13', G13Comp(), promotes=['*'])\n",
    "\n",
    "\n",
    "for i in [1, 2]:\n",
    "    prob.model.add_design_var(f'x{i}', lower=-2.3, upper=2.3)\n",
    "\n",
    "for i in [3, 4, 5]:\n",
    "    prob.model.add_design_var(f'x{i}', lower=-3.2, upper=3.2)\n",
    "\n",
    "prob.model.add_objective('f')\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    prob.model.add_constraint(f'h{i}', equals=0)\n",
    "\n",
    "\n",
    "prob.driver = om.ScipyOptimizeDriver()\n",
    "prob.driver.opt_settings['ATOL'] = 1.0E-8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.053949845295294487\n",
      "            Iterations: 20\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 20\n",
      "Optimization Complete\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Problem: problem10\n",
       "Driver:  ScipyOptimizeDriver\n",
       "  success     : True\n",
       "  iterations  : 27\n",
       "  runtime     : 1.6207E-01 s\n",
       "  model_evals : 27\n",
       "  model_time  : 2.1283E-02 s\n",
       "  deriv_evals : 20\n",
       "  deriv_time  : 1.3072E-01 s\n",
       "  exit_status : SUCCESS"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.setup()\n",
    "\n",
    "prob.set_val('a', 10)\n",
    "prob.set_val('b', 5)\n",
    "prob.set_val('c', 1)\n",
    "\n",
    "# prob.set_val('x1', -1.717143)\n",
    "# prob.set_val('x2', 1.595709)\n",
    "# prob.set_val('x3', 1.827247)\n",
    "# prob.set_val('x4', -0.7636413)\n",
    "# prob.set_val('x5', -0.763645)\n",
    "\n",
    "prob.run_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Design Variables\n",
      "----------------\n",
      "name  val            size  lower  upper  ref  ref0  indices  adder  scaler  parallel_deriv_color  cache_linear_solution  units  \n",
      "----  -------------  ----  -----  -----  ---  ----  -------  -----  ------  --------------------  ---------------------  ----- \n",
      "x1    [-1.71714333]  1     -2.3   2.3    1.0  0.0   None     None   None    None                  False                  None   \n",
      "x2    [1.59570942]   1     -2.3   2.3    1.0  0.0   None     None   None    None                  False                  None   \n",
      "x3    [1.8272462]    1     -3.2   3.2    1.0  0.0   None     None   None    None                  False                  None   \n",
      "x4    [0.76363602]   1     -3.2   3.2    1.0  0.0   None     None   None    None                  False                  None   \n",
      "x5    [0.76365019]   1     -3.2   3.2    1.0  0.0   None     None   None    None                  False                  None   \n",
      "\n",
      "-----------\n",
      "Constraints\n",
      "-----------\n",
      "name  val                size  alias  lower   upper  equals  ref  ref0  indices  adder  scaler  linear  parallel_deriv_color  cache_linear_solution  units  \n",
      "----  -----------------  ----  -----  ------  -----  ------  ---  ----  -------  -----  ------  ------  --------------------  ---------------------  ----- \n",
      "h1    [4.45402524e-08]   1     None   -1e+30  1e+30  0.0     1.0  0.0   None     None   None    False   None                  False                  None   \n",
      "h2    [-1.69241313e-08]  1     None   -1e+30  1e+30  0.0     1.0  0.0   None     None   None    False   None                  False                  None   \n",
      "h3    [9.92039628e-09]   1     None   -1e+30  1e+30  0.0     1.0  0.0   None     None   None    False   None                  False                  None   \n",
      "\n",
      "----------\n",
      "Objectives\n",
      "----------\n",
      "name  val           size  ref  ref0  indices  adder  scaler  units  parallel_deriv_color  cache_linear_solution  \n",
      "----  ------------  ----  ---  ----  -------  -----  ------  -----  --------------------  --------------------- \n",
      "f     [0.05394985]  1     1.0  0.0   None     None   None    None   None                  False                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob.list_driver_vars();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0001544497507061727)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openmdao.utils.assert_utils import assert_near_equal\n",
    "\n",
    "assert_near_equal(prob.get_val('f'), 0.053941514041898, tolerance=1.0E-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
