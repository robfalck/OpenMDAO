{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Post-Optimality Sensitivities of a Constrained Optimization Problem\n",
    "\n",
    "Lets consider a problem such that we have an active bound and an active inequality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta_0,\\, \\theta_1} \\quad & f(\\theta_0, \\theta_1; \\mathbf{p}) = (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\\\\n",
    "\\text{where} \\quad \\mathbf{p} &= \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "\\text{bounds:} \\quad \\theta_0 &\\le 6 \\\\\n",
    "\\text{equality constraints:} \\quad \\theta_0 + \\theta_1 &= 0\n",
    "\\end{align*}\n",
    "\n",
    "We want to know the sensitivities of the optimization outputs with respect to the optimization inputs.\n",
    "\n",
    "In this context, consider the outputs of the optimization to be the objective and any other functions of interest, $f$.\n",
    "\n",
    "The design variables $\\theta$ and Lagrange multipliers $\\lambda$ are effectively the implicit outputs of the optimization.\n",
    "\n",
    "The _inputs_ to the optimization process consists of:\n",
    "- any independent parameters, $\\bar{p}$\n",
    "- the bounding values of any **active** design variables, $\\bar{b}_{\\theta}$\n",
    "- the bounding values of any **active** constraints, $\\bar{b}o_{g}$\n",
    "\n",
    "In our case we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{p} &= \\begin{bmatrix} p_0 \\\\ p_1 \\\\ p_2 \\end{bmatrix} \\\\\n",
    "    \\bar{b}_{\\theta} &= \\begin{bmatrix} \\theta_0^{ub} \\end{bmatrix} \\\\\n",
    "    \\bar{b}_g &= \\begin{bmatrix} g_0^{eq} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<!-- If active, we can treat the bound on $\\theta_0$ as just another equality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{\\mathcal{G}}(\\bar{\\theta}, \\bar{p}) &= \\begin{bmatrix}\n",
    "                                   \\theta_0 + \\theta_1 \\\\\n",
    "                                   \\theta_0 - p_3\n",
    "                                \\end{bmatrix} = \\bar 0\n",
    "\\end{align*}\n",
    "\n",
    "**How will my system design ($\\bar{\\theta}^*$) respond to changes in my assumptions and system inputs ($\\bar{p}$)?** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Derivatives Equation\n",
    "\n",
    "The UDE is:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right] \\left[ \\frac{d u}{d \\mathcal{R}} \\right]\n",
    "  &=\n",
    "  \\left[ I \\right]\n",
    "  =\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right]^T \\left[ \\frac{d u}{d \\mathcal{R}} \\right]^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, the residuals are the primal and dual residuals of the optimization process, given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the UDE to solving post-optimality sensitivities\n",
    "\n",
    "In our case, the unknowns vector consists of\n",
    "- the optimization parameters ($\\bar{p}$)\n",
    "- the bounding values of any active design variables ($\\bar{b}_{\\theta}$)\n",
    "- the bounding values of any active constraints ($\\bar{b}_{g}$)\n",
    "- the design variables of the optimization ($\\bar{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active design variables ($\\bar{\\lambda}_{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active constraints ($\\bar{\\lambda}_{g}$)\n",
    "- the objective value **as well as** any other outputs for which we want the sensitivities ($f$)\n",
    "\n",
    "The total size of the unknowns vector is $N_p + N_{\\theta} + 2N_{\\lambda \\theta} + 2N_{\\lambda g} + N_{f}$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{u} &=\n",
    "  \\begin{bmatrix}\n",
    "    \\bar{p} \\\\\n",
    "    \\bar{b}_{\\theta} \\\\\n",
    "    \\bar{b}_{g} \\\\\n",
    "    \\bar{\\theta} \\\\\n",
    "    \\bar{\\lambda_{\\theta}} \\\\\n",
    "    \\bar{\\lambda_{g}} \\\\\n",
    "    \\bar{f}\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Under the UDE, the corresponding residual equations for these unknowns are\n",
    "- the implicit form of the parameter values\n",
    "- the implicit form of the active design variable values\n",
    "- the implicit form of the active constraint values\n",
    "- the stationarity condition\n",
    "- the active design variable residuals\n",
    "- the active constraint residuals\n",
    "- the implicit form of the explicit calculations of $f$\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathcal{R}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathcal{R}}_p \\\\\n",
    "\\bar{\\mathcal{R}}_{b \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{b g} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda g} \\\\\n",
    "\\bar{\\mathcal{R}}_{f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\check{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{\\theta} - \\check{b}_{\\theta} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{g} - \\check{b}_g \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ \\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_g + \\nabla_{\\bar{\\theta}} \\check{\\theta}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda \\theta} - \\left[ \\check{\\theta}_{\\mathcal{A}} \\left( \\bar{\\theta} \\right) - \\bar{b}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda g} - \\left[ \\check{g}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_g \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{f} - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&= \n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\check{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{\\theta} - \\check{b}_{\\theta} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{g} - \\check{b}_g \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\nabla_{\\theta} \\check{\\mathcal{L}} \\left( \\bar{\\theta}, \\bar{p} \\right) \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda \\theta} - \\left[ \\check{\\theta}_{\\mathcal{A}} \\left( \\bar{\\theta} \\right) - \\bar{b}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda g} - \\left[ \\check{g}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_g \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{f} - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\bar 0\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the total derivatives that we seek ($\\frac{d f^*}{d \\bar{p}}$ and $\\frac{d \\bar{\\theta}^*}{d \\bar{p}}$), we need $\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}$.\n",
    "\n",
    "The optimizer has served as the nonlinear solver in this case which has computed the values in the unknowns vector: $\\bar{\\theta}$, $\\bar{\\lambda}$, and $\\bar{f}$ such that the residuals are satisfied.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_p}}{\\partial \\bar{p}} & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} \\theta}}}{\\partial \\bar{b}_{\\theta}} & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} g}}}{\\partial \\bar{b}_{g}} & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_{\\theta}}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_g}} & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{b}_{\\theta}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{\\theta}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{p}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{b}_g} & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{\\theta}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}} & 0 & 0 & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}} & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}} & 0 & \\left[ I_{bg} \\right] & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{f}}{\\partial \\bar{p}} & 0 & 0 & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This nomenclature can be a bit confusing.\n",
    "\n",
    "**The _partial_ derivatives of the post-optimality residuals are the _total_ derivatives of the analysis.**\n",
    "\n",
    "In this case of the stationarity residuals $\\mathcal{R}_{\\bar{\\theta}}$, which already include _total_ derivatives of the analysis for the objective and constraint gradients, second derivatives are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding total derivaties which we need to solve for are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\bar{u}}{d \\bar{\\mathcal{R}}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{b}_{\\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{b}_{g}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_p}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{b\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{bg}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\left[ I_p \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  \\left[ I_{b\\theta} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  0 &\n",
    "  \\left[ I_{bg} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we can solve them with four linear solves of the forward system, or three solves of the reverse system.\n",
    "\n",
    "TODO: Need to explain how du/dRf becomes du/df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDE for this case, in forward form, is\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}} & 0 & 0 & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}} & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T & 0 \\\\[1.1ex]\n",
    "0 & \\left[ I_{b\\theta} \\right] & 0 & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}} & 0 & \\left[ I_{bg} \\right] & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "-\\frac{\\partial \\check{f}}{\\partial \\bar{p}} & 0 & 0 & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "  \\left[ I_p \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  \\left[ I_{b\\theta} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  0 &\n",
    "  0 &\n",
    "  \\left[ I_{bg} \\right] &\n",
    "  0 &\n",
    "  0 &\n",
    "  0 &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  0\n",
    "\\\\[1.1ex]\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}} &\n",
    "  \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}} &\n",
    "  \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}} &\n",
    "  \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_{\\theta} \\right] & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & \\left[ I_{\\lambda \\theta} \\right] & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & \\left[ I_{\\lambda g} \\right] & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "The linear solve of this system can proceed one column of $\\left[ \\frac{d \\bar{u}}{d \\mathcal{R}} \\right]$ at a time (the forward solve).\n",
    "In this case we would need one solve for each column ($N_p + $N_{b\\theta}$ + $N_b{g}$).\n",
    "In our example optimization with three parameters, an active bound and an active equality constraint, this would be five solves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could transpose this system and solve it in reverse mode. Solving for one column at a time in the transposed system would mean solving once for each design variable and each output of interest. In our example optimziation this would be three solves.\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                    & 0                    & -\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}}^T & 0                    & -\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check{f}}{\\partial \\bar{p}}^T \\\\[1.1ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right] & 0                    & 0                                                                & \\left[ I_{b\\theta} \\right] & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & \\left[ I_{bg} \\right]      & 0                                                                & 0                    & \\left[ I_{bg} \\right]                                      & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}}                        & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}  & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}}                & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T               & 0                    & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T                    & 0                    & 0                                                    & 0 \\\\[1.1ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                         & 0                       & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}}^T                    & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{p}}^T              & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{p}}^T                 & \\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}}^T \\\\[1.1ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right]      & 0                       & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}}^T           & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_{\\theta}}}^T     & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_{\\theta}}}^T        & \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & \\left[ I_{bg} \\right]         & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}}^T                  & \\frac{d \\bar{\\lambda_{\\theta}}}{d \\bar{b_g}}^T            & \\frac{d \\bar{\\lambda_{g}}}{d \\bar{b_g}}^T                & \\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}}^T          & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\theta}}}^T    & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\theta}}}^T       & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\theta}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T  & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda \\theta}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T       & \\frac{d \\bar{\\lambda \\theta}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T    & \\frac{d \\bar{\\lambda g}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T       & \\frac{d \\bar{f}}{d \\bar{\\mathcal{R}_{\\lambda g}}}^T \\\\[1.1ex]\n",
    "0                                                     & 0                         & 0                       & 0                                                                & 0                                                       & 0                                                         & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & \\left[ I_{b\\theta} \\right] & 0 & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_{bg} \\right] & 0 & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_{\\theta} \\right] & 0 & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & \\left[ I_{\\lambda \\theta} \\right] & 0 & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & \\left[ I_{\\lambda g} \\right] & 0\\\\[1.1ex]\n",
    "    0 & 0 & 0 & 0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One more trick\n",
    "\n",
    "Computing the Hessian of the Lagrangian is potentially expensive and we'd like to avoid it if possible.\n",
    "\n",
    "When we seed this solve for the sensitivties of the objective (the last column) we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                    & 0                    & -\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}}^T & 0                    & -\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check{f}}{\\partial \\bar{p}}^T \\\\[1.3ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right] & 0                    & 0                                                                & \\left[ I_{b\\theta} \\right] & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & \\left[ I_{bg} \\right]      & 0                                                                & 0                    & \\left[ I_{bg} \\right]                                      & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}}                        & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}  & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}}                & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}}^T \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T               & 0                    & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T                    & 0                    & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{\\frac{d \\bar{f}}{d \\bar{p}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{f}}{d \\bar{b_{\\theta}}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{f}}{d \\bar{b_g}}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}}{d \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}}{d \\bar{\\lambda \\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}}{d \\bar{\\lambda g}}^T \\\\[1.1ex]\n",
    "\\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we seed this solve for the sensitivties of the design variables (the fourth column) we have:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                    & 0                    & -\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}}^T & 0                    & -\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check{f}}{\\partial \\bar{p}}^T \\\\[1.3ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right] & 0                    & 0                                                                & \\left[ I_{b\\theta} \\right] & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & \\left[ I_{bg} \\right]      & 0                                                                & 0                    & \\left[ I_{bg} \\right]                                      & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}}                        & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}  & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}}                & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}}^T \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T               & 0                    & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T                    & 0                    & 0                                                    & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_{\\theta}}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{b_g}}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\lambda \\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\lambda g}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{f}}^T \\\\[1.1ex]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In this case, we seed the right vector with a one somewhere in the rows corresponding the $\\theta$, while all other rows are zero.\n",
    "This means that we need to multiply by the Hessian of the Lagrangian, which is expensive to compute.\n",
    "\n",
    "We don't have this issue when we're solving for outputs, because the rows initially being multiplied by the Hessian of the Lagrangian are zero.\n",
    "\n",
    "But if we had an output function that just echoed the values of the design variables, lets call it $f_{\\bar{\\theta}}$, then solving for the sensitivities of that function with respect to the parameters and bounds _would be the same thing_ as solving for the sensitivities of the design variables with respect to the parameters and bounds.\n",
    "\n",
    "So we effectively augument the unknowns vector $\\bar{u}$ with $f_{\\bar{\\theta}}$ and add its corresponding residual $\\mathcal{R}_{f\\theta} = \\bar{f}_{\\theta} - \\bar{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of solving for the column of design variable sensitivities, we solve for additional columns corresponding to $f_{\\theta}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right]                                    & 0                    & 0                    & -\\frac{\\partial \\nabla_{\\theta} \\bar{\\mathcal{L}}}{\\partial \\bar{p}}^T & 0                    & -\\frac{\\partial \\check{g}_{\\mathcal{A}}}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check{f}}{\\partial \\bar{p}}^T & 0 \\\\[1.3ex]\n",
    "0                                                     & \\left[ I_{b\\theta} \\right] & 0                    & 0                                                                & \\left[ I_{b\\theta} \\right] & 0                                                    & 0 & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & \\left[ I_{bg} \\right]      & 0                                                                & 0                    & \\left[ I_{bg} \\right]                                      & 0 & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta}^2 \\check{\\mathcal{L}}                        & -\\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}  & -\\nabla_{\\theta} \\check{g}_{\\mathcal{A}}                & -\\frac{\\partial \\check{f}}{\\partial \\bar{\\theta}}^T  & -\\left[I_{f\\theta}\\right] \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{\\theta}_{\\mathcal{A}}^T               & 0                    & 0                                                    & 0 & 0\\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & - \\nabla_{\\theta} \\check{g}_{\\mathcal{A}}^T                    & 0                    & 0                                                    & 0 & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & \\left[ I_f \\right] & 0 \\\\[1.3ex]\n",
    "0                                                     & 0                    & 0                    & 0                                                                & 0                    & 0                                                    & 0 & \\left[ I_{f\\theta} \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{\\frac{d \\bar{f}_{\\theta}}{d \\bar{p}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{f}_{\\theta}}{d \\bar{b_{\\theta}}}}^T \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{f}_{\\theta}}{d \\bar{b_g}}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}_{\\theta}}{d \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}_{\\theta}}{d \\bar{\\lambda \\theta}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}_{\\theta}}{d \\bar{\\lambda g}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{f}_{\\theta}}{d \\bar{f}}^T \\\\[1.1ex]\n",
    "\\left[ I_{f\\theta} \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    0\\\\[1.8ex]\n",
    "    \\left[ I_{f\\theta} \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "============================================================\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Solving for sensitivities of f...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[ 0.79471941  0.52981294  0.13245324  0.          0.          0.\n",
      " -0.26490647  0.          0.          0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.05448526  0.48128647 -0.00908088  0.          0.          0.25880499\n",
      "  0.79457673  0.          0.25880499  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[ 0.0358737   0.1143094  -0.08025979  0.          0.25871623 -0.81719063\n",
      "  0.29611    -0.25871623 -0.29975817  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.20882503  0.26031995 -0.00167372 -0.35466222  0.60993311  0.17641056\n",
      " -0.10667204  0.45405356 -0.37703434  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.28802544  0.28758436  0.09178881  0.72515809  0.3594584   0.13373555\n",
      " -0.2430132  -0.30577893  0.02013532  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000000e+00\n",
      "  2.00000000e+00  2.77555756e-17 -4.44089210e-16  2.00000000e+00\n",
      " -2.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian!\n",
      "  df/dp₀ = -6.000000\n",
      "  df/dp₁ = -4.000000\n",
      "  df/dp₂ = -1.000000\n",
      "  df/db_θ₀ = -2.000000\n",
      "  df/db_g₀ = +2.000000\n",
      "\n",
      "Solving for sensitivities of θ₀...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[ 0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.]\n",
      "using hessian!\n",
      "[-0.75592895  0.          0.          0.          0.          0.\n",
      "  0.37796447  0.37796447  0.37796447  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.4472136  -0.4472136   0.          0.2236068   0.2236068   0.\n",
      " -0.67082039  0.         -0.2236068   0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.18351288  0.34080963  0.          0.61607894  0.35391769  0.\n",
      "  0.24905319 -0.52432251 -0.09175644  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.03310594  0.40554772  0.         -0.34761233  0.63728927  0.\n",
      " -0.01241473  0.3641653  -0.41796244  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-8.32667268e-17  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -1.66533454e-16  9.25185854e-17 -8.32667268e-17 -1.00000000e+00\n",
      "  1.11022302e-16  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "using hessian!\n",
      "  dθ₀/dp₀ = -0.000000\n",
      "  dθ₀/dp₁ = +0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₀/db_g₀ = -0.000000\n",
      "\n",
      "Solving for sensitivities of θ₁...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[ 0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.]\n",
      "using hessian!\n",
      "[0.         0.81649658 0.         0.         0.         0.40824829\n",
      " 0.         0.         0.40824829 0.         0.         0.        ]\n",
      "using hessian!\n",
      "[ 0.4472136   0.4472136   0.          0.          0.2236068  -0.67082039\n",
      "  0.         -0.2236068  -0.2236068   0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.21622499  0.14414999  0.         -0.36037499  0.61263747  0.14414999\n",
      "  0.          0.46848748 -0.43244998  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[-0.58556655  0.07060022  0.          0.62294314  0.37999532 -0.08928852\n",
      "  0.         -0.33015986 -0.05191193  0.          0.          0.        ]\n",
      "using hessian!\n",
      "[ 5.55111512e-17 -3.60822483e-16  0.00000000e+00 -1.00000000e+00\n",
      "  1.00000000e+00 -5.55111512e-17 -9.32242397e-17  1.00000000e+00\n",
      " -1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "using hessian!\n",
      "  dθ₁/dp₀ = +0.000000\n",
      "  dθ₁/dp₁ = -0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results\n",
      "============================================================\n",
      "\n",
      "Note: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\n",
      "- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\n",
      "- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\n",
      "- The objective is most sensitive to p₀ (df/dp₀ = -6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with identity outputs to avoid Hessian computations.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "    n_f_theta = 2  # identity outputs for design variables\n",
    "    n_outputs_total = n_f + n_f_theta  # total outputs\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_outputs_total\n",
    "\n",
    "    # Compute required derivatives using JAX\n",
    "\n",
    "    # Objective derivatives\n",
    "    df_dtheta = jax.jacobian(f, argnums=0)(Θ_opt, p_opt).reshape(-1)\n",
    "    df_dp = jax.jacobian(f, argnums=1)(Θ_opt, p_opt).reshape(-1)\n",
    "\n",
    "    # Active bound derivatives (Θ[0] - b_theta = 0)\n",
    "    dtheta_active_dtheta = jax.jacobian(Θ_active, argnums=0)(Θ_opt).reshape(n_lambda_theta, n_theta)\n",
    "\n",
    "    # Active constraint derivatives\n",
    "    dg_active_dtheta = jax.jacobian(g_active, argnums=0)(Θ_opt, p_opt).reshape(n_lambda_g, n_theta)\n",
    "    dg_active_dp = jax.jacobian(g_active, argnums=1)(Θ_opt, p_opt).reshape(n_lambda_g, n_p)\n",
    "\n",
    "    # Lagrangian gradient: ∇L = ∇f + λ_theta^T ∇Θ_active + λ_g^T ∇g_active\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    # Hessian of Lagrangian w.r.t. Θ\n",
    "    d2L_dtheta2 = jax.hessian(lagrangian, argnums=0)(Θ_opt, p_opt, λ_theta_opt, λ_g_opt)\n",
    "\n",
    "    # Mixed derivatives of Lagrangian\n",
    "    d2L_dtheta_dp = jax.jacobian(jax.grad(lagrangian, argnums=0), argnums=1)(\n",
    "        Θ_opt, p_opt, λ_theta_opt, λ_g_opt\n",
    "    )\n",
    "\n",
    "    def matvec_transpose(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u]^T @ v\n",
    "        This implements the transpose of the UDE Jacobian matrix.\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_outputs = v[idx:idx+n_outputs_total]\n",
    "        v_f = v_outputs[:n_f]\n",
    "        v_f_theta = v_outputs[n_f:]\n",
    "\n",
    "        print(v)\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Effect on p\n",
    "        result[:n_p] = v_p\n",
    "        if np.any(v_theta):\n",
    "            result[:n_p] -= d2L_dtheta_dp.T @ v_theta\n",
    "        if np.any(v_lambda_g):\n",
    "            result[:n_p] -= dg_active_dp.T @ v_lambda_g\n",
    "        if np.any(v_f):\n",
    "            result[:n_p] -= df_dp * v_f[0]\n",
    "\n",
    "        # Block 2: Effect on b_theta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta + v_lambda_theta\n",
    "\n",
    "        # Block 3: Effect on b_g\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg + v_lambda_g\n",
    "\n",
    "        # Block 4: Effect on theta\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "        if np.any(v_theta):\n",
    "            print('using hessian!')\n",
    "            result_theta -= d2L_dtheta2 @ v_theta\n",
    "        if np.any(v_lambda_theta):\n",
    "            result_theta -= dtheta_active_dtheta.T @ v_lambda_theta\n",
    "        if np.any(v_lambda_g):\n",
    "            result_theta -= dg_active_dtheta.T @ v_lambda_g\n",
    "        if np.any(v_f):\n",
    "            result_theta -= df_dtheta * v_f[0]\n",
    "        # Identity outputs contribution (avoids Hessian!)\n",
    "        if np.any(v_f_theta):\n",
    "            result_theta -= v_f_theta  # -I @ v_f_theta\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Effect on lambda_theta\n",
    "        idx += n_theta\n",
    "        if np.any(v_theta):\n",
    "            result[idx:idx+n_lambda_theta] = -dtheta_active_dtheta @ v_theta\n",
    "\n",
    "        # Block 6: Effect on lambda_g\n",
    "        idx += n_lambda_theta\n",
    "        if np.any(v_theta):\n",
    "            result[idx:idx+n_lambda_g] = -dg_active_dtheta @ v_theta\n",
    "\n",
    "        # Block 7: Effect on outputs\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_outputs_total] = v_outputs\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator\n",
    "    A_transpose = LinearOperator((total_size, total_size), matvec=matvec_transpose)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # Solve for each output\n",
    "    output_names = ['f', 'θ₀', 'θ₁']\n",
    "\n",
    "    for i, name in enumerate(output_names):\n",
    "        # Create RHS vector with 1 in appropriate output position\n",
    "        rhs = np.zeros(total_size)\n",
    "        output_start_idx = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g\n",
    "        rhs[output_start_idx + i] = 1.0\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities of {name}...\")\n",
    "        solution, info = gmres(A_transpose, rhs, rtol=1e-10, maxiter=1000)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities\n",
    "            sens_p = solution[:n_p]\n",
    "            sens_btheta = solution[n_p:n_p+n_btheta]\n",
    "            sens_bg = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "\n",
    "            sensitivities[name] = {\n",
    "                'wrt_p': sens_p,\n",
    "                'wrt_btheta': sens_btheta,\n",
    "                'wrt_bg': sens_bg\n",
    "            }\n",
    "\n",
    "            print(f\"  d{name}/dp₀ = {sens_p[0]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₁ = {sens_p[1]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₂ = {sens_p[2]:+.6f}\")\n",
    "            print(f\"  d{name}/db_θ₀ = {sens_btheta[0]:+.6f}\")\n",
    "            print(f\"  d{name}/db_g₀ = {sens_bg[0]:+.6f}\")\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    # For simplicity, we'll verify df/dp₀ and dθ₀/db_θ₀\n",
    "\n",
    "    # Verify df/dp₀\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary of Key Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\")\n",
    "    print(\"- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\")\n",
    "    print(\"- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\")\n",
    "    print(\"- The objective is most sensitive to p₀ (df/dp₀ = -6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres a version without the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "WITHOUT Computing the Hessian of the Lagrangian\n",
      "============================================================\n",
      "v_theta=array([0, 0], dtype=int8)\n",
      "v_f_theta=array([0, 0], dtype=int8)\n",
      "\n",
      "NOTE: Computing sensitivities WITHOUT computing the Hessian!\n",
      "The identity output approach allows us to skip ∇²L entirely.\n",
      "\n",
      "\n",
      "Solving for sensitivities of f...\n",
      "v_theta=array([0., 0.])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([ 0.        , -0.26490647])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0.        , 0.52245748])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-0.5754727 , -0.24824313])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([ 0.42731203, -0.55680053])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-0.19323376, -0.30288722])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-6.66133815e-16,  0.00000000e+00])\n",
      "v_f_theta=array([0., 0.])\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000000e+00\n",
      "  2.00000000e+00 -6.66133815e-16  0.00000000e+00  2.00000000e+00\n",
      " -2.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "  df/dp₀ = -6.000000\n",
      "  df/dp₁ = -4.000000\n",
      "  df/dp₂ = -1.000000\n",
      "  df/db_θ₀ = -2.000000\n",
      "  df/db_g₀ = +2.000000\n",
      "\n",
      "Solving for sensitivities of θ₀...\n",
      "  (Using identity output - no Hessian needed!)\n",
      "v_theta=array([0., 0.])\n",
      "v_f_theta=array([1., 0.])\n",
      "v_theta=array([-1.,  0.])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0., 0.])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([ 0.        , -0.48038446])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0.        , 0.27006383])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([ 0.        , -0.27971546])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0.00000000e+00, 2.35922393e-16])\n",
      "v_f_theta=array([1., 0.])\n",
      "[-1.11022302e-16  3.33066907e-16  0.00000000e+00  1.00000000e+00\n",
      " -1.80411242e-16  0.00000000e+00  2.35922393e-16 -1.00000000e+00\n",
      "  1.11022302e-16  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  dθ₀/dp₀ = -0.000000\n",
      "  dθ₀/dp₁ = +0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₀/db_g₀ = -0.000000\n",
      "\n",
      "Solving for sensitivities of θ₁...\n",
      "  (Using identity output - no Hessian needed!)\n",
      "v_theta=array([0., 0.])\n",
      "v_f_theta=array([0., 1.])\n",
      "v_theta=array([ 0., -1.])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0., 0.])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-0.5976143,  0.       ])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([0.15034619, 0.        ])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-0.36594312,  0.        ])\n",
      "v_f_theta=array([0., 0.])\n",
      "v_theta=array([-2.77555756e-17,  1.48952049e-16])\n",
      "v_f_theta=array([0., 1.])\n",
      "[-5.55111512e-17  2.77555756e-16  0.00000000e+00 -1.00000000e+00\n",
      "  1.00000000e+00 -2.77555756e-17  1.48952049e-16  1.00000000e+00\n",
      " -1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  dθ₁/dp₀ = -0.000000\n",
      "  dθ₁/dp₁ = +0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "dθ₁/db_θ₀:\n",
      "  UDE:    -1.000000\n",
      "  FD:     -1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary\n",
      "============================================================\n",
      "\n",
      "Key insight: By using identity outputs for θ₀ and θ₁,\n",
      "we completely avoid computing the expensive Hessian ∇²L.\n",
      "\n",
      "The trick is that when solving for θ sensitivities,\n",
      "the RHS vector has zeros in the θ residual positions,\n",
      "so v_theta = 0 throughout the GMRES iteration,\n",
      "eliminating the need for Hessian-vector products!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with identity outputs to avoid Hessian computations.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "    n_f_theta = 2  # identity outputs for design variables\n",
    "    n_outputs_total = n_f + n_f_theta  # total outputs\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_outputs_total\n",
    "\n",
    "    # Compute required derivatives using JAX (NO HESSIAN NEEDED!)\n",
    "\n",
    "    # Objective derivatives\n",
    "    df_dtheta = jax.jacobian(f, argnums=0)(Θ_opt, p_opt).reshape(-1)\n",
    "    df_dp = jax.jacobian(f, argnums=1)(Θ_opt, p_opt).reshape(-1)\n",
    "\n",
    "    # Active bound derivatives (Θ[0] - b_theta = 0)\n",
    "    dtheta_active_dtheta = jax.jacobian(Θ_active, argnums=0)(Θ_opt).reshape(n_lambda_theta, n_theta)\n",
    "\n",
    "    # Active constraint derivatives\n",
    "    dg_active_dtheta = jax.jacobian(g_active, argnums=0)(Θ_opt, p_opt).reshape(n_lambda_g, n_theta)\n",
    "    dg_active_dp = jax.jacobian(g_active, argnums=1)(Θ_opt, p_opt).reshape(n_lambda_g, n_p)\n",
    "\n",
    "    # Lagrangian gradient: ∇L = ∇f + λ_theta^T ∇Θ_active + λ_g^T ∇g_active\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    # Mixed derivatives of Lagrangian (still needed for the p block)\n",
    "    d2L_dtheta_dp = jax.jacobian(jax.grad(lagrangian, argnums=0), argnums=1)(\n",
    "        Θ_opt, p_opt, λ_theta_opt, λ_g_opt\n",
    "    )\n",
    "\n",
    "    def matvec_transpose(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u]^T @ v\n",
    "        This implements the transpose of the UDE Jacobian matrix.\n",
    "\n",
    "        KEY: When v corresponds to an identity output seed (v_f_theta nonzero),\n",
    "        we avoid the Hessian computation entirely!\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_outputs = v[idx:idx+n_outputs_total]\n",
    "        v_f = v_outputs[:n_f]\n",
    "        v_f_theta = v_outputs[n_f:]\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Effect on p\n",
    "        result[:n_p] = v_p\n",
    "        if np.any(v_theta):\n",
    "            result[:n_p] -= d2L_dtheta_dp.T @ v_theta\n",
    "        if np.any(v_lambda_g):\n",
    "            result[:n_p] -= dg_active_dp.T @ v_lambda_g\n",
    "        if np.any(v_f):\n",
    "            result[:n_p] -= df_dp * v_f[0]\n",
    "\n",
    "        # Block 2: Effect on b_theta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta + v_lambda_theta\n",
    "\n",
    "        # Block 3: Effect on b_g\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg + v_lambda_g\n",
    "\n",
    "        # Block 4: Effect on theta\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "\n",
    "        # CRITICAL: Only compute Hessian term if v_theta is nonzero\n",
    "        # When solving for θ sensitivities via identity outputs, v_theta = 0!\n",
    "        # if np.any(v_theta):\n",
    "        #     # We would need Hessian here, but for identity output solves this is zero!\n",
    "        #     raise ValueError(\"Hessian computation required - this shouldn't happen for identity outputs!\")\n",
    "\n",
    "        # print(f'{v_p=}')\n",
    "        # print(f'{v_btheta=}')\n",
    "        # print(f'{v_bg=}')\n",
    "        print(f'{v_theta=}')\n",
    "        # print(f'{v_lambda_theta=}')\n",
    "        # print(f'{v_lambda_g=}')\n",
    "        # print(f'{v_outputs=}')\n",
    "        # print(f'{v_f=}')\n",
    "        print(f'{v_f_theta=}')\n",
    "        # print()\n",
    "\n",
    "\n",
    "        if np.any(v_lambda_theta):\n",
    "            result_theta -= dtheta_active_dtheta.T @ v_lambda_theta\n",
    "        if np.any(v_lambda_g):\n",
    "            result_theta -= dg_active_dtheta.T @ v_lambda_g\n",
    "        if np.any(v_f):\n",
    "            result_theta -= df_dtheta * v_f[0]\n",
    "\n",
    "        # Identity outputs contribution (avoids Hessian!)\n",
    "        if np.any(v_f_theta):\n",
    "            result_theta -= v_f_theta  # -I @ v_f_theta\n",
    "\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Effect on lambda_theta\n",
    "        idx += n_theta\n",
    "        if np.any(v_theta):\n",
    "            result[idx:idx+n_lambda_theta] = -dtheta_active_dtheta @ v_theta\n",
    "\n",
    "        # Block 6: Effect on lambda_g\n",
    "        idx += n_lambda_theta\n",
    "        if np.any(v_theta):\n",
    "            result[idx:idx+n_lambda_g] = -dg_active_dtheta @ v_theta\n",
    "\n",
    "        # Block 7: Effect on outputs\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_outputs_total] = v_outputs\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator\n",
    "    A_transpose = LinearOperator((total_size, total_size), matvec=matvec_transpose)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # Solve for each output\n",
    "    output_names = ['f', 'θ₀', 'θ₁']\n",
    "\n",
    "    print(\"\\nNOTE: Computing sensitivities WITHOUT computing the Hessian!\")\n",
    "    print(\"The identity output approach allows us to skip ∇²L entirely.\\n\")\n",
    "\n",
    "    for i, name in enumerate(output_names):\n",
    "        # Create RHS vector with 1 in appropriate output position\n",
    "        rhs = np.zeros(total_size)\n",
    "        output_start_idx = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g\n",
    "        rhs[output_start_idx + i] = 1.0\n",
    "\n",
    "        # Check that we're not triggering Hessian computation\n",
    "        # For identity outputs (i > 0), v_theta should always be zero in the iteration\n",
    "        is_identity_output = (i > 0)\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities of {name}...\")\n",
    "        if is_identity_output:\n",
    "            print(f\"  (Using identity output - no Hessian needed!)\")\n",
    "\n",
    "        solution, info = gmres(A_transpose, rhs, rtol=1e-10, maxiter=1000)\n",
    "        print(solution)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities\n",
    "            sens_p = solution[:n_p]\n",
    "            sens_btheta = solution[n_p:n_p+n_btheta]\n",
    "            sens_bg = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "\n",
    "            sensitivities[name] = {\n",
    "                'wrt_p': sens_p,\n",
    "                'wrt_btheta': sens_btheta,\n",
    "                'wrt_bg': sens_bg\n",
    "            }\n",
    "\n",
    "            print(f\"  d{name}/dp₀ = {sens_p[0]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₁ = {sens_p[1]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₂ = {sens_p[2]:+.6f}\")\n",
    "            print(f\"  d{name}/db_θ₀ = {sens_btheta[0]:+.6f}\")\n",
    "            print(f\"  d{name}/db_g₀ = {sens_bg[0]:+.6f}\")\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    # For simplicity, we'll verify df/dp₀ and dθ₀/db_θ₀\n",
    "\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "    # Additional verification: dθ₁/db_θ₀ (should be -1.0 due to equality constraint)\n",
    "    dtheta1_dbtheta_fd = (theta_plus[1] - theta_minus[1]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₁/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₁']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta1_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₁']['wrt_btheta'][0] - dtheta1_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"WITHOUT Computing the Hessian of the Lagrangian\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nKey insight: By using identity outputs for θ₀ and θ₁,\")\n",
    "    print(\"we completely avoid computing the expensive Hessian ∇²L.\")\n",
    "    print(\"\\nThe trick is that when solving for θ sensitivities,\")\n",
    "    print(\"the RHS vector has zeros in the θ residual positions,\")\n",
    "    print(\"so v_theta = 0 throughout the GMRES iteration,\")\n",
    "    print(\"eliminating the need for Hessian-vector products!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The All-JVP Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "with Jacobian-Vector Products for All Operations\n",
      "============================================================\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Solving for sensitivities of f...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[ 0.79471941  0.52981294  0.13245324  0.          0.          0.\n",
      " -0.26490647 -0.         -0.          0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.05448525  0.48128646 -0.00908087  0.          0.          0.258805\n",
      "  0.79457674  0.          0.25880498  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.03587375  0.11430935 -0.08025979  0.          0.25871623 -0.8171906\n",
      "  0.29611005 -0.25871625 -0.29975822  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.20882484  0.26031975 -0.00167373 -0.35466231  0.6099331   0.17641066\n",
      " -0.10667187  0.45405355 -0.37703453  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.28802468  0.28758354  0.09178891  0.72515838  0.35945893  0.13373641\n",
      " -0.24301251 -0.30577933  0.02013409  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.38021066 -0.41856288  0.04395775  0.17200982  0.18615408  0.40225341\n",
      "  0.32548512 -0.23024192 -0.5415831   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000007e+00\n",
      "  1.99999992e+00 -6.35602682e-15  4.44089210e-15  2.00000007e+00\n",
      " -1.99999992e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.61311682e-08 -5.37705608e-09 -2.41967524e-08 -2.68852804e-08\n",
      " -4.03279206e-09  8.89647789e-01  4.56647360e-01 -3.84795576e-08\n",
      " -1.15942772e-08 -1.34426402e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 6.75226000e-01 -3.46586812e-01 -3.55182898e-08 -5.35005812e-08\n",
      " -1.02346121e-08  1.01020749e-01 -1.96810283e-01 -3.37613054e-01\n",
      " -5.10906427e-01 -1.94489745e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 5.37839726e-01  2.12417028e-01  6.78784366e-09 -3.31610853e-01\n",
      " -5.01823361e-01 -1.96351919e-01  3.82536003e-01  6.26910105e-02\n",
      "  3.39112017e-01  4.97476364e-10  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-7.66510273e-02 -4.39298683e-01  7.89752809e-09 -4.12020335e-01\n",
      " -3.20240517e-01  2.52900730e-01 -4.92705296e-01  4.50345873e-01\n",
      "  1.38916696e-01  5.66154910e-10  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.48818591e-01 -3.92368368e-01 -2.13592695e-07  5.09220882e-01\n",
      " -4.89536283e-01  1.15383836e-02 -2.24792775e-02 -4.34812227e-01\n",
      "  3.67761181e-01 -6.36739488e-12  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 1.73150196e-01  6.23904914e-01 -2.00484329e-02  8.11311584e-02\n",
      " -4.38269452e-02  3.08992615e-01 -6.01984415e-01 -2.27851558e-01\n",
      "  2.49155889e-01 -5.38847987e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000000e+00\n",
      "  2.00000000e+00 -1.91905902e-22  1.25731453e-22  2.00000000e+00\n",
      " -2.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  df/dp₀ = -6.000000\n",
      "  df/dp₁ = -4.000000\n",
      "  df/dp₂ = -1.000000\n",
      "  df/db_θ₀ = -2.000000\n",
      "  df/db_g₀ = +2.000000\n",
      "\n",
      "Solving for sensitivities of θ₀...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[ 0.  0.  0.  0.  0. -1.  0. -0. -0.  0.  0.  0.]\n",
      "using hessian-vector product!\n",
      "[-0.75592895  0.          0.          0.          0.          0.\n",
      "  0.37796447  0.37796447  0.37796447  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-4.47213594e-01 -4.47213583e-01  0.00000000e+00  2.23606793e-01\n",
      "  2.23606793e-01  0.00000000e+00 -6.70820410e-01  4.42988541e-09\n",
      " -2.23606787e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-0.18351289  0.3408097   0.          0.61607894  0.35391769  0.\n",
      "  0.24905312 -0.5243225  -0.0917564   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.03310596  0.40554808  0.         -0.34761234  0.6372892   0.\n",
      " -0.01241506  0.36416532 -0.41796218  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.05279402  0.64672708  0.          0.01319864 -0.11878706  0.\n",
      " -0.58733402  0.01319837  0.46854761  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 4.16333634e-17 -5.55111512e-17  0.00000000e+00  1.00000000e+00\n",
      " -4.19637103e-09 -9.25185856e-18 -2.77555756e-17 -1.00000000e+00\n",
      "  4.19637097e-09  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-4.92992270e-09  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.18318145e-08  4.47213600e-01  8.94427189e-01 -1.97196909e-09\n",
      " -7.88787634e-09  0.00000000e+00  2.36636290e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 3.54663444e-01 -7.09326894e-01  0.00000000e+00 -7.81936599e-10\n",
      "  1.47004085e-08 -2.12798093e-01  1.06399045e-01 -1.77331728e-01\n",
      " -5.31995181e-01  0.00000000e+00  3.56563098e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.62885602e-01 -5.28035744e-01  0.00000000e+00 -1.42301147e-01\n",
      " -4.26903439e-01  5.53095049e-01 -2.76547521e-01  2.23743948e-01\n",
      "  2.44328368e-01  0.00000000e+00  1.89426842e-10  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 5.37599216e-01  1.11092167e-01  0.00000000e+00 -5.63615169e-02\n",
      " -6.26422518e-01 -2.64159434e-01  1.32079733e-01 -2.12438089e-01\n",
      "  4.13168993e-01  0.00000000e+00 -1.03758400e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 4.03041926e-01  2.00184169e-01  0.00000000e+00 -7.00143262e-01\n",
      "  1.01261763e-01  7.65311956e-02 -3.82655981e-02  4.98622299e-01\n",
      " -2.02690642e-01  0.00000000e+00 -9.46055986e-10  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 4.53006766e-01  2.53958342e-01  0.00000000e+00  1.92184614e-01\n",
      "  1.02956148e-01  6.10872822e-01 -3.05436382e-01 -4.18688286e-01\n",
      " -2.02480290e-01  0.00000000e+00  1.42940670e-06  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-6.20385459e-25 -2.06795153e-25  0.00000000e+00  1.00000000e+00\n",
      " -2.12837443e-17  2.56984237e-25 -2.58493941e-25 -1.00000000e+00\n",
      "  2.12837443e-17  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  dθ₀/dp₀ = -0.000000\n",
      "  dθ₀/dp₁ = -0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₀/db_g₀ = -0.000000\n",
      "\n",
      "Solving for sensitivities of θ₁...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[ 0.  0.  0.  0.  0.  0. -1. -0. -0.  0.  0.  0.]\n",
      "using hessian-vector product!\n",
      "[ 0.          0.81649658  0.          0.          0.          0.40824829\n",
      "  0.         -0.          0.40824829  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.4472136   0.44721359  0.          0.          0.2236068  -0.67082039\n",
      "  0.         -0.2236068  -0.2236068   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.21622499  0.14414999  0.         -0.36037499  0.61263747  0.14415\n",
      "  0.          0.46848748 -0.43244998  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.58556665  0.07060024  0.          0.6229431   0.37999528 -0.08928864\n",
      "  0.         -0.33015978 -0.05191184  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.46834518  0.08028775  0.         -0.18064761 -0.16057562 -0.5954676\n",
      "  0.          0.4148202   0.43489208  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 5.55111512e-17  5.55111512e-17  0.00000000e+00 -9.99999925e-01\n",
      "  1.00000008e+00  5.55111512e-17 -5.39719284e-17  9.99999925e-01\n",
      " -1.00000008e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-9.88294907e-10 -9.70025832e-10  0.00000000e+00  1.31772654e-09\n",
      "  0.00000000e+00 -8.94427190e-01 -4.47213597e-01  3.29431636e-10\n",
      "  9.13453760e-12  0.00000000e+00  0.00000000e+00  1.97658981e-09]\n",
      "using hessian-vector product!\n",
      "[-6.78063508e-01  3.39031753e-01  0.00000000e+00  2.02290596e-09\n",
      "  3.46243756e-12 -1.01709516e-01  2.03419032e-01  3.39031753e-01\n",
      "  5.08547630e-01  0.00000000e+00  0.00000000e+00  2.84705283e-09]\n",
      "using hessian-vector product!\n",
      "[-5.39016596e-01 -2.22391439e-01  0.00000000e+00  3.27933191e-01\n",
      "  4.91899787e-01  1.97890722e-01 -3.95781440e-01 -5.84248932e-02\n",
      " -3.33587208e-01  0.00000000e+00  0.00000000e+00 -7.08038328e-10]\n",
      "using hessian-vector product!\n",
      "[ 6.66757756e-02  4.36849753e-01  0.00000000e+00  4.19069486e-01\n",
      "  3.28933765e-01 -2.43119581e-01  4.86239161e-01 -4.52407374e-01\n",
      " -1.43846776e-01  0.00000000e+00  0.00000000e+00 -8.54063429e-10]\n",
      "using hessian-vector product!\n",
      "[ 1.43631227e-01  3.91764875e-01  0.00000000e+00 -5.06288242e-01\n",
      "  4.93642922e-01 -1.02594027e-02  2.05188030e-02  4.34472629e-01\n",
      " -3.69576098e-01  0.00000000e+00  0.00000000e+00  7.38810512e-12]\n",
      "using hessian-vector product!\n",
      "[-1.83314384e-01 -6.24626713e-01  0.00000000e+00 -1.05235902e-01\n",
      "  4.07365174e-02 -3.02129202e-01  6.04258363e-01  1.96893171e-01\n",
      " -2.61392621e-01  0.00000000e+00  0.00000000e+00  4.88514852e-08]\n",
      "using hessian-vector product!\n",
      "[ 1.32348898e-23  2.64697796e-23  0.00000000e+00 -1.00000000e+00\n",
      "  1.00000000e+00 -1.98523347e-23  1.17746045e-23  1.00000000e+00\n",
      " -1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  dθ₁/dp₀ = +0.000000\n",
      "  dθ₁/dp₁ = +0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results\n",
      "============================================================\n",
      "\n",
      "Note: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\n",
      "- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\n",
      "- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\n",
      "- The objective is most sensitive to p₀ (df/dp₀ = -6)\n",
      "\n",
      "Advantages of Jacobian-Vector Product approach:\n",
      "- Memory efficient: O(n) operations, no matrix storage\n",
      "- Automatic differentiation ensures exact derivatives\n",
      "- Scales linearly with problem size\n",
      "- Numerically stable and accurate\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with all matrix-vector products computed using Jacobian-vector products.\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    Uses Jacobian-vector products for all matrix-vector operations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "    n_f_theta = 2  # identity outputs for design variables\n",
    "    n_outputs_total = n_f + n_f_theta  # total outputs\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_outputs_total\n",
    "\n",
    "    # Define Lagrangian and its gradient\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Lagrangian function\"\"\"\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    def lagrangian_grad(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Gradient of Lagrangian w.r.t. Θ\"\"\"\n",
    "        return jax.grad(lagrangian, argnums=0)(Θ, p, λ_theta, λ_g)\n",
    "\n",
    "    # Function to compute Hessian-vector product using finite differences\n",
    "    def hessian_vector_product(v_theta, h=1e-8):\n",
    "        \"\"\"\n",
    "        Compute H @ v where H is the Hessian of the Lagrangian w.r.t. Θ\n",
    "        Uses finite differences: H @ v ≈ (∇L(Θ + h*v) - ∇L(Θ - h*v)) / (2*h)\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Compute gradients at perturbed points\n",
    "        grad_plus = lagrangian_grad(Θ_opt + h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "        grad_minus = lagrangian_grad(Θ_opt - h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "\n",
    "        # Finite difference approximation of Hessian-vector product\n",
    "        hvp = (grad_plus - grad_minus) / (2 * h)\n",
    "\n",
    "        return np.array(hvp)\n",
    "\n",
    "    # JVP functions for matrix-transpose-vector products\n",
    "    def jvp_d2L_dtheta_dp_T(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂²L/∂θ∂p]^T @ v_theta using JVP\n",
    "        This is equivalent to ∂/∂p [∇_θ L^T @ v_theta]\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        def scalar_func(p):\n",
    "            grad_L = lagrangian_grad(Θ_opt, p, λ_theta_opt, λ_g_opt)\n",
    "            return jnp.dot(grad_L, v_theta_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(p_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    def jvp_dg_active_dp_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂p]^T @ v_lambda_g using JVP\n",
    "        This is equivalent to ∂/∂p [g_active^T @ v_lambda_g]\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        def scalar_func(p):\n",
    "            g_vals = g_active(Θ_opt, p)\n",
    "            return jnp.dot(g_vals, v_lambda_g_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(p_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    def jvp_df_dp_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂p]^T @ v_f using JVP\n",
    "        This is equivalent to ∂/∂p [f^T @ v_f]\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        def scalar_func(p):\n",
    "            f_vals = f(Θ_opt, p)\n",
    "            return jnp.dot(f_vals, v_f_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(p_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    def jvp_dtheta_active_dtheta_T(v_lambda_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ]^T @ v_lambda_theta using JVP\n",
    "        This is equivalent to ∂/∂θ [Θ_active^T @ v_lambda_theta]\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_theta_jax = jnp.array(v_lambda_theta)\n",
    "\n",
    "        def scalar_func(theta):\n",
    "            theta_active_vals = Θ_active(theta)\n",
    "            return jnp.dot(theta_active_vals, v_lambda_theta_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(Θ_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    def jvp_dg_active_dtheta_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ]^T @ v_lambda_g using JVP\n",
    "        This is equivalent to ∂/∂θ [g_active^T @ v_lambda_g]\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        def scalar_func(theta):\n",
    "            g_vals = g_active(theta, p_opt)\n",
    "            return jnp.dot(g_vals, v_lambda_g_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(Θ_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    def jvp_df_dtheta_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂θ]^T @ v_f using JVP\n",
    "        This is equivalent to ∂/∂θ [f^T @ v_f]\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        def scalar_func(theta):\n",
    "            f_vals = f(theta, p_opt)\n",
    "            return jnp.dot(f_vals, v_f_jax)\n",
    "\n",
    "        result = jax.grad(scalar_func)(Θ_opt)\n",
    "        return np.array(result)\n",
    "\n",
    "    # Regular JVP functions for forward matrix-vector products\n",
    "    def jvp_dtheta_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(Θ_active, (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dg_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_g)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(lambda theta: g_active(theta, p_opt), (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def matvec_transpose(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u]^T @ v\n",
    "        This implements the transpose of the UDE Jacobian matrix.\n",
    "        All operations use JVPs instead of explicit matrix formation.\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_outputs = v[idx:idx+n_outputs_total]\n",
    "        v_f = v_outputs[:n_f]\n",
    "        v_f_theta = v_outputs[n_f:]\n",
    "\n",
    "        print(v)\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Effect on p\n",
    "        result[:n_p] = v_p\n",
    "        result[:n_p] -= jvp_d2L_dtheta_dp_T(v_theta)\n",
    "        result[:n_p] -= jvp_dg_active_dp_T(v_lambda_g)\n",
    "        result[:n_p] -= jvp_df_dp_T(v_f)\n",
    "\n",
    "        # Block 2: Effect on b_theta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta + v_lambda_theta\n",
    "\n",
    "        # Block 3: Effect on b_g\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg + v_lambda_g\n",
    "\n",
    "        # Block 4: Effect on theta\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "        if np.any(v_theta):\n",
    "            print('using hessian-vector product!')\n",
    "            # Use Hessian-vector product instead of full Hessian\n",
    "            result_theta -= hessian_vector_product(v_theta)\n",
    "        result_theta -= jvp_dtheta_active_dtheta_T(v_lambda_theta)\n",
    "        result_theta -= jvp_dg_active_dtheta_T(v_lambda_g)\n",
    "        result_theta -= jvp_df_dtheta_T(v_f)\n",
    "        # Identity outputs contribution\n",
    "        if np.any(v_f_theta):\n",
    "            result_theta -= v_f_theta  # -I @ v_f_theta\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Effect on lambda_theta\n",
    "        idx += n_theta\n",
    "        result[idx:idx+n_lambda_theta] = -jvp_dtheta_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 6: Effect on lambda_g\n",
    "        idx += n_lambda_theta\n",
    "        result[idx:idx+n_lambda_g] = -jvp_dg_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 7: Effect on outputs\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_outputs_total] = v_outputs\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator\n",
    "    A_transpose = LinearOperator((total_size, total_size), matvec=matvec_transpose)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # Solve for each output\n",
    "    output_names = ['f', 'θ₀', 'θ₁']\n",
    "\n",
    "    for i, name in enumerate(output_names):\n",
    "        # Create RHS vector with 1 in appropriate output position\n",
    "        rhs = np.zeros(total_size)\n",
    "        output_start_idx = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g\n",
    "        rhs[output_start_idx + i] = 1.0\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities of {name}...\")\n",
    "        solution, info = gmres(A_transpose, rhs, rtol=1e-10, maxiter=1000)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities\n",
    "            sens_p = solution[:n_p]\n",
    "            sens_btheta = solution[n_p:n_p+n_btheta]\n",
    "            sens_bg = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "\n",
    "            sensitivities[name] = {\n",
    "                'wrt_p': sens_p,\n",
    "                'wrt_btheta': sens_btheta,\n",
    "                'wrt_bg': sens_bg\n",
    "            }\n",
    "\n",
    "            print(f\"  d{name}/dp₀ = {sens_p[0]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₁ = {sens_p[1]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₂ = {sens_p[2]:+.6f}\")\n",
    "            print(f\"  d{name}/db_θ₀ = {sens_btheta[0]:+.6f}\")\n",
    "            print(f\"  d{name}/db_g₀ = {sens_bg[0]:+.6f}\")\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    # For simplicity, we'll verify df/dp₀ and dθ₀/db_θ₀\n",
    "\n",
    "    # Verify df/dp₀\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"with Jacobian-Vector Products for All Operations\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary of Key Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\")\n",
    "    print(\"- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\")\n",
    "    print(\"- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\")\n",
    "    print(\"- The objective is most sensitive to p₀ (df/dp₀ = -6)\")\n",
    "    print(\"\\nAdvantages of Jacobian-Vector Product approach:\")\n",
    "    print(\"- Memory efficient: O(n) operations, no matrix storage\")\n",
    "    print(\"- Automatic differentiation ensures exact derivatives\")\n",
    "    print(\"- Scales linearly with problem size\")\n",
    "    print(\"- Numerically stable and accurate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The All-VJP way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Post-Optimality Sensitivity Analysis using JAX\n",
      "with Vector-Jacobian Products (VJPs)\n",
      "============================================================\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Solving for sensitivities of f...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[ 0.79471941  0.52981294  0.13245324  0.          0.          0.\n",
      " -0.26490647 -0.         -0.          0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.05448525  0.48128646 -0.00908087  0.          0.          0.258805\n",
      "  0.79457674  0.          0.25880498  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.03587375  0.11430935 -0.08025979  0.          0.25871623 -0.8171906\n",
      "  0.29611005 -0.25871625 -0.29975822  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.20882484  0.26031975 -0.00167373 -0.35466231  0.6099331   0.17641066\n",
      " -0.10667187  0.45405355 -0.37703453  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.28802468  0.28758354  0.09178891  0.72515838  0.35945893  0.13373641\n",
      " -0.24301251 -0.30577933  0.02013409  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.38021066 -0.41856288  0.04395775  0.17200982  0.18615408  0.40225341\n",
      "  0.32548512 -0.23024192 -0.5415831   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000007e+00\n",
      "  1.99999992e+00 -6.35602682e-15  4.44089210e-15  2.00000007e+00\n",
      " -1.99999992e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.61311682e-08 -5.37705608e-09 -2.41967524e-08 -2.68852804e-08\n",
      " -4.03279206e-09  8.89647789e-01  4.56647360e-01 -3.84795576e-08\n",
      " -1.15942772e-08 -1.34426402e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 6.75226000e-01 -3.46586812e-01 -3.55182898e-08 -5.35005812e-08\n",
      " -1.02346121e-08  1.01020749e-01 -1.96810283e-01 -3.37613054e-01\n",
      " -5.10906427e-01 -1.94489745e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 5.37839726e-01  2.12417028e-01  6.78784366e-09 -3.31610853e-01\n",
      " -5.01823361e-01 -1.96351919e-01  3.82536003e-01  6.26910105e-02\n",
      "  3.39112017e-01  4.97476364e-10  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-7.66510273e-02 -4.39298683e-01  7.89752809e-09 -4.12020335e-01\n",
      " -3.20240517e-01  2.52900730e-01 -4.92705296e-01  4.50345873e-01\n",
      "  1.38916696e-01  5.66154910e-10  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.48818591e-01 -3.92368368e-01 -2.13592695e-07  5.09220882e-01\n",
      " -4.89536283e-01  1.15383836e-02 -2.24792775e-02 -4.34812227e-01\n",
      "  3.67761181e-01 -6.36739488e-12  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 1.73150196e-01  6.23904914e-01 -2.00484329e-02  8.11311584e-02\n",
      " -4.38269452e-02  3.08992615e-01 -6.01984415e-01 -2.27851558e-01\n",
      "  2.49155889e-01 -5.38847987e-09  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-6.00000000e+00 -4.00000000e+00 -1.00000000e+00 -2.00000000e+00\n",
      "  2.00000000e+00 -1.91905902e-22  1.25731453e-22  2.00000000e+00\n",
      " -2.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  df/dp₀ = -6.000000\n",
      "  df/dp₁ = -4.000000\n",
      "  df/dp₂ = -1.000000\n",
      "  df/db_θ₀ = -2.000000\n",
      "  df/db_g₀ = +2.000000\n",
      "\n",
      "Solving for sensitivities of θ₀...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "[ 0.  0.  0.  0.  0. -1.  0. -0. -0.  0.  0.  0.]\n",
      "using hessian-vector product!\n",
      "[-0.75592895  0.          0.          0.          0.          0.\n",
      "  0.37796447  0.37796447  0.37796447  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-4.47213594e-01 -4.47213583e-01  0.00000000e+00  2.23606793e-01\n",
      "  2.23606793e-01  0.00000000e+00 -6.70820410e-01  4.42988541e-09\n",
      " -2.23606787e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-0.18351289  0.3408097   0.          0.61607894  0.35391769  0.\n",
      "  0.24905312 -0.5243225  -0.0917564   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.03310596  0.40554808  0.         -0.34761234  0.6372892   0.\n",
      " -0.01241506  0.36416532 -0.41796218  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.05279402  0.64672708  0.          0.01319864 -0.11878706  0.\n",
      " -0.58733402  0.01319837  0.46854761  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 4.16333634e-17 -5.55111512e-17  0.00000000e+00  1.00000000e+00\n",
      " -4.19637103e-09 -9.25185856e-18 -2.77555756e-17 -1.00000000e+00\n",
      "  4.19637097e-09  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-4.92992270e-09  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.18318145e-08  4.47213600e-01  8.94427189e-01 -1.97196909e-09\n",
      " -7.88787634e-09  0.00000000e+00  2.36636290e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 3.54663444e-01 -7.09326894e-01  0.00000000e+00 -7.81936599e-10\n",
      "  1.47004085e-08 -2.12798093e-01  1.06399045e-01 -1.77331728e-01\n",
      " -5.31995181e-01  0.00000000e+00  3.56563098e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-1.62885602e-01 -5.28035744e-01  0.00000000e+00 -1.42301147e-01\n",
      " -4.26903439e-01  5.53095049e-01 -2.76547521e-01  2.23743948e-01\n",
      "  2.44328368e-01  0.00000000e+00  1.89426842e-10  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 5.37599216e-01  1.11092167e-01  0.00000000e+00 -5.63615169e-02\n",
      " -6.26422518e-01 -2.64159434e-01  1.32079733e-01 -2.12438089e-01\n",
      "  4.13168993e-01  0.00000000e+00 -1.03758400e-08  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 4.03041926e-01  2.00184169e-01  0.00000000e+00 -7.00143262e-01\n",
      "  1.01261763e-01  7.65311956e-02 -3.82655981e-02  4.98622299e-01\n",
      " -2.02690642e-01  0.00000000e+00 -9.46055986e-10  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[ 4.53006766e-01  2.53958342e-01  0.00000000e+00  1.92184614e-01\n",
      "  1.02956148e-01  6.10872822e-01 -3.05436382e-01 -4.18688286e-01\n",
      " -2.02480290e-01  0.00000000e+00  1.42940670e-06  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-6.20385459e-25 -2.06795153e-25  0.00000000e+00  1.00000000e+00\n",
      " -2.12837443e-17  2.56984237e-25 -2.58493941e-25 -1.00000000e+00\n",
      "  2.12837443e-17  0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  dθ₀/dp₀ = -0.000000\n",
      "  dθ₀/dp₁ = -0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/db_θ₀ = +1.000000\n",
      "  dθ₀/db_g₀ = -0.000000\n",
      "\n",
      "Solving for sensitivities of θ₁...\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[ 0.  0.  0.  0.  0.  0. -1. -0. -0.  0.  0.  0.]\n",
      "using hessian-vector product!\n",
      "[ 0.          0.81649658  0.          0.          0.          0.40824829\n",
      "  0.         -0.          0.40824829  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 0.4472136   0.44721359  0.          0.          0.2236068  -0.67082039\n",
      "  0.         -0.2236068  -0.2236068   0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.21622499  0.14414999  0.         -0.36037499  0.61263747  0.14415\n",
      "  0.          0.46848748 -0.43244998  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.58556665  0.07060024  0.          0.6229431   0.37999528 -0.08928864\n",
      "  0.         -0.33015978 -0.05191184  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[-0.46834518  0.08028775  0.         -0.18064761 -0.16057562 -0.5954676\n",
      "  0.          0.4148202   0.43489208  0.          0.          0.        ]\n",
      "using hessian-vector product!\n",
      "[ 5.55111512e-17  5.55111512e-17  0.00000000e+00 -9.99999925e-01\n",
      "  1.00000008e+00  5.55111512e-17 -5.39719284e-17  9.99999925e-01\n",
      " -1.00000008e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "using hessian-vector product!\n",
      "[-9.88294907e-10 -9.70025832e-10  0.00000000e+00  1.31772654e-09\n",
      "  0.00000000e+00 -8.94427190e-01 -4.47213597e-01  3.29431636e-10\n",
      "  9.13453760e-12  0.00000000e+00  0.00000000e+00  1.97658981e-09]\n",
      "using hessian-vector product!\n",
      "[-6.78063508e-01  3.39031753e-01  0.00000000e+00  2.02290596e-09\n",
      "  3.46243756e-12 -1.01709516e-01  2.03419032e-01  3.39031753e-01\n",
      "  5.08547630e-01  0.00000000e+00  0.00000000e+00  2.84705283e-09]\n",
      "using hessian-vector product!\n",
      "[-5.39016596e-01 -2.22391439e-01  0.00000000e+00  3.27933191e-01\n",
      "  4.91899787e-01  1.97890722e-01 -3.95781440e-01 -5.84248932e-02\n",
      " -3.33587208e-01  0.00000000e+00  0.00000000e+00 -7.08038328e-10]\n",
      "using hessian-vector product!\n",
      "[ 6.66757756e-02  4.36849753e-01  0.00000000e+00  4.19069486e-01\n",
      "  3.28933765e-01 -2.43119581e-01  4.86239161e-01 -4.52407374e-01\n",
      " -1.43846776e-01  0.00000000e+00  0.00000000e+00 -8.54063429e-10]\n",
      "using hessian-vector product!\n",
      "[ 1.43631227e-01  3.91764875e-01  0.00000000e+00 -5.06288242e-01\n",
      "  4.93642922e-01 -1.02594027e-02  2.05188030e-02  4.34472629e-01\n",
      " -3.69576098e-01  0.00000000e+00  0.00000000e+00  7.38810512e-12]\n",
      "using hessian-vector product!\n",
      "[-1.83314384e-01 -6.24626713e-01  0.00000000e+00 -1.05235902e-01\n",
      "  4.07365174e-02 -3.02129202e-01  6.04258363e-01  1.96893171e-01\n",
      " -2.61392621e-01  0.00000000e+00  0.00000000e+00  4.88514852e-08]\n",
      "using hessian-vector product!\n",
      "[ 1.32348898e-23  2.64697796e-23  0.00000000e+00 -1.00000000e+00\n",
      "  1.00000000e+00 -1.98523347e-23  1.17746045e-23  1.00000000e+00\n",
      " -1.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "using hessian-vector product!\n",
      "  dθ₁/dp₀ = +0.000000\n",
      "  dθ₁/dp₁ = +0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/db_θ₀ = -1.000000\n",
      "  dθ₁/db_g₀ = +1.000000\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "\n",
      "df/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  8.39e-10\n",
      "\n",
      "dθ₀/db_θ₀:\n",
      "  UDE:    +1.000000\n",
      "  FD:     +1.000000\n",
      "  Error:  1.40e-10\n",
      "\n",
      "============================================================\n",
      "Summary of Key Results\n",
      "============================================================\n",
      "\n",
      "Note: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\n",
      "- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\n",
      "- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\n",
      "- The objective is most sensitive to p₀ (df/dp₀ = -6)\n",
      "\n",
      "Advantages of Vector-Jacobian Product (VJP) approach:\n",
      "- Memory efficient: O(n) operations, no matrix storage\n",
      "- VJPs are the natural choice for A^T @ v operations\n",
      "- Automatic differentiation ensures exact derivatives\n",
      "- Scales linearly with problem size\n",
      "- More efficient than the JVP approach for transpose operations\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using JAX and the UDE approach\n",
    "with all matrix-vector products computed using Vector-Jacobian Products (VJPs).\n",
    "\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import LinearOperator, gmres\n",
    "from functools import partial\n",
    "\n",
    "# Define the optimization problem functions\n",
    "def f(Θ, p):\n",
    "    \"\"\"Objective function\"\"\"\n",
    "    f_val = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return jnp.array([f_val])\n",
    "\n",
    "def Θ_active(Θ):\n",
    "    \"\"\"Active bound constraint on Θ[0]\"\"\"\n",
    "    return jnp.array([Θ[0]])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    \"\"\"Active equality constraint\"\"\"\n",
    "    return jnp.array([Θ[0] + Θ[1]])\n",
    "\n",
    "def compute_sensitivities_jax():\n",
    "    \"\"\"\n",
    "    Compute post-optimality sensitivities using JAX and the UDE approach.\n",
    "    Uses Vector-Jacobian Products (VJPs) for all matrix-transpose-vector operations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Known optimal solution\n",
    "    Θ_opt = jnp.array([6.0, -6.0])\n",
    "    p_opt = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_opt = jnp.array([6.0])\n",
    "    b_g_opt = jnp.array([0.0])\n",
    "    λ_theta_opt = jnp.array([2.0])\n",
    "    λ_g_opt = jnp.array([-2.0])\n",
    "\n",
    "    # Problem dimensions\n",
    "    n_p = 3  # parameters\n",
    "    n_btheta = 1  # active bounds on design vars\n",
    "    n_bg = 1  # active constraint bounds\n",
    "    n_theta = 2  # design variables\n",
    "    n_lambda_theta = 1  # multipliers for active bounds\n",
    "    n_lambda_g = 1  # multipliers for active constraints\n",
    "    n_f = 1  # original objective\n",
    "    n_f_theta = 2  # identity outputs for design variables\n",
    "    n_outputs_total = n_f + n_f_theta  # total outputs\n",
    "\n",
    "    total_size = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g + n_outputs_total\n",
    "\n",
    "    # Define Lagrangian and its gradient\n",
    "    def lagrangian(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Lagrangian function\"\"\"\n",
    "        return f(Θ, p)[0] + λ_theta @ Θ_active(Θ) + λ_g @ g_active(Θ, p)\n",
    "\n",
    "    def lagrangian_grad(Θ, p, λ_theta, λ_g):\n",
    "        \"\"\"Gradient of Lagrangian w.r.t. Θ\"\"\"\n",
    "        return jax.grad(lagrangian, argnums=0)(Θ, p, λ_theta, λ_g)\n",
    "\n",
    "    # Function to compute Hessian-vector product using finite differences\n",
    "    def hessian_vector_product(v_theta, h=1e-8):\n",
    "        \"\"\"\n",
    "        Compute H @ v where H is the Hessian of the Lagrangian w.r.t. Θ\n",
    "        Uses finite differences: H @ v ≈ (∇L(Θ + h*v) - ∇L(Θ - h*v)) / (2*h)\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Compute gradients at perturbed points\n",
    "        grad_plus = lagrangian_grad(Θ_opt + h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "        grad_minus = lagrangian_grad(Θ_opt - h * v_theta_jax, p_opt, λ_theta_opt, λ_g_opt)\n",
    "\n",
    "        # Finite difference approximation of Hessian-vector product\n",
    "        hvp = (grad_plus - grad_minus) / (2 * h)\n",
    "\n",
    "        return np.array(hvp)\n",
    "\n",
    "    # VJP functions for matrix-transpose-vector products\n",
    "    def vjp_d2L_dtheta_dp_T(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂²L/∂θ∂p]^T @ v_theta using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        # Get VJP function for the gradient w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda p: lagrangian_grad(Θ_opt, p, λ_theta_opt, λ_g_opt), p_opt)\n",
    "\n",
    "        # Apply VJP with v_theta\n",
    "        result = vjp_fun(v_theta_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dg_active_dp_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂p]^T @ v_lambda_g using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        # Get VJP function for g_active w.r.t. p\n",
    "        _, vjp_fun = jax.vjp(lambda p: g_active(Θ_opt, p), p_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_g\n",
    "        result = vjp_fun(v_lambda_g_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_df_dp_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂p]^T @ v_f using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_p)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        # Get VJP function for f w.r.t. p\n",
    "        _, vjp_fun = jax.vjp(lambda p: f(Θ_opt, p), p_opt)\n",
    "\n",
    "        # Apply VJP with v_f\n",
    "        result = vjp_fun(v_f_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dtheta_active_dtheta_T(v_lambda_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ]^T @ v_lambda_theta using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_theta):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_theta_jax = jnp.array(v_lambda_theta)\n",
    "\n",
    "        # Get VJP function for Θ_active w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(Θ_active, Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_theta\n",
    "        result = vjp_fun(v_lambda_theta_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_dg_active_dtheta_T(v_lambda_g):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ]^T @ v_lambda_g using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_lambda_g):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_lambda_g_jax = jnp.array(v_lambda_g)\n",
    "\n",
    "        # Get VJP function for g_active w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda theta: g_active(theta, p_opt), Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_lambda_g\n",
    "        result = vjp_fun(v_lambda_g_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    def vjp_df_dtheta_T(v_f):\n",
    "        \"\"\"\n",
    "        Compute [∂f/∂θ]^T @ v_f using VJP\n",
    "        \"\"\"\n",
    "        if not np.any(v_f):\n",
    "            return np.zeros(n_theta)\n",
    "\n",
    "        v_f_jax = jnp.array(v_f)\n",
    "\n",
    "        # Get VJP function for f w.r.t. θ\n",
    "        _, vjp_fun = jax.vjp(lambda theta: f(theta, p_opt), Θ_opt)\n",
    "\n",
    "        # Apply VJP with v_f\n",
    "        result = vjp_fun(v_f_jax)[0]\n",
    "        return np.array(result)\n",
    "\n",
    "    # Regular JVP functions for forward matrix-vector products\n",
    "    def jvp_dtheta_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂Θ_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_theta)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(Θ_active, (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def jvp_dg_active_dtheta(v_theta):\n",
    "        \"\"\"\n",
    "        Compute [∂g_active/∂θ] @ v_theta using JVP\n",
    "        \"\"\"\n",
    "        if not np.any(v_theta):\n",
    "            return np.zeros(n_lambda_g)\n",
    "\n",
    "        v_theta_jax = jnp.array(v_theta)\n",
    "\n",
    "        _, jvp_result = jax.jvp(lambda theta: g_active(theta, p_opt), (Θ_opt,), (v_theta_jax,))\n",
    "        return np.array(jvp_result)\n",
    "\n",
    "    def matvec_transpose(v):\n",
    "        \"\"\"\n",
    "        Compute matrix-vector product for [∂R/∂u]^T @ v\n",
    "        This implements the transpose of the UDE Jacobian matrix.\n",
    "        Uses VJPs for transpose operations and JVPs for forward operations.\n",
    "        \"\"\"\n",
    "        # Split v into blocks\n",
    "        idx = 0\n",
    "        v_p = v[idx:idx+n_p]\n",
    "        idx += n_p\n",
    "        v_btheta = v[idx:idx+n_btheta]\n",
    "        idx += n_btheta\n",
    "        v_bg = v[idx:idx+n_bg]\n",
    "        idx += n_bg\n",
    "        v_theta = v[idx:idx+n_theta]\n",
    "        idx += n_theta\n",
    "        v_lambda_theta = v[idx:idx+n_lambda_theta]\n",
    "        idx += n_lambda_theta\n",
    "        v_lambda_g = v[idx:idx+n_lambda_g]\n",
    "        idx += n_lambda_g\n",
    "        v_outputs = v[idx:idx+n_outputs_total]\n",
    "        v_f = v_outputs[:n_f]\n",
    "        v_f_theta = v_outputs[n_f:]\n",
    "\n",
    "        print(v)\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(total_size)\n",
    "\n",
    "        # Block 1: Effect on p\n",
    "        result[:n_p] = v_p\n",
    "        result[:n_p] -= vjp_d2L_dtheta_dp_T(v_theta)\n",
    "        result[:n_p] -= vjp_dg_active_dp_T(v_lambda_g)\n",
    "        result[:n_p] -= vjp_df_dp_T(v_f)\n",
    "\n",
    "        # Block 2: Effect on b_theta\n",
    "        idx = n_p\n",
    "        result[idx:idx+n_btheta] = v_btheta + v_lambda_theta\n",
    "\n",
    "        # Block 3: Effect on b_g\n",
    "        idx += n_btheta\n",
    "        result[idx:idx+n_bg] = v_bg + v_lambda_g\n",
    "\n",
    "        # Block 4: Effect on theta\n",
    "        idx += n_bg\n",
    "        result_theta = np.zeros(n_theta)\n",
    "        if np.any(v_theta):\n",
    "            print('using hessian-vector product!')\n",
    "            # Use Hessian-vector product for second derivatives\n",
    "            result_theta -= hessian_vector_product(v_theta)\n",
    "        result_theta -= vjp_dtheta_active_dtheta_T(v_lambda_theta)\n",
    "        result_theta -= vjp_dg_active_dtheta_T(v_lambda_g)\n",
    "        result_theta -= vjp_df_dtheta_T(v_f)\n",
    "        # Identity outputs contribution\n",
    "        if np.any(v_f_theta):\n",
    "            result_theta -= v_f_theta  # -I @ v_f_theta\n",
    "        result[idx:idx+n_theta] = result_theta\n",
    "\n",
    "        # Block 5: Effect on lambda_theta\n",
    "        idx += n_theta\n",
    "        result[idx:idx+n_lambda_theta] = -jvp_dtheta_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 6: Effect on lambda_g\n",
    "        idx += n_lambda_theta\n",
    "        result[idx:idx+n_lambda_g] = -jvp_dg_active_dtheta(v_theta)\n",
    "\n",
    "        # Block 7: Effect on outputs\n",
    "        idx += n_lambda_g\n",
    "        result[idx:idx+n_outputs_total] = v_outputs\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Create LinearOperator\n",
    "    A_transpose = LinearOperator((total_size, total_size), matvec=matvec_transpose)\n",
    "\n",
    "    # Storage for sensitivities\n",
    "    sensitivities = {}\n",
    "\n",
    "    # Solve for each output\n",
    "    output_names = ['f', 'θ₀', 'θ₁']\n",
    "\n",
    "    for i, name in enumerate(output_names):\n",
    "        # Create RHS vector with 1 in appropriate output position\n",
    "        rhs = np.zeros(total_size)\n",
    "        output_start_idx = n_p + n_btheta + n_bg + n_theta + n_lambda_theta + n_lambda_g\n",
    "        rhs[output_start_idx + i] = 1.0\n",
    "\n",
    "        # Solve the system\n",
    "        print(f\"\\nSolving for sensitivities of {name}...\")\n",
    "        solution, info = gmres(A_transpose, rhs, rtol=1e-10, maxiter=1000)\n",
    "\n",
    "        if info == 0:\n",
    "            # Extract sensitivities\n",
    "            sens_p = solution[:n_p]\n",
    "            sens_btheta = solution[n_p:n_p+n_btheta]\n",
    "            sens_bg = solution[n_p+n_btheta:n_p+n_btheta+n_bg]\n",
    "\n",
    "            sensitivities[name] = {\n",
    "                'wrt_p': sens_p,\n",
    "                'wrt_btheta': sens_btheta,\n",
    "                'wrt_bg': sens_bg\n",
    "            }\n",
    "\n",
    "            print(f\"  d{name}/dp₀ = {sens_p[0]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₁ = {sens_p[1]:+.6f}\")\n",
    "            print(f\"  d{name}/dp₂ = {sens_p[2]:+.6f}\")\n",
    "            print(f\"  d{name}/db_θ₀ = {sens_btheta[0]:+.6f}\")\n",
    "            print(f\"  d{name}/db_g₀ = {sens_bg[0]:+.6f}\")\n",
    "        else:\n",
    "            print(f\"  Warning: GMRES did not converge (info={info})\")\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "def verify_with_finite_differences(sensitivities, h=1e-6):\n",
    "    \"\"\"\n",
    "    Verify sensitivities using finite differences.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Base values\n",
    "    Θ_base = jnp.array([6.0, -6.0])\n",
    "    p_base = jnp.array([3.0, 4.0, 3.0])\n",
    "    b_theta_base = 6.0\n",
    "\n",
    "    # For simplicity, we'll verify df/dp₀ and dθ₀/db_θ₀\n",
    "\n",
    "    # Verify df/dp₀\n",
    "    def solve_optimization(p_val, b_theta_val):\n",
    "        \"\"\"\n",
    "        Simplified optimization solve assuming active constraints remain active.\n",
    "        For the active set: θ₀ = b_theta_val, θ₁ = -b_theta_val\n",
    "        \"\"\"\n",
    "        θ_opt = jnp.array([b_theta_val, -b_theta_val])\n",
    "        f_val = f(θ_opt, p_val)[0]\n",
    "        return f_val, θ_opt\n",
    "\n",
    "    # df/dp₀ by finite differences\n",
    "    p_plus = p_base.at[0].set(p_base[0] + h)\n",
    "    f_plus, _ = solve_optimization(p_plus, b_theta_base)\n",
    "    p_minus = p_base.at[0].set(p_base[0] - h)\n",
    "    f_minus, _ = solve_optimization(p_minus, b_theta_base)\n",
    "    df_dp0_fd = (f_plus - f_minus) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndf/dp₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['f']['wrt_p'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {df_dp0_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['f']['wrt_p'][0] - df_dp0_fd):.2e}\")\n",
    "\n",
    "    # dθ₀/db_θ₀ by finite differences (should be 1.0 since θ₀ = b_θ₀ at optimum)\n",
    "    _, theta_plus = solve_optimization(p_base, b_theta_base + h)\n",
    "    _, theta_minus = solve_optimization(p_base, b_theta_base - h)\n",
    "    dtheta0_dbtheta_fd = (theta_plus[0] - theta_minus[0]) / (2 * h)\n",
    "\n",
    "    print(f\"\\ndθ₀/db_θ₀:\")\n",
    "    print(f\"  UDE:    {sensitivities['θ₀']['wrt_btheta'][0]:+.6f}\")\n",
    "    print(f\"  FD:     {dtheta0_dbtheta_fd:+.6f}\")\n",
    "    print(f\"  Error:  {abs(sensitivities['θ₀']['wrt_btheta'][0] - dtheta0_dbtheta_fd):.2e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*60)\n",
    "    print(\"Post-Optimality Sensitivity Analysis using JAX\")\n",
    "    print(\"with Vector-Jacobian Products (VJPs)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = compute_sensitivities_jax()\n",
    "\n",
    "    # Verify with finite differences\n",
    "    verify_with_finite_differences(sensitivities)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Summary of Key Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNote: Since θ₀ is bounded at 6.0 and the constraint θ₀ + θ₁ = 0 is active:\")\n",
    "    print(\"- Changes in b_θ₀ directly affect θ₀ (dθ₀/db_θ₀ = 1)\")\n",
    "    print(\"- Changes in b_θ₀ inversely affect θ₁ (dθ₁/db_θ₀ = -1)\")\n",
    "    print(\"- The objective is most sensitive to p₀ (df/dp₀ = -6)\")\n",
    "    print(\"\\nAdvantages of Vector-Jacobian Product (VJP) approach:\")\n",
    "    print(\"- Memory efficient: O(n) operations, no matrix storage\")\n",
    "    print(\"- VJPs are the natural choice for A^T @ v operations\")\n",
    "    print(\"- Automatic differentiation ensures exact derivatives\")\n",
    "    print(\"- Scales linearly with problem size\")\n",
    "    print(\"- More efficient than the JVP approach for transpose operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
