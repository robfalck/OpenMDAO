{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Post-Optimality Sensitivities of a Constrained Optimization Problem\n",
    "\n",
    "Lets consider a problem such that we have an active bound and an active inequality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta_0,\\, \\theta_1} \\quad & f(\\theta_0, \\theta_1; \\mathbf{p}) = (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\\\\n",
    "\\text{where} \\quad \\mathbf{p} &= \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "\\text{bounds:} \\quad \\theta_0 &\\le 6 \\\\\n",
    "\\text{equality constraints:} \\quad \\theta_0 + \\theta_1 &= 0\n",
    "\\end{align*}\n",
    "\n",
    "We want to know the sensitivities of the optimization outputs with respect to the optimization inputs.\n",
    "\n",
    "In this context, consider the outputs of the optimization to be the objective and any other functions of interest, $f$.\n",
    "\n",
    "The design variables $\\theta$ and Lagrange multipliers $\\lambda$ are effectively the implicit outputs of the optimization.\n",
    "\n",
    "The _inputs_ to the optimization process consists of:\n",
    "- any independent parameters, $\\bar{p}$\n",
    "- the bounding values of any **active** design variables, $\\bar{b}_{\\theta}$\n",
    "- the bounding values of any **active** constraints, $\\bar{b}o_{g}$\n",
    "\n",
    "In our case we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{p} &= \\begin{bmatrix} p_0 \\\\ p_1 \\\\ p_2 \\end{bmatrix} \\\\\n",
    "    \\bar{b}_{\\theta} &= \\begin{bmatrix} \\theta_0^{ub} \\end{bmatrix} \\\\\n",
    "    \\bar{b}_g &= \\begin{bmatrix} g_0^{eq} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "<!-- If active, we can treat the bound on $\\theta_0$ as just another equality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{\\mathcal{G}}(\\bar{\\theta}, \\bar{p}) &= \\begin{bmatrix}\n",
    "                                   \\theta_0 + \\theta_1 \\\\\n",
    "                                   \\theta_0 - p_3\n",
    "                                \\end{bmatrix} = \\bar 0\n",
    "\\end{align*}\n",
    "\n",
    "**How will my system design ($\\bar{\\theta}^*$) respond to changes in my assumptions and system inputs ($\\bar{p}$)?** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Derivatives Equation\n",
    "\n",
    "The UDE is:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right] \\left[ \\frac{d u}{d \\mathcal{R}} \\right]\n",
    "  &=\n",
    "  \\left[ I \\right]\n",
    "  =\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right]^T \\left[ \\frac{d u}{d \\mathcal{R}} \\right]^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, the residuals are the primal and dual residuals of the optimization process, given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the UDE to solving post-optimality sensitivities\n",
    "\n",
    "In our case, the unknowns vector consists of\n",
    "- the optimization parameters ($\\bar{p}$)\n",
    "- the bounding values of any active design variables ($\\bar{b}_{\\theta}$)\n",
    "- the bounding values of any active constraints ($\\bar{b}_{g}$)\n",
    "- the design variables of the optimization ($\\bar{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active design variables ($\\bar{\\lambda}_{\\theta}$)\n",
    "- the Lagrange multipliers associated with the active constraints ($\\bar{\\lambda}_{g}$)\n",
    "- the objective value **as well as** any other outputs for which we want the sensitivities ($f$)\n",
    "\n",
    "The total size of the unknowns vector is $N_p + N_{\\theta} + 2N_{\\lambda \\theta} + 2N_{\\lambda g} + N_{f}$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{u} &=\n",
    "  \\begin{bmatrix}\n",
    "    \\bar{p} \\\\\n",
    "    \\bar{b}_{\\theta} \\\\\n",
    "    \\bar{b}_{g} \\\\\n",
    "    \\bar{\\theta} \\\\\n",
    "    \\bar{\\lambda_{\\theta}} \\\\\n",
    "    \\bar{\\lambda_{g}} \\\\\n",
    "    \\bar{f}\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Under the UDE, the corresponding residual equations for these unknowns are\n",
    "- the implicit form of the parameter values\n",
    "- the implicit form of the active design variable values\n",
    "- the implicit form of the active constraint values\n",
    "- the stationarity condition\n",
    "- the active design variable residuals\n",
    "- the active constraint residuals\n",
    "- the implicit form of the explicit calculations of $f$\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{\\mathcal{R}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathcal{R}}_p \\\\\n",
    "\\bar{\\mathcal{R}}_{b \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{b g} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda \\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{\\lambda g} \\\\\n",
    "\\bar{\\mathcal{R}}_{f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\check{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{\\theta} - \\check{b}_{\\theta} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{b}_{g} - \\check{b}_g \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ \\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_g + \\nabla_{\\bar{\\theta}} \\check{\\theta}_{\\mathcal{A}} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda \\theta} - \\left[ \\check{\\theta}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_{\\theta} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda g} - \\left[ \\check{g}_{\\mathcal{A}} \\left( \\bar{\\theta}, \\bar{p} \\right) - \\bar{b}_g \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  f - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&= \\bar 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the total derivatives that we seek ($\\frac{d f^*}{d \\bar{p}}$ and $\\frac{d \\bar{\\theta}^*}{d \\bar{p}}$), we need $\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}$.\n",
    "\n",
    "The optimizer has served as the nonlinear solver in this case which has computed the values in the unknowns vector: $\\bar{\\theta}$, $\\bar{\\lambda}$, and $\\bar{f}$ such that the residuals are satisfied.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_p}}{\\partial \\bar{p}} & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} \\theta}}}{\\partial \\bar{b}_{\\theta}} & 0 & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\bar{b} g}}}{\\partial \\bar{b}_{g}} & 0 & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_{\\theta}}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda_g}} & 0 \\\\[1.1ex]\n",
    "0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{b}_g} & 0 & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\lambda \\theta}}}{\\partial \\bar{\\lambda_g}} & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{p}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{b}_g} & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{\\theta}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}}_{\\lambda g}}{\\partial \\bar{\\lambda_g}} & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{p}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{\\theta}} & 0 & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\check{\\mathcal{L}}}{\\partial \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\frac{\\partial \\check g}{\\partial \\bar{\\theta}} & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This nomenclature can be a bit confusing.\n",
    "\n",
    "**The _partial_ derivatives of the post-optimality residuals are the _total_ derivatives of the analysis.**\n",
    "\n",
    "In this case of the stationarity residuals $\\mathcal{R}_{\\bar{\\theta}}$, which already include _total_ derivatives of the analysis for the objective and constraint gradients, second derivatives are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding total derivaties which we need to solve for are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\bar{u}}{d \\bar{\\mathcal{R}}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d f}{d \\bar{\\mathcal{R}_p}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{\\mathcal{R}_f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{p}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d f}{d \\bar{p}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we can solve them with four linear solves of the forward system, or three solves of the reverse system.\n",
    "\n",
    "TODO: Need to explain how du/dRf becomes du/df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDE for this case, in forward form, is\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    0 & \\left[ I_\\theta \\right] & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_\\lambda \\right] & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we have four parameters and thus four columns for which we need to solve the system.\n",
    "\n",
    "Alternatively, we have three rows of interest in this system...two for the design variables $\\theta_0$ and $\\theta_1$ and one for the objective $f$.\n",
    "\n",
    "Taking the transpose and solving this system using the reverse form would require three linear system solves.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & \\frac{\\partial \\nabla\\check{\\mathcal{L}}}{\\partial \\bar{p}}^T & \\frac{\\partial \\check g}{\\partial \\bar{p}}^T & -\\frac{\\partial \\check f}{\\partial \\bar{p}}^T \\\\[1.1ex]\n",
    "    0 & \\nabla^2 \\check{\\mathcal{L}}^T & \\nabla \\check g & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}}^T \\\\[1.1ex]\n",
    "    0 & \\nabla \\check g ^T & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}^T} & \\frac{d \\bar{\\lambda}}{d \\bar{p}}^T & \\mathbf{\\frac{d f}{d \\bar{p}}^T} \\\\[1.1ex]\n",
    "\\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}}^T & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}}^T & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}}^T & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}}^T & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}}^T & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}}^T & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}}^T \\\\[1.1ex]\n",
    "\\frac{d \\bar{p}}{d \\bar{f}}^T & \\frac{d \\bar{\\theta}}{d \\bar{f}}^T & \\frac{d \\bar{\\lambda}}{d \\bar{f}}^T & \\frac{d f}{d \\bar{f}}^T\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    0 & \\left[ I_\\theta \\right] & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_\\lambda \\right] & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets use OpenMDAO to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -25.999999999999993\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization Complete\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Problem: problem\n",
       "Driver:  ScipyOptimizeDriver\n",
       "  success     : True\n",
       "  iterations  : 3\n",
       "  runtime     : 1.6263E-01 s\n",
       "  model_evals : 3\n",
       "  model_time  : 5.5082E-02 s\n",
       "  deriv_evals : 2\n",
       "  deriv_time  : 1.0327E-01 s\n",
       "  exit_status : SUCCESS"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import openmdao.api as om\n",
    "\n",
    "\n",
    "class ObjComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('f', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "        return np.array([f])\n",
    "\n",
    "class ConComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('g', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        g = Θ[0] + Θ[1]\n",
    "        return np.array([g])\n",
    "\n",
    "\n",
    "_prob = om.Problem()\n",
    "_prob.model.add_subsystem('f_comp', ObjComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "_prob.model.add_subsystem('g_comp', ConComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "\n",
    "_prob.model.add_design_var('Θ', upper=[6., None])\n",
    "_prob.model.add_constraint('g', equals=0.)\n",
    "_prob.model.add_objective('f')\n",
    "\n",
    "_prob.driver = om.ScipyOptimizeDriver()\n",
    "\n",
    "_prob.setup()\n",
    "\n",
    "_prob.set_val('p', [3, 4, 3, 6])\n",
    "\n",
    "_prob.run_driver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Variables(s) in 'model'\n",
      "\n",
      "varname  val                  io      prom_name\n",
      "-------  -------------------  ------  ---------\n",
      "f_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  f      [-26.]               output  f        \n",
      "g_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  g      [1.77635684e-15]     output  g        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_prob.model.list_vars(print_arrays=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dvs, active_cons = _prob.driver.compute_lagrange_multipliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets form the UDE system and compute the sensitivities, outside of OpenMDAO first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert OpenMDAO values to jax arrays\n",
    "\n",
    "f_opt = np.array(_prob.get_val('f'))\n",
    "Θ_opt = np.array(_prob.get_val('Θ'))\n",
    "p = np.array(_prob.get_val('p'))\n",
    "\n",
    "# The lagrange multipliers of the active constraints are\n",
    "λ_opt = np.array([active_dvs['Θ']['multipliers'][active_dvs['Θ']['indices']],\n",
    "              active_cons['g']['multipliers'][active_cons['g']['indices']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our objective and active constraint (and bounds) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(Θ, p):\n",
    "    f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return np.array([f])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    return np.array([Θ[0] + Θ[1],\n",
    "                     Θ[0] - p[3]])\n",
    "\n",
    "# def df_dΘ(Θ, p):\n",
    "#     df_dΘ = [[2 * (Θ[0] - p[0]) + Θ[1]],\n",
    "#              [Θ[0] + 2 * (Θ[1] + p[1])]]\n",
    "#     return np.array(df_dΘ)\n",
    "\n",
    "# def df_dp(Θ, p):\n",
    "#     df_dp = [[-2 * (Θ[0] - p[0])],\n",
    "#              [2 * (Θ[1] + p[1])],\n",
    "#              [-1],\n",
    "#              [0]]\n",
    "#     return np.array(df_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Θ*:\n",
      "[ 6. -6.]\n",
      "\n",
      "λ*:\n",
      "[[ 2.]\n",
      " [-2.]]\n",
      "\n",
      "∂f/∂Θ:\n",
      "[[1.77635684e-15 2.00000000e+00]]\n",
      "\n",
      "∂f/∂p:\n",
      "[[-6. -4. -1.  0.]]\n",
      "\n",
      "∂g/∂Θ:\n",
      "[[1. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "∂g/∂p:\n",
      "[[ 0.  0.  0. -0.]\n",
      " [ 0.  0.  0. -1.]]\n",
      "\n",
      "∇L:\n",
      "[[-3.10862447e-15]\n",
      " [-1.77635684e-15]]\n",
      "\n",
      "∇²f (via jacobian of jacobian):\n",
      "[[[2. 1.]\n",
      "  [1. 2.]]]\n",
      "\n",
      "∂∇f/∂p (via jacobian of jacobian):\n",
      "[[[-2.  0.  0.  0.]\n",
      "  [ 0.  2.  0.  0.]]]\n",
      "\n",
      "∇²g (via jacobian of jacobian):\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n",
      "\n",
      "∂∇g/∂p (via jacobian of jacobian):\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "I_p:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "I_f:\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# The design vars, from OpenMDAO\n",
    "print('\\nΘ*:')\n",
    "print(Θ_opt)\n",
    "\n",
    "# The Lagrange multipliers, from OpenMDAO\n",
    "print(\"\\nλ*:\")\n",
    "print(λ_opt)\n",
    "\n",
    "# Jacobian of f with respect to Θ\n",
    "df_dΘ = jax.jacobian(f, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂Θ:\")\n",
    "print(df_dΘ)\n",
    "\n",
    "# Jacobian of f with respect to p\n",
    "df_dp = jax.jacobian(f, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂p:\")\n",
    "print(df_dp)\n",
    "\n",
    "# Jacobian of g_active with respect to Θ\n",
    "dg_dΘ = jax.jacobian(g_active, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂Θ:\")\n",
    "print(dg_dΘ)\n",
    "\n",
    "# Jacobian of g_active with respect to p\n",
    "dg_dp = jax.jacobian(g_active, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂p:\")\n",
    "print(dg_dp)\n",
    "\n",
    "# Lagrangian\n",
    "dL_dΘ = -df_dΘ.T + np.matmul(dg_dΘ.T, λ_opt)\n",
    "print(\"\\n∇L:\")\n",
    "print(dL_dΘ)\n",
    "\n",
    "# Hessian of the objective\n",
    "d2f_dΘ2 = jax.jacobian(jax.jacobian(f, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²f (via jacobian of jacobian):\")\n",
    "print(d2f_dΘ2)\n",
    "\n",
    "d2f_dΘdp = jax.jacobian(jax.jacobian(f, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇f/∂p (via jacobian of jacobian):\")\n",
    "print(d2f_dΘdp)\n",
    "\n",
    "# Hessian of the constraints\n",
    "d2g_dΘ2 = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²g (via jacobian of jacobian):\")\n",
    "print(d2g_dΘ2)\n",
    "\n",
    "d2g_dΘdp = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇g/∂p (via jacobian of jacobian):\")\n",
    "print(d2g_dΘdp)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘ2 = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘdp = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "I_p = np.eye(4)\n",
    "print(\"\\nI_p:\")\n",
    "print(I_p)\n",
    "\n",
    "I_f = np.eye(1)\n",
    "print(\"\\nI_f:\")\n",
    "print(I_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def assemble_ude_matrix_scipy_sparse(nabla2_L, nabla_g, dg_dp, df_dtheta, df_dp, Np, Nx, Ng):\n",
    "    \"\"\"\n",
    "    Assemble the UDE matrix using SciPy sparse matrices.\n",
    "    This is more memory efficient for large problems.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert JAX arrays to numpy for SciPy\n",
    "    nabla2_L_np = np.array(nabla2_L).reshape((Nx, Nx))\n",
    "    nabla_g_np = np.array(nabla_g)\n",
    "    dg_dp_np = np.array(dg_dp)\n",
    "    df_dtheta_np = np.array(df_dtheta)\n",
    "    df_dp_np = np.array(df_dp)\n",
    "\n",
    "    # Create sparse blocks\n",
    "\n",
    "    # Row 1: [I_p, 0, 0, 0]\n",
    "    row1 = [\n",
    "        sp.eye(Np),                                    # I_p\n",
    "        sp.csr_matrix((Np, Nx)),                      # 0\n",
    "        sp.csr_matrix((Np, Ng)),                      # 0\n",
    "        sp.csr_matrix((Np, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 2: [∂∇L/∂p, ∇²L, ∇g^T, 0]\n",
    "    row2 = [\n",
    "        sp.csr_matrix((Nx, Np)),                      # ∂∇L/∂p (placeholder)\n",
    "        sp.csr_matrix(nabla2_L_np),                   # ∇²L\n",
    "        sp.csr_matrix(nabla_g_np.T),                  # ∇g^T\n",
    "        sp.csr_matrix((Nx, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 3: [∂g/∂p, ∇g, 0, 0]\n",
    "    row3 = [\n",
    "        sp.csr_matrix(dg_dp_np),                      # ∂g/∂p\n",
    "        sp.csr_matrix(nabla_g_np),                    # ∇g\n",
    "        sp.csr_matrix((Ng, Ng)),                      # 0\n",
    "        sp.csr_matrix((Ng, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 4: [-∂f/∂p, -∂f/∂θ, 0, I_f]\n",
    "    row4 = [\n",
    "        sp.csr_matrix(-df_dp_np),                     # -∂f/∂p\n",
    "        sp.csr_matrix(-df_dtheta_np),                 # -∂f/∂θ\n",
    "        sp.csr_matrix((1, Ng)),                       # 0\n",
    "        sp.eye(1)                                     # I_f\n",
    "    ]\n",
    "\n",
    "    # Assemble using bmat\n",
    "    partial_R_partial_u = sp.bmat([row1, row2, row3, row4], format='csr')\n",
    "\n",
    "    return partial_R_partial_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_R_partial_u = assemble_ude_matrix_scipy_sparse(d2L_dΘ2, dg_dΘ, dg_dp, df_dΘ, df_dp, Np=4, Nx=2, Ng=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -1  1  1  0]\n",
      " [ 0  0  0  0 -1 -2  1  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 6  3  1  0  0 -2  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(linewidth=1024, precision=1):\n",
    "    print(np.asarray(partial_R_partial_u.todense(), dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the sensitivities of the objective with respect to the parameters, the final row of $\\frac{d u}{d \\mathcal{R}}$, we transpose $\\frac{\\partial \\mathcal{R}}{\\partial u}$ and seed the right hand side with a 1 in the last row.\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{bmatrix}\n",
    "    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-2 &-1 & 1 & 1 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-1 &-2 & 1 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 &-1 & 1 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    6 & 3 & 1 & 0 & 0 &-2 & 0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  ^T\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d f^*}{d p_0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_2} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_3} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d f^*}\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    1\n",
    "  \\end{bmatrix}  \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 0, 0, 0, 1]]).T\n",
    "dfstar_du = spsolve(partial_R_partial_u.T, rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2., -0., -0.,  2., -2.,  1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstar_dp = dfstar_du[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "Recall that the optimal objective value $f^*$ was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.999999999999993"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_opt.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the sensitivities by perturbing each element in p and reoptimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_f_sensitivity(h=1.0E-8):\n",
    "    _prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity               {\"UDE Result\":20s}       {\"FD Result\":20s}          {\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        _prob.set_val('p', p_nom + dp)\n",
    "        _prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        _prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        _prob.run_driver()\n",
    "        _prob.set_val('p', p_nom)\n",
    "        _prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dfstar_dpi_fd = (_prob.get_val('f') - f_opt) / h\n",
    "\n",
    "        print(f'   df*/dp_{p_idx}     {dfstar_dp[p_idx]:20.12f}      {dfstar_dpi_fd[0]:20.12f}      {dfstar_dp[p_idx]-dfstar_dpi_fd[p_idx]:20.12f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity               UDE Result                 FD Result                     Error               \n",
      "   df*/dp_0          -6.000000000000           -6.000000496442            0.000000496442\n",
      "   df*/dp_1          -4.000000000000           -3.999999975690           -0.000000024310\n",
      "   df*/dp_2          -1.000000000000           -1.000000082740            0.000000082740\n",
      "   df*/dp_3          -2.000000000000           -2.000000876023            0.000000876023\n"
     ]
    }
   ],
   "source": [
    "check_f_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the sensitivities of $\\theta^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "dthetastar0_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 1, 0, 0, 0]]).T\n",
    "dthetastar1_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "dthetastar_du = np.vstack((dthetastar0_du, dthetastar1_du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dthetastar_dp = dthetastar_du[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., -1.]], dtype=float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dthetastar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, at the optimum $\\theta_0$ is on its upper bound, so modifying this upper bound will necessarily change $\\theta_0$ since we assume the bound remains active.\n",
    "\n",
    "Since $\\theta_1$ is constrained to be equal and opposite to $\\theta_0$, the increase in $\\theta_0$ will result in a equal and opposite change in $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Θ_sensitivity(h=1.0E-8):\n",
    "    _prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity                                       '\n",
    "          f'{\"UDE Result\":20s}                      '\n",
    "          f'{\"FD Result\":20s}                         '\n",
    "          f'{\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        _prob.set_val('p', p_nom + dp)\n",
    "        _prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        _prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        _prob.run_driver()\n",
    "        _prob.set_val('p', p_nom)\n",
    "        _prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dthetastar_dpi_fd = (_prob.get_val('Θ') - Θ_opt) / h\n",
    "\n",
    "        # print(prob.get_val('Θ'), Θ_opt, dthetastar_dpi_fd)\n",
    "\n",
    "        with np.printoptions(precision=4, formatter={'all':lambda x: f'{x:16.12f}'}):\n",
    "            print(f'   dΘ*/dp_{p_idx}              {dthetastar_dp[:, p_idx]}      {dthetastar_dpi_fd}      {dthetastar_dp[:, p_idx]-dthetastar_dpi_fd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity                                       UDE Result                                FD Result                                    Error               \n",
      "   dΘ*/dp_0              [  0.000000000000   0.000000000000]      [  0.000000000000  -0.000000177636]      [  0.000000000000   0.000000177636]\n",
      "   dΘ*/dp_1              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_2              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_3              [  1.000000000000  -1.000000000000]      [  0.999999993923  -1.000000171558]      [  0.000000006077   0.000000171558]\n"
     ]
    }
   ],
   "source": [
    "check_Θ_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A matrix-free approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationProblem(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Component that defines the optimization problem:\n",
    "    min f(θ₀, θ₁; p) = (θ₀ - p₀)² + θ₀θ₁ + (θ₁ + p₁)² - p₂\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        # Inputs\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta1', val=1.0)\n",
    "        self.add_input('p0', val=3.0)\n",
    "        self.add_input('p1', val=4.0)\n",
    "        self.add_input('p2', val=3.0)\n",
    "\n",
    "        # Outputs\n",
    "        self.add_output('f', val=1.0)\n",
    "\n",
    "        # Declare partials\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        theta0 = inputs['theta0']\n",
    "        theta1 = inputs['theta1']\n",
    "        p0 = inputs['p0']\n",
    "        p1 = inputs['p1']\n",
    "        p2 = inputs['p2']\n",
    "\n",
    "        outputs['f'] = (theta0 - p0)**2 + theta0*theta1 + (theta1 + p1)**2 - p2\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        theta0 = inputs['theta0']\n",
    "        theta1 = inputs['theta1']\n",
    "        p0 = inputs['p0']\n",
    "        p1 = inputs['p1']\n",
    "\n",
    "        # Derivatives of f\n",
    "        partials['f', 'theta0'] = 2*(theta0 - p0) + theta1\n",
    "        partials['f', 'theta1'] = theta0 + 2*(theta1 + p1)\n",
    "        partials['f', 'p0'] = -2*(theta0 - p0)\n",
    "        partials['f', 'p1'] = 2*(theta1 + p1)\n",
    "        partials['f', 'p2'] = -1.0\n",
    "\n",
    "\n",
    "class EqualityConstraint(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Equality constraint: θ₁ = -θ₀\n",
    "    Formulated as g₁ = θ₀ + θ₁ = 0\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta1', val=1.0)\n",
    "        self.add_output('g1', val=0.0)\n",
    "\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['g1'] = inputs['theta0'] + inputs['theta1']\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        partials['g1', 'theta0'] = 1.0\n",
    "        partials['g1', 'theta1'] = 1.0\n",
    "\n",
    "\n",
    "class BoundConstraint(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Upper bound constraint: θ₀ ≤ θ₀_ub\n",
    "    Formulated as g₂ = θ₀ - θ₀_ub ≤ 0\n",
    "    When active, treated as g₂ = θ₀ - θ₀_ub = 0\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta0_ub', val=6.0)\n",
    "        self.add_output('g2', val=0.0)\n",
    "\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['g2'] = inputs['theta0'] - inputs['theta0_ub']\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        partials['g2', 'theta0'] = 1.0\n",
    "        partials['g2', 'theta0_ub'] = -1.0\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the UDE sensitivity analysis.\n",
    "    \"\"\"\n",
    "    # Create OpenMDAO problem\n",
    "    prob = om.Problem()\n",
    "\n",
    "    # Add subsystems\n",
    "    prob.model.add_subsystem('obj', OptimizationProblem(),\n",
    "                            promotes_inputs=['theta0', 'theta1', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f'])\n",
    "\n",
    "    prob.model.add_subsystem('eq_con', EqualityConstraint(),\n",
    "                            promotes_inputs=['theta0', 'theta1'],\n",
    "                            promotes_outputs=['g1'])\n",
    "\n",
    "    prob.model.add_subsystem('bound_con', BoundConstraint(),\n",
    "                            promotes_inputs=['theta0', 'theta0_ub'],\n",
    "                            promotes_outputs=['g2'])\n",
    "\n",
    "    # Add driver (optimizer)\n",
    "    prob.driver = om.ScipyOptimizeDriver()\n",
    "    prob.driver.options['optimizer'] = 'SLSQP'\n",
    "    prob.driver.options['tol'] = 1e-8\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    # Define design variables\n",
    "    prob.model.add_design_var('theta0', lower=-50, upper=6.0)\n",
    "    prob.model.add_design_var('theta1', lower=-50, upper=50)\n",
    "\n",
    "    # Define objective\n",
    "    prob.model.add_objective('f')\n",
    "\n",
    "    # Define constraints\n",
    "    prob.model.add_constraint('g1', equals=0.0)\n",
    "\n",
    "    # Setup the problem\n",
    "    prob.setup()\n",
    "\n",
    "    # Set parameter values\n",
    "    prob.set_val('p0', 3.0)\n",
    "    prob.set_val('p1', 4.0)\n",
    "    prob.set_val('p2', 3.0)\n",
    "    prob.set_val('theta0_ub', 6.0)\n",
    "\n",
    "    # Initial guess\n",
    "    prob.set_val('theta0', 5.0)\n",
    "    prob.set_val('theta1', -5.0)\n",
    "\n",
    "    # Run optimization\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Running Optimization\")\n",
    "    print(\"=\"*60)\n",
    "    prob.run_driver()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Optimization Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"θ₀* = {prob.get_val('theta0')[0]:.6f}\")\n",
    "    print(f\"θ₁* = {prob.get_val('theta1')[0]:.6f}\")\n",
    "    print(f\"f* = {prob.get_val('f')[0]:.6f}\")\n",
    "    print(f\"g₁ = {prob.get_val('g1')[0]:.6e} (should be ~0)\")\n",
    "    print(f\"g₂ = {prob.get_val('g2')[0]:.6f}\")\n",
    "\n",
    "    # Check if bound is active\n",
    "    bound_active = abs(prob.get_val('theta0')[0] - 6.0) < 1e-6\n",
    "    print(f\"Bound active: {'bound_active'}\")\n",
    "\n",
    "    # Determine active constraints\n",
    "    active_constraints = ['g1']  # Equality always active\n",
    "    if bound_active:\n",
    "        active_constraints.append('g2')\n",
    "\n",
    "    # Compute Lagrange multipliers\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Computing Lagrange Multipliers\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Get gradients at optimum\n",
    "    grad_f = prob.compute_totals(of='f', wrt=['theta0', 'theta1'], return_format='array').flatten()\n",
    "    print(f\"∇f = {grad_f}\")\n",
    "\n",
    "    # Build constraint Jacobian\n",
    "    jac_g = []\n",
    "    for con in active_constraints:\n",
    "        grad_g = prob.compute_totals(of=con, wrt=['theta0', 'theta1'], return_format='array').flatten()\n",
    "        jac_g.append(grad_g)\n",
    "        print(f\"∇{con} = {grad_g}\")\n",
    "    jac_g = np.array(jac_g)\n",
    "\n",
    "    # Solve for multipliers: ∇f = Σλᵢ∇gᵢ\n",
    "    lambda_vals = np.linalg.lstsq(jac_g.T, -grad_f, rcond=None)[0]\n",
    "    print(f\"Lagrange multipliers: {lambda_vals}\")\n",
    "\n",
    "    # Verify KKT conditions\n",
    "    residual = grad_f + jac_g.T @ lambda_vals\n",
    "    print(f\"KKT residual: {residual} (should be ~0)\")\n",
    "\n",
    "    # Create UDE solver\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Computing Post-Optimality Sensitivities\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    ude_solver = PostOptimalitySensitivitySolver(\n",
    "        prob=prob,\n",
    "        design_vars=['theta0', 'theta1'],\n",
    "        parameters=['p0', 'p1', 'p2', 'theta0_ub'],\n",
    "        outputs=['f'],\n",
    "        verbose=False  # Set to True for debugging\n",
    "    )\n",
    "\n",
    "    jac_g_act = ude_solver.compute_active_constraint_jacobian()\n",
    "\n",
    "    print(jac_g_act.todense())\n",
    "\n",
    "    jvp = ude_solver.compute_active_constraint_jvp(v_theta=np.array([1.0, 0.0]))\n",
    "\n",
    "    print(jvp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # # # Set Lagrange multipliers\n",
    "    # # ude_solver.lambda_vals = lambda_vals\n",
    "\n",
    "    # # Compute sensitivities\n",
    "    # sensitivities = ude_solver.solve_sensitivities()\n",
    "\n",
    "    # # Display results\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"Sensitivity Results\")\n",
    "    # print(\"=\"*60)\n",
    "\n",
    "    # param_names = ['p₀', 'p₁', 'p₂', 'θ₀_ub']\n",
    "\n",
    "    # print(\"\\nDesign Variable Sensitivities (dθ/dp):\")\n",
    "    # print(\"-\" * 40)\n",
    "    # dtheta_dp = sensitivities['dtheta_dp']\n",
    "    # for i, var in enumerate(['θ₀', 'θ₁']):\n",
    "    #     print(f\"{var}:\")\n",
    "    #     for j, param in enumerate(param_names):\n",
    "    #         print(f\"  d{var}/d{param} = {dtheta_dp[i, j]:+.6f}\")\n",
    "\n",
    "    # print(\"\\nObjective Function Sensitivity (df*/dp):\")\n",
    "    # print(\"-\" * 40)\n",
    "    # df_dp = sensitivities['df_dp'].flatten()\n",
    "    # for j, param in enumerate(param_names):\n",
    "    #     print(f\"  df*/d{param} = {df_dp[j]:+.6f}\")\n",
    "\n",
    "    # # Identify most sensitive parameter\n",
    "    # max_idx = np.argmax(np.abs(df_dp[:3]))  # Only check p0, p1, p2 (not bound)\n",
    "    # print(f\"\\nMost sensitive parameter: {param_names[max_idx]} \"\n",
    "    #       f\"(df*/d{param_names[max_idx]} = {df_dp[max_idx]:+.6f})\")\n",
    "\n",
    "    # # Verify sensitivities with finite differences\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"Verification with Finite Differences\")\n",
    "    # print(\"=\"*60)\n",
    "\n",
    "    # eps = 1e-5\n",
    "    # for j, param in enumerate(['p0', 'p1', 'p2', 'theta0_ub']):\n",
    "    #     # Save original value\n",
    "    #     orig_val = prob.get_val(param)[0] if param != 'theta0_ub' else 6.0\n",
    "\n",
    "    #     # Perturb parameter\n",
    "    #     if param == 'theta0_ub':\n",
    "    #         prob.model._design_vars['theta0']['upper'] = 6.0 + eps\n",
    "    #     else:\n",
    "    #         prob.set_val(param, orig_val + eps)\n",
    "\n",
    "    #     prob.set_val('theta0', 5.0)\n",
    "    #     prob.set_val('theta1', -5.0)\n",
    "\n",
    "    #     # Re-optimize\n",
    "    #     prob.run_driver()\n",
    "    #     f_plus = prob.get_val('f')[0]\n",
    "\n",
    "    #     # Restore and perturb in other direction\n",
    "    #     if param == 'theta0_ub':\n",
    "    #         prob.model._design_vars['theta0']['upper'] = 6.0 - eps\n",
    "    #     else:\n",
    "    #         prob.set_val(param, orig_val - eps)\n",
    "\n",
    "    #     prob.set_val('theta0', 5.0)\n",
    "    #     prob.set_val('theta1', -5.0)\n",
    "\n",
    "    #     prob.run_driver()\n",
    "    #     f_minus = prob.get_val('f')[0]\n",
    "\n",
    "    #     # Compute finite difference\n",
    "    #     fd_sensitivity = (f_plus - f_minus) / (2 * eps)\n",
    "\n",
    "    #     # Restore original\n",
    "    #     if param == 'theta0_ub':\n",
    "    #         prob.model._design_vars['theta0']['upper'] = 6.0\n",
    "    #     else:\n",
    "    #         prob.set_val(param, orig_val)\n",
    "\n",
    "    #     print(f\"df*/d{param_names[j]}:\")\n",
    "    #     print(f\"  UDE:    {df_dp[j]:+.6f}\")\n",
    "    #     print(f\"  FD:     {fd_sensitivity:+.6f}\")\n",
    "    #     print(f\"  Error:  {abs(df_dp[j] - fd_sensitivity):.2e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Corrected post-optimality sensitivity analysis using OpenMDAO with matrix-free UDE approach.\n",
    "\"\"\"\n",
    "import itertools\n",
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import gmres, LinearOperator\n",
    "\n",
    "\n",
    "class PostOptimalitySensitivitySolver:\n",
    "    \"\"\"\n",
    "    Matrix-free UDE solver for post-optimality sensitivity analysis.\n",
    "    Uses Hessian-vector products to avoid forming the full Hessian.\n",
    "    \"\"\"\n",
    "    def __init__(self, prob, design_vars=None, parameters=None, outputs=None,\n",
    "                 hvp_method='forward', hvp_eps=1e-7, verbose=False, driver_scaling=False):\n",
    "        \"\"\"\n",
    "        Initialize the UDE solver.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hvp_method : str\n",
    "            Method for Hessian-vector products: 'forward', 'central', or 'forward_basepoint'\n",
    "        hvp_eps : float\n",
    "            Finite difference step size for Hessian-vector products\n",
    "        \"\"\"\n",
    "        self.prob = prob\n",
    "        self.design_vars = self.prob.driver._designvars\n",
    "        self.constraints = self.prob.driver._cons\n",
    "        self.outputs = outputs\n",
    "        self.verbose = verbose\n",
    "        self.hvp_method = hvp_method\n",
    "        self.hvp_eps = hvp_eps\n",
    "\n",
    "        param_dict = {meta['prom_name'] : {'size': meta['size'], 'units': meta['units']}\n",
    "                      for meta in\n",
    "                      self.prob.model.get_io_metadata(is_indep_var=True, is_design_var=False).values()}\n",
    "        if parameters:\n",
    "            if not set(parameters).issubset(param_dict.keys()):\n",
    "                missing = set(parameters) - set(param_dict.keys())\n",
    "                raise KeyError(f\"The following specified parameters are not independent variables in the model:\\n{missing}\")\n",
    "            self.parameters = {k: param_dict[k] for k in parameters if k in param_dict}\n",
    "        else:\n",
    "            self.parameters = param_dict\n",
    "\n",
    "        # self.n_lambda = len(constraints)\n",
    "        self.n_p = len(parameters)\n",
    "        self.n_f = len(outputs)\n",
    "\n",
    "        # Store the optimal point\n",
    "        self.x_opt = np.array([prob[var] for var in design_vars])\n",
    "\n",
    "        # Counter for gradient evaluations\n",
    "        self.grad_eval_count = 0\n",
    "\n",
    "        # Store information on the active constraints and design vars\n",
    "        driver = self.prob.driver\n",
    "        self._active_dvs, self._active_cons = driver.compute_lagrange_multipliers(driver_scaling=driver_scaling)\n",
    "\n",
    "        self.n_theta = 0\n",
    "        idx0 = 0\n",
    "        active_dv_idxs = []\n",
    "        for dv, dv_meta in self.prob.driver._designvars.items():\n",
    "            size = dv_meta['size']\n",
    "            self.n_theta += size\n",
    "            if dv in self._active_dvs:\n",
    "                active_dv_idxs.append(idx0 + self._active_dvs[dv]['indices'])\n",
    "            idx0 += size\n",
    "        self._active_dv_idxs = np.concatenate(active_dv_idxs) if active_dv_idxs else None\n",
    "\n",
    "        self._n_active_cons = 0 if not self._active_cons else sum([act['indices'] for act in self._active_cons.values()])\n",
    "        self._n_active_dvs = 0 if not self._active_dvs else sum([act['indices'] for act in self._active_dvs.values()])\n",
    "        self._n_lambda = self._n_active_cons + self._n_active_dvs\n",
    "\n",
    "        self.total_size = self.n_p + self.n_theta + self._n_lambda + self.n_f\n",
    "\n",
    "        # The portion of the constraint jacobian due to the active dvs\n",
    "        self._jac_g_theta = np.eye(self.n_theta)[self._active_dv_idxs, :] if self._active_dv_idxs else None\n",
    "\n",
    "    def gradient_lagrangian(self, x_perturbed=None):\n",
    "        \"\"\"\n",
    "        Compute gradient of Lagrangian with evaluation counting.\n",
    "        \"\"\"\n",
    "        self.grad_eval_count += 1\n",
    "\n",
    "        if x_perturbed is not None:\n",
    "            # Set perturbed values\n",
    "            for i, var in enumerate(self.design_vars):\n",
    "                self.prob[var] = x_perturbed[i]\n",
    "            self.prob.run_model()\n",
    "\n",
    "        # Rest of implementation...\n",
    "        grad_f = self.prob.compute_totals(\n",
    "            of=self.outputs[0],\n",
    "            wrt=self.design_vars,\n",
    "            return_format='array'\n",
    "        ).flatten()\n",
    "\n",
    "        grad_L = grad_f.copy()\n",
    "\n",
    "        for i, con in enumerate(self.constraints):\n",
    "            grad_g = self.prob.compute_totals(\n",
    "                of=con,\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "\n",
    "            if hasattr(self, 'lambda_vals'):\n",
    "                grad_L -= self.lambda_vals[i] * grad_g\n",
    "\n",
    "        return grad_L\n",
    "\n",
    "    def hessian_vector_product(self, v, eps=1e-7, method='forward'):\n",
    "        \"\"\"\n",
    "        Compute Hessian-vector product ∇²L · v using finite differences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array\n",
    "            Vector to multiply with Hessian\n",
    "        eps : float\n",
    "            Finite difference step size\n",
    "        method : str\n",
    "            'forward': H·v ≈ (∇L(x + εv) - ∇L(x)) / ε  (2 gradient evals per call)\n",
    "            'central': H·v ≈ (∇L(x + εv) - ∇L(x - εv)) / (2ε)  (2 gradient evals per call)\n",
    "            'forward_basepoint': H·v ≈ (∇L(x + εv) - ∇L(x)) / ε  (1 gradient eval if ∇L(x) cached)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array\n",
    "            Hessian-vector product\n",
    "        \"\"\"\n",
    "        # Save current state\n",
    "        x_current = self.x_opt.copy()\n",
    "\n",
    "        if method == 'forward':\n",
    "            # Forward difference - only need x + eps*v\n",
    "            x_plus = x_current.ravel() + eps * v.ravel()\n",
    "            grad_L_plus = self.gradient_lagrangian(x_plus)\n",
    "\n",
    "            # Restore state and compute gradient at base point\n",
    "            for i, var in enumerate(self.design_vars):\n",
    "                self.prob[var] = x_current[i]\n",
    "            self.prob.run_model()\n",
    "            grad_L_base = self.gradient_lagrangian()\n",
    "\n",
    "            return (grad_L_plus - grad_L_base) / eps\n",
    "\n",
    "        elif method == 'central':\n",
    "            # Central difference - need both x ± eps*v\n",
    "            x_plus = x_current.ravel() + eps * v.ravel()\n",
    "            grad_L_plus = self.gradient_lagrangian(x_plus)\n",
    "\n",
    "            x_minus = x_current.ravel() - eps * v.ravel()\n",
    "            grad_L_minus = self.gradient_lagrangian(x_minus)\n",
    "\n",
    "            # Restore original state\n",
    "            for i, var in enumerate(self.design_vars):\n",
    "                self.prob[var] = x_current[i]\n",
    "            self.prob.run_model()\n",
    "\n",
    "            return (grad_L_plus - grad_L_minus) / (2 * eps)\n",
    "\n",
    "        elif method == 'forward_basepoint':\n",
    "            # Forward difference using cached gradient at base point\n",
    "            # This is most efficient when solving multiple RHS with same A matrix\n",
    "\n",
    "            # Cache gradient at base point if not already done\n",
    "            if not hasattr(self, '_cached_grad_L_base'):\n",
    "                for i, var in enumerate(self.design_vars):\n",
    "                    self.prob[var] = x_current[i]\n",
    "                self.prob.run_model()\n",
    "                self._cached_grad_L_base = self.gradient_lagrangian()\n",
    "\n",
    "            # Compute gradient at perturbed point\n",
    "            x_plus = x_current.ravel() + eps * v.ravel()\n",
    "            grad_L_plus = self.gradient_lagrangian(x_plus)\n",
    "\n",
    "            # Restore original state\n",
    "            for i, var in enumerate(self.design_vars):\n",
    "                self.prob[var] = x_current[i]\n",
    "            self.prob.run_model()\n",
    "\n",
    "            return (grad_L_plus - self._cached_grad_L_base) / eps\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}. Use 'forward', 'central', or 'forward_basepoint'\")\n",
    "\n",
    "    def constraint_jacobian(self):\n",
    "        \"\"\"\n",
    "        Get Jacobian of constraints ∇g.\n",
    "        \"\"\"\n",
    "        jac = []\n",
    "        for con in self.constraints:\n",
    "            grad_g = self.prob.compute_totals(\n",
    "                of=con,\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            jac.append(grad_g)\n",
    "        return np.array(jac)\n",
    "\n",
    "    def compute_active_constraint_jacobian(self):\n",
    "        \"\"\"\n",
    "        Get Jacobian of active constraints and bounds ∇g.\n",
    "        \"\"\"\n",
    "        jac_block_rows = []\n",
    "        for con, act_meta in self._active_cons.items():\n",
    "            grad_g = self.prob.compute_totals(\n",
    "                of=con,\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            )\n",
    "            jac_block_rows.append(sp.csr_matrix(grad_g[act_meta['indices'], ...]))\n",
    "\n",
    "        jac_block_rows.append(self._jac_g_theta)\n",
    "\n",
    "        return sp.vstack(jac_block_rows)\n",
    "\n",
    "    def compute_active_constraint_jvp(self, v_theta, mode='fwd'):\n",
    "        \"\"\"\n",
    "        Compute the jacobian vector product for the problem jacobian and\n",
    "        the associated vector v_theta.\n",
    "        \"\"\"\n",
    "        result = np.zeros((self.n_theta, 1))\n",
    "        # In forward mode, convert v_theta to seeds.\n",
    "        dv_seeds = {}\n",
    "        theta_idx0 = 0\n",
    "        # jvp_active_dvs = []\n",
    "        for dv, dv_meta in self.prob.driver._designvars.items():\n",
    "            dv_size = dv_meta['size']\n",
    "            if np.any(v_theta[theta_idx0: theta_idx0 + dv_size]):\n",
    "                dv_seeds[dv] = v_theta[theta_idx0: theta_idx0 + dv_size]\n",
    "            theta_idx0 += dv_size\n",
    "            # We need to append the block due to the active design variables\n",
    "            # to the bottom of the jvp, so do that here\n",
    "            # if dv in self._active_dvs:\n",
    "            #     result += v_theta[self._active_dvs['indices']]\n",
    "        # block_rows = []\n",
    "        self.prob.model.run_linearize(True)\n",
    "        row0 = 0\n",
    "        for con, act_meta in self._active_cons.items():\n",
    "            jvp = self.prob.compute_jacvec_product(of=[con],\n",
    "                                                   wrt=list(dv_seeds.keys()),\n",
    "                                                   seed=dv_seeds,\n",
    "                                                   mode='fwd')[con]\n",
    "            n_active = len(act_meta['indices'])\n",
    "            result[row0:row0 + n_active] += jvp[act_meta['indices']]\n",
    "            # jvp contains an element for every scalar constraint\n",
    "            # extract the active ones\n",
    "            # block_rows.append(jvp[act_meta['indices']])\n",
    "            row0 += len(act_meta['indices'])\n",
    "\n",
    "        result[:, 0] += v_theta\n",
    "\n",
    "        return result\n",
    "\n",
    "    def ude_matvec(self, vec):\n",
    "        \"\"\"\n",
    "        Matrix-vector product for the UDE system matrix.\n",
    "        \"\"\"\n",
    "        self.matvec_count += 1\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\nMatvec call #{self.matvec_count}\")\n",
    "            print(f\"  Input norm: {np.linalg.norm(vec):.6e}\")\n",
    "            print(f\"  Non-zeros: {np.count_nonzero(vec)}\")\n",
    "\n",
    "        # Extract blocks from input vector\n",
    "        idx = 0\n",
    "        v_p = vec[idx:idx+self.n_p]\n",
    "        idx += self.n_p\n",
    "        v_theta = vec[idx:idx+self.n_theta]\n",
    "        idx += self.n_theta\n",
    "        v_lambda = vec[idx:idx+self.n_lambda]\n",
    "        idx += self.n_lambda\n",
    "        v_f = vec[idx:idx+self.n_f]\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(self.total_size)\n",
    "        idx = 0\n",
    "\n",
    "        # First block row: [I_p, 0, 0, 0] @ v\n",
    "        result[idx:idx+self.n_p] = v_p\n",
    "        idx += self.n_p\n",
    "\n",
    "        # Second block row: [∂∇L/∂p, ∇²L, (∇g)ᵀ, 0] @ v\n",
    "        row2 = np.zeros(self.n_theta)\n",
    "\n",
    "        # ∇²L @ v_theta using Hessian-vector product\n",
    "        if np.any(v_theta):\n",
    "            Hv = self.hessian_vector_product(v_theta)\n",
    "            row2 += Hv\n",
    "\n",
    "        # (∇g)ᵀ @ v_lambda\n",
    "        if np.any(v_lambda):\n",
    "            jac_g = self.constraint_jacobian()\n",
    "            row2 += jac_g.T @ v_lambda\n",
    "\n",
    "        # ∂∇L/∂p @ v_p (often small near optimum, approximating as zero)\n",
    "        # Could be computed via finite differences if needed\n",
    "\n",
    "        result[idx:idx+self.n_theta] = row2\n",
    "        idx += self.n_theta\n",
    "\n",
    "        # Third block row: [∂g/∂p, ∇g, 0, 0] @ v\n",
    "        row3 = np.zeros(self.n_lambda)\n",
    "\n",
    "        # ∇g @ v_theta\n",
    "        if np.any(v_theta):\n",
    "            jac_g = self.compute_active_constraint_jacobian()\n",
    "            row3 += jac_g.dot(v_theta)\n",
    "\n",
    "        # ∂g/∂p @ v_p\n",
    "        if np.any(v_p):\n",
    "            for i, con in enumerate(self.constraints):\n",
    "                grad_g_p = self.prob.compute_totals(\n",
    "                    of=con,\n",
    "                    wrt=self.parameters,\n",
    "                    return_format='array'\n",
    "                ).flatten()\n",
    "                row3[i] += np.dot(grad_g_p, v_p)\n",
    "\n",
    "        result[idx:idx+self.n_lambda] = row3\n",
    "        idx += self.n_lambda\n",
    "\n",
    "        # Fourth block row: [-∂f/∂p, -∂f/∂θ, 0, I_f] @ v\n",
    "        row4 = v_f.copy()  # I_f @ v_f\n",
    "\n",
    "        if np.any(v_p):\n",
    "            df_dp = self.prob.compute_totals(\n",
    "                of=self.outputs[0],\n",
    "                wrt=self.parameters,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            row4 -= np.dot(df_dp, v_p)\n",
    "\n",
    "        if np.any(v_theta):\n",
    "            df_dtheta = self.prob.compute_totals(\n",
    "                of=self.outputs[0],\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            row4 -= np.dot(df_dtheta, v_theta)\n",
    "\n",
    "        # Fourth block row via jvp: [-∂f/∂p, -∂f/∂θ, 0, I_f] @ v\n",
    "        block_row_4 = v_f.copy()  # I_f @ v_f\n",
    "        df_dp_accum = itertools.product(list(self.outputs.keys()), list(self.parameters.keys()))\n",
    "\n",
    "\n",
    "        if np.any(v_p):\n",
    "            # Break v_p into a seed for each parameter (wrt)\n",
    "            p_idx0 = 0\n",
    "            vp_seeds = {}\n",
    "            for param_name, param_meta in self.parameters.items():\n",
    "                p_size = param_meta['size']\n",
    "                if np.any(v_theta[p_idx0: p_idx0 + p_size]):\n",
    "                    vp_seeds[param_name] = v_p[p_idx0: p_idx0 + p_size]\n",
    "                p_idx0 += p_size\n",
    "            jvp_dfdp_v_p = self.prob.compute_jacvec_product(of=self.outputs,\n",
    "                                                            wrt=list(self.parameters.keys()),\n",
    "                                                            mode='fwd',\n",
    "                                                            seed=vp_seeds)\n",
    "\n",
    "\n",
    "            # df_dp = self.prob.compute_totals(\n",
    "            #     of=self.outputs[0],\n",
    "            #     wrt=self.parameters,\n",
    "            #     return_format='array'\n",
    "            # ).flatten()\n",
    "            # row4_2 -= np.dot(df_dp, v_p)\n",
    "\n",
    "        if np.any(v_theta):\n",
    "            df_dtheta = self.prob.compute_totals(\n",
    "                of=self.outputs[0],\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            row4_2 -= np.dot(df_dtheta, v_theta)\n",
    "\n",
    "        result[idx:idx+self.n_f] = row4_2\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"  Output norm: {np.linalg.norm(result):.6e}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def solve_sensitivities(self, tol=1e-6, maxiter=1000):\n",
    "        \"\"\"\n",
    "        Solve for post-optimality sensitivities using GMRES.\n",
    "        \"\"\"\n",
    "        # Reset counter\n",
    "        self.matvec_count = 0\n",
    "\n",
    "        # Create linear operator\n",
    "        A_op = LinearOperator(\n",
    "            (self.total_size, self.total_size),\n",
    "            matvec=self.ude_matvec\n",
    "        )\n",
    "\n",
    "        # Test the operator first\n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"Testing LinearOperator\")\n",
    "            print(\"=\"*60)\n",
    "            test_vec = np.zeros(self.total_size)\n",
    "            test_vec[0] = 1.0\n",
    "            test_result = A_op.matvec(test_vec)\n",
    "            print(f\"Test result norm: {np.linalg.norm(test_result):.6e}\")\n",
    "\n",
    "        # Storage for solutions\n",
    "        X = np.zeros((self.total_size, self.total_size))\n",
    "\n",
    "        print(f\"\\nSolving UDE system ({self.total_size}x{self.total_size})...\")\n",
    "        print(f\"Parameters: {self.parameters}\")\n",
    "        print(f\"Design vars: {self.design_vars}\")\n",
    "        print(f\"Constraints: {self.constraints}\")\n",
    "        print(f\"Outputs: {self.outputs}\")\n",
    "\n",
    "        # Solve for each column of the identity matrix\n",
    "        col = 0\n",
    "\n",
    "        # Columns for I_p\n",
    "        for i in range(self.n_p):\n",
    "            print(f\"\\n  Solving for parameter {self.parameters[i]}...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[i] = 1.0\n",
    "\n",
    "            # Use a non-zero initial guess can help convergence\n",
    "            x0 = rhs.copy() * 0.1\n",
    "\n",
    "            sol, info = gmres(A_op, rhs, x0=x0, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "\n",
    "            if info == 0:\n",
    "                pass\n",
    "                # print(f\"    Converged! ||residual|| = {np.linalg.norm(A_op.matvec(sol) - rhs):.6e}\")\n",
    "            else:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Columns for I_theta\n",
    "        for i in range(self.n_theta):\n",
    "            print(f\"\\n  Solving for design var {self.design_vars[i]} sensitivity...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[self.n_p + i] = 1.0\n",
    "\n",
    "            x0 = rhs.copy() * 0.1\n",
    "\n",
    "            sol, info = gmres(A_op, rhs, x0=x0, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "\n",
    "            if info == 0:\n",
    "                pass\n",
    "                # print(f\"    Converged! ||residual|| = {np.linalg.norm(A_op.matvec(sol) - rhs):.6e}\")\n",
    "            else:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Columns for I_lambda\n",
    "        for i in range(self.n_lambda):\n",
    "            print(f\"\\n  Solving for constraint {self.constraints[i]} multiplier sensitivity...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[self.n_p + self.n_theta + i] = 1.0\n",
    "\n",
    "            x0 = rhs.copy() * 0.1\n",
    "\n",
    "            sol, info = gmres(A_op, rhs, x0=x0, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "\n",
    "            if info == 0:\n",
    "                pass\n",
    "                # print(f\"    Converged! ||residual|| = {np.linalg.norm(A_op.matvec(sol) - rhs):.6e}\")\n",
    "            else:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Columns for I_f\n",
    "        for i in range(self.n_f):\n",
    "            print(f\"\\n  Solving for output {self.outputs[i]} sensitivity...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[self.n_p + self.n_theta + self.n_lambda + i] = 1.0\n",
    "\n",
    "            x0 = rhs.copy() * 0.1\n",
    "\n",
    "            sol, info = gmres(A_op, rhs, x0=x0, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "\n",
    "            if info == 0:\n",
    "                pass\n",
    "                # print(f\"    Converged! ||residual|| = {np.linalg.norm(A_op.matvec(sol) - rhs):.6e}\")\n",
    "            else:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        print(f\"\\nTotal matvec calls: {self.matvec_count}\")\n",
    "\n",
    "        # Extract sensitivities\n",
    "        idx_theta = self.n_p\n",
    "        idx_lambda = self.n_p + self.n_theta\n",
    "        idx_f = self.n_p + self.n_theta + self.n_lambda\n",
    "\n",
    "        sensitivities = {\n",
    "            'dtheta_dp': X[idx_theta:idx_lambda, :self.n_p],\n",
    "            'dlambda_dp': X[idx_lambda:idx_f, :self.n_p],\n",
    "            'df_dp': X[idx_f:, :self.n_p]\n",
    "        }\n",
    "\n",
    "        return sensitivities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running Optimization\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Optimization Results\n",
      "============================================================\n",
      "θ₀* = 6.000000\n",
      "θ₁* = -6.000000\n",
      "f* = -26.000000\n",
      "g₁ = 0.000000e+00 (should be ~0)\n",
      "g₂ = -0.000000\n",
      "Bound active: bound_active\n",
      "\n",
      "============================================================\n",
      "Computing Lagrange Multipliers\n",
      "============================================================\n",
      "∇f = [-8.8817842e-16  2.0000000e+00]\n",
      "∇g1 = [1. 1.]\n",
      "∇g2 = [ 1. -0.]\n",
      "Lagrange multipliers: [-2.  2.]\n",
      "KKT residual: [8.8817842e-16 4.4408921e-16] (should be ~0)\n",
      "\n",
      "============================================================\n",
      "Computing Post-Optimality Sensitivities\n",
      "============================================================\n",
      "[[1. 1.]]\n",
      "[[2.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
