{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      equationNumbers: {\n",
    "        autoNumber: \"AMS\",\n",
    "        useLabelIds: true\n",
    "      }\n",
    "    },\n",
    "    displayAlign: \"center\"\n",
    "  });\n",
    "</script>\n",
    "\n",
    "# Post-Optimality Sensitivities\n",
    "\n",
    "Post-optimality sensitivities refer to the derivative of an optimization output wrt an optimization input.\n",
    "These sensitivities are intended to inform the user of the cost of a design choice.\n",
    "**These sensitivities only apply in the context of gradient-based optimization.**\n",
    "\n",
    "## Viewing optimization as a nonlinear solve\n",
    "\n",
    "From a MAUD perspective, one could view a gradient-based optimization as a nonlinear solve.\n",
    "\n",
    "The implicit outputs in this context are the design varaible values, and the residual\n",
    "equations come from the Karush-Kuhn-Tucker conditions for optimization.\n",
    "\n",
    "### The Unconstrained Case\n",
    "\n",
    "Consider the optimization of some function. Here is the standard form of the OpenMDAO \n",
    "paraboloid where we've replaced the constants with some parameters $p_0$, $p_1$, and $p_2$.\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{x,\\, y} \\quad \n",
    "    & f(x, y; \\mathbf{p}) = (x - p_0)^2 + x y + (y + p_1)^2 - p_2 \\\\\n",
    "    \\text{where} \\quad \n",
    "    & \\mathbf{p} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "    & x, y \\in \\mathbb{R}\n",
    "\\end{align}\n",
    "\n",
    "This system has 3 outputs ($f$, $x$, $y$) and 3 inputs ($\\bar{p}$). So we may be interested in\n",
    "in how those outputs change at the next converged solution subject to small changes in $\\bar{p}$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d x^*}{d \\bar{p}} \\\\\n",
    "    \\frac{d y^*}{d \\bar{p}} \\\\\n",
    "    \\frac{d f^*}{d \\bar{p}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In an unconstrained problem, the optimizer is finding the \"bottom\" of a multidimensional bowl:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}}(\\bar\\theta, \\bar{p}) = \\nabla_\\theta f(\\bar{\\theta}, \\bar{p}) &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\theta$ here represents the design variables instead of the typical $x$ in order to not confuse it with\n",
    "the inputs the to the paraboloid above.\n",
    "With two design variables, the residual vector $\\mathcal{R_{KKT}}$ has two elements.\n",
    "\n",
    "Our residual system has the following structure.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{d\\bar{\\theta}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{d \\nabla_{\\theta} f}{d\\bar{\\theta}}  \\\\\n",
    "    \\end{bmatrix}^{-1}\n",
    "  }\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d \\nabla_{\\theta} f}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "or, expanding terms...\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df^*}{dp_0} & \\frac{df^*}{dp_1} & \\frac{df^*}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{dp_0} & \\frac{df}{dp_1} & \\frac{df}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{dx} & \\frac{df}{dy} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{d\\nabla_x f}{dx} & \\frac{d\\nabla_x f}{dy} \\\\\n",
    "        \\frac{d\\nabla_y f}{dx} & \\frac{d\\nabla_y f}{dy} \\\\\n",
    "    \\end{bmatrix}^{-1}\n",
    "  }\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_x f}{dp_0} & \\frac{d\\nabla_x f}{dp_1} & \\frac{d\\nabla_x f}{dp_2} \\\\\n",
    "      \\frac{d\\nabla_y f}{dp_0} & \\frac{d\\nabla_y f}{dp_1} & \\frac{d\\nabla_y f}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In an unconstrained optimization such as this, the Hessian $\\left(\\frac{d\\nabla_{\\theta} f}{d\\theta}\\right)$ is zero, so the post-optimality sensitivity of the objective wrt the parameters is identical to the total derivative of the objective wrt the parameters.\n",
    "\n",
    "The following code generates plots that sweep the value of each parameter and plot the resulting objective value, overlaid by vectors indicating the calculated derivative $\\frac{df^*}{dp_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('parab', om.ExecComp('f_xy = (x-p0)**2 + x*y + (y+p1)**2 - p2'),\n",
    "                            promotes_inputs=['x', 'y', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f_xy'])\n",
    "\n",
    "# Design variables 'x' and 'y' span components, so we need to provide a common initial\n",
    "# value for them.\n",
    "prob.model.set_input_defaults('x', 3.0)\n",
    "prob.model.set_input_defaults('y', -4.0)\n",
    "prob.model.set_input_defaults('p0', 3.0)\n",
    "prob.model.set_input_defaults('p1', 4.0)\n",
    "prob.model.set_input_defaults('p2', 3.0)\n",
    "\n",
    "# setup the optimization\n",
    "prob.driver = om.pyOptSparseDriver()\n",
    "prob.driver.options['print_results'] = False\n",
    "prob.driver.options['optimizer'] = 'SLSQP'\n",
    "prob.driver.options['singular_jac_behavior'] = 'ignore'\n",
    "\n",
    "prob.model.add_design_var('x', lower=-50, upper=50)\n",
    "prob.model.add_design_var('y', lower=-50, upper=50)\n",
    "prob.model.add_objective('f_xy')\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "prob.run_model()\n",
    "driver_vars = prob.list_driver_vars(out_stream=None)\n",
    "des_vars = [dv for dv, _ in driver_vars['design_vars']]\n",
    "constraints = [dv for dv, _ in driver_vars['constraints']]\n",
    "objs = [dv for dv, _ in driver_vars['objectives']]\n",
    "other_ofs = []\n",
    "other_wrts = ['p0', 'p1', 'p2']\n",
    "\n",
    "ofs = objs + constraints + other_ofs\n",
    "wrts = des_vars + other_wrts\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "ax3 = axs[2]\n",
    "\n",
    "n_pts = 10\n",
    "ps = np.linspace(0, 10, n_pts)\n",
    "f_stars = np.zeros(n_pts)\n",
    "df_stars_dps = np.zeros((3, n_pts))\n",
    "\n",
    "prob.run_driver()\n",
    "# No constraints, no lagrange multipliers\n",
    "totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "print('Nominal Solution:')\n",
    "print('   f*:', prob.get_val('f_xy'))\n",
    "print('   x*:', prob.get_val('x'))\n",
    "print('   y*:', prob.get_val('y'))\n",
    "print('   df*/dp0:', totals['f_xy', 'p0'])\n",
    "print('   df*/dp1:', totals['f_xy', 'p1'])\n",
    "print('   df*/dp2:', totals['f_xy', 'p2'])\n",
    "\n",
    "\n",
    "for j in range(3):\n",
    "\n",
    "    prob.set_val('p0', 3)\n",
    "    prob.set_val('p1', 4)\n",
    "    prob.set_val('p2', 3)\n",
    "\n",
    "    for i, p in enumerate(ps):\n",
    "        prob[f'p{j}'] = p\n",
    "        prob.run_driver()\n",
    "        f_stars[i] = np.copy(prob['f_xy'])[0]\n",
    "\n",
    "        est_multipliers, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6)\n",
    "\n",
    "        totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "        # dfstar_dg = est_multipliers['g']\n",
    "        # dg_dp = totals['g', 'p0']\n",
    "        dfstar_dpj= totals['f_xy', f'p{j}']\n",
    "\n",
    "        # df_stars_dps[i] = np.copy(est_multipliers['g'] * (prob['x'] + prob['y']))\n",
    "        df_stars_dps[j, i] = np.copy(dfstar_dpj).ravel()[0]\n",
    "\n",
    "\n",
    "\n",
    "    uv = np.column_stack([np.ones_like(df_stars_dps[j]),\n",
    "                            df_stars_dps[j]])\n",
    "\n",
    "    uv_norm = np.linalg.norm(uv, axis=1)\n",
    "    uv /= uv_norm[:, np.newaxis]\n",
    "\n",
    "    axs[j].plot(ps,\n",
    "                f_stars,\n",
    "                zorder=0,\n",
    "                linewidth=2.0)\n",
    "    axs[j].quiver(ps,\n",
    "                f_stars,\n",
    "                uv[:,0],\n",
    "                uv[:,1],\n",
    "                angles=\"xy\",\n",
    "                pivot=\"mid\",\n",
    "                color=\"red\",\n",
    "                width=0.01,\n",
    "                headwidth=2,\n",
    "                headlength=1.0,\n",
    "                headaxislength=1.0,\n",
    "                minshaft=0.1,\n",
    "                minlength=0.5)\n",
    "\n",
    "    axs[j].set(xlabel=f'$p_{j}$', ylabel=r'$f^*$')\n",
    "    axs[j].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal design variable sensitivities for unconstrained optimization\n",
    "\n",
    "In the section above, we demonstrate the ability to get the post-optimality sensitivity of the objective with respect to some parameters of the problem.\n",
    "\n",
    "Now we want to know, how would the optimal values of $x$ and $y$ change as the values of $\\bar{p}$ are changed?\n",
    "The KKT residual is still the same for the unconstrained case.\n",
    "Viewing the problem as an implicit solve, we have the following linear system for the residuals\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\theta}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "      \\frac{d{\\theta}}{d{\\bar{p}}} \\\\\n",
    "    \\end{bmatrix}\n",
    "  }\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\theta}}{d{\\theta}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\theta} {f}}{d{\\theta}} \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\theta} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The term $\\frac{d{\\theta}}{d{\\bar{p}}}$ drops to zero because the unoptimized design variables are not impacted by the choice of the parameters.\n",
    "The term $\\frac{d{\\theta}}{d{\\theta}}$ is just an identity matrix, so we're left with:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\theta}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\theta} {f}}{d{\\theta}} \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\theta} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\theta} {f}}{d{\\theta}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\theta}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\theta} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "So given $\\frac{d\\nabla_{\\theta} {f}}{d{\\theta}}$ and $\\frac{d\\nabla_{\\theta} {f}}{d\\bar{p}}$, a linear solve provides the sensitivities $\\frac{d{\\theta}^*}{d\\bar{p}}$.\n",
    "\n",
    "But how do we obtain these?\n",
    "\n",
    "A derivative of a gradient is a second derivative, and OpenMDAO currently does not provide second derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the Lagrange Multipliers\n",
    "\n",
    "In a constrained optimization problem, Lagrange multiplier terms are included in the KKT residual for each active constraint.\n",
    "The presence of active constraints means that the optimization will not find the \"bottom of the bowl\", but rather be limited to finding some minimal value on the surface of the bowl where the active constraint residual is zero.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}} =\n",
    "  \\begin{bmatrix}\n",
    "  \\nabla_x f + \\nabla_x \\bar{g}^T \\bar\\lambda + \\nabla_x \\bar{h}^T_\\mathcal{A} \\bar\\mu_\\mathcal{A} + \\bar\\nu_\\mathcal{A}\n",
    "  \\end{bmatrix} &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\bar{g}$ are the equality constraints, $\\bar{h}_\\mathcal{A}$ are the active inequality constraints, and $\\bar\\nu_\\mathcal{A}$\n",
    "\n",
    "Lumping all active constraints and bounds into a single pseudo-equality-constraint vector $\\mathcal{G}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "  \\nabla_x \\mathcal{G}^T \\begin{bmatrix}\\bar\\lambda \\\\ \\bar\\mu \\\\ \\bar\\nu \\end{bmatrix} = -\\nabla_x f\n",
    "\\end{align*}\n",
    "\n",
    "The gradients in the above equation are obtained by a call to OpenMDAO's `problem.compute_totals` method.\n",
    "\n",
    "From this we can find the multipliers using a least squares method in order to allow a little slack due to numerical issues.\n",
    "These multipliers give the _post-optimality sensitivities_ of the objective $f$ with respect to the active constraint bound values for the equality constraints, inequality constraints, and design variables, respectively.\n",
    "\n",
    "Now suppose, having already found the optimal solution that minimizes $f$, we treat the design variable vector $\\theta$ as if it were equality-constrained to those optimal values:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{G} = \\begin{bmatrix}x - x* \\\\ y - y* \\end{bmatrix} &= \\left[ 0 \\right]\n",
    "\\end{align*}\n",
    "\n",
    "If we were to perturb the design variables $x$ and $y$ away from their optimal values and solve the KKT system, the resulting multpliers would approximate the sensitivities of the objective with respect to the design variables, $\\frac{d\\nabla_{\\theta} {f}}{d{\\theta}}$.\n",
    "\n",
    "Similarly, a perturbation of the parameter values and treating them as if they were constraints would give us $\\frac{d\\nabla_{\\theta} {f}}{d{\\bar{p}}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('parab', om.ExecComp('f_xy = (x-p0)**2 + x*y + (y+p1)**2 - p2'),\n",
    "                            promotes_inputs=['x', 'y', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f_xy'])\n",
    "\n",
    "# Design variables 'x' and 'y' span components, so we need to provide a common initial\n",
    "# value for them.\n",
    "prob.model.set_input_defaults('x', 3.0)\n",
    "prob.model.set_input_defaults('y', -4.0)\n",
    "prob.model.set_input_defaults('p0', 3.0)\n",
    "prob.model.set_input_defaults('p1', 4.0)\n",
    "prob.model.set_input_defaults('p2', 3.0)\n",
    "\n",
    "# setup the optimization\n",
    "prob.driver = om.pyOptSparseDriver()\n",
    "prob.driver.options['print_results'] = False\n",
    "prob.driver.options['optimizer'] = 'SLSQP'\n",
    "prob.driver.options['singular_jac_behavior'] = 'ignore'\n",
    "\n",
    "prob.model.add_design_var('x', lower=-50, upper=50)\n",
    "prob.model.add_design_var('y', lower=-50, upper=50)\n",
    "prob.model.add_objective('f_xy')\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "PERTURB_H = 1.0E-6\n",
    "\n",
    "p0 = 3.\n",
    "p1 = 4.\n",
    "p2 = 3.\n",
    "prob.set_val('p0', p0)\n",
    "prob.set_val('p1', p1)\n",
    "prob.set_val('p2', p2)\n",
    "\n",
    "# prob[f'p{j}'] = p\n",
    "prob.run_driver()\n",
    "# x_stars[i] = np.copy(prob['x'])[0]\n",
    "# y_stars[i] = np.copy(prob['y'])[0]\n",
    "\n",
    "fstar = prob.get_val('f_xy')\n",
    "xstar = prob.get_val('x')\n",
    "ystar = prob.get_val('y')\n",
    "\n",
    "print('f*', fstar)\n",
    "print('x*', xstar)\n",
    "print('y*', ystar)\n",
    "\n",
    "print()\n",
    "\n",
    "# The nominal multipliers\n",
    "nom_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6)\n",
    "\n",
    "driver_vars = prob.list_driver_vars(out_stream=None)\n",
    "des_vars = [dv for dv, _ in driver_vars['design_vars']]\n",
    "constraints = [dv for dv, _ in driver_vars['constraints']]\n",
    "objs = [dv for dv, _ in driver_vars['objectives']]\n",
    "other_ofs = []\n",
    "other_wrts = ['p0', 'p1', 'p2']\n",
    "\n",
    "ofs = objs + constraints + other_ofs\n",
    "wrts = des_vars + other_wrts\n",
    "\n",
    "totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "# Perturb the design variables to obtain dgradf/dtheta\n",
    "# First do x\n",
    "prob.set_val('x', xstar + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "# The perturbed multipliers\n",
    "pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "prob.set_val('x', xstar)\n",
    "dgradf_dx = pert_mult['x'] / PERTURB_H\n",
    "dgradf_dy = pert_mult['y'] / PERTURB_H\n",
    "\n",
    "print('\\nperturbed x multipliers')\n",
    "print(f'{dgradf_dx[0]=}')\n",
    "print(f'{dgradf_dy[0]=}')\n",
    "\n",
    "# Now do y\n",
    "prob.set_val('y', ystar + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "# The perturbed y multipliers\n",
    "pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "prob.set_val('y', ystar)\n",
    "dfstar_dx = pert_mult['x'] / PERTURB_H\n",
    "dfstar_dy = pert_mult['y'] / PERTURB_H\n",
    "\n",
    "print('\\nperturbed y multipliers')\n",
    "print(f'{dfstar_dx[0]=}')\n",
    "print(f'{dfstar_dy[0]=}')\n",
    "\n",
    "# Perturb the parameters to obtain dgradf/dp\n",
    "for p_idx in range(3):\n",
    "\n",
    "    dfstar_dpi = totals['f_xy', f'p{p_idx}']\n",
    "\n",
    "    # print('nominal multipliers')\n",
    "    # print(nom_mult)\n",
    "    # prob[f'p{j}'] = p + PERTURB_H\n",
    "    p_save = prob.get_val(f'p{p_idx}')\n",
    "    prob.set_val(f'p{p_idx}', p0 + PERTURB_H)\n",
    "    prob.run_model()\n",
    "\n",
    "    # The perturbed multipliers\n",
    "    pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "    prob.set_val(f'p{p_idx}', p_save)\n",
    "\n",
    "    # print('nominal totals')\n",
    "    # print(totals)\n",
    "\n",
    "    print('\\nperturbed multipliers')\n",
    "    # print(pert_mult)\n",
    "    # print([(name, pm_val) for name, pm_val in pert_mult.items()])\n",
    "    dfstar_dx = pert_mult['x'] / PERTURB_H\n",
    "    dfstar_dy = pert_mult['y'] / PERTURB_H\n",
    "    print('df*/dx', dfstar_dx)\n",
    "    print('df*/dy', dfstar_dy)\n",
    "\n",
    "    # The perturbed multiplier on x is df*/dx*\n",
    "\n",
    "    # The totals of f wrt p0 is\n",
    "    print(f'df*/dp{p_idx}', dfstar_dpi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These equations are used to solve for the design variables and various lagrange multipliers.  Due to numerical peculiarities in the optimizer, solving this system for both the design variables and the multipliers ($\\lambda$, $\\mu$, $\\nu$) may yield different results than those obtained by the optimizer.\n",
    "\n",
    "Instead, use the same value of the design variables ($\\bar{x}$) that was obtained from the optimization.\n",
    "Then this residual is just a function of the multipliers, and we can remove the equality constraints from the residual, since they are not a function of the multipliers.\n",
    "\n",
    "The corresponding residual vector is now\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}} =\n",
    "  \\begin{bmatrix}\n",
    "  \\nabla_x f + \\nabla_x \\bar{g}^T \\bar\\lambda + \\nabla_x \\bar{h}^T(\\bar\\mu_{hl} - \\bar\\mu_{hu}) + \\bar\\nu_{xl} - \\bar\\nu_{xu} &(5)\\\\\n",
    "  \\mu_{hl} \\odot \\left[ \\bar{h}(\\bar{x}, \\bar{p}) - h_l \\right]  &(2)\\\\\n",
    "  \\mu_{hu} \\odot \\left[ h_u - \\bar{h}(\\bar{x}, \\bar{p}) \\right]  &(2)\\\\\n",
    "  \\nu_l \\odot (x - x_l)  &(5)\\\\ \n",
    "  \\nu_u \\odot (x_u - x)  &(5)\\\\ \n",
    "  \\end{bmatrix} &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Note that this vector size is 19, but the number of lagrange multipliers is 16 (2 $\\lambda$, 4 $\\mu$, 10 $\\nu$). So we can't necessarily solve this system at the $x$ found by the optimizer.\n",
    "\n",
    "## Inactive constraints\n",
    "\n",
    "Inactive constraints do not contribute to the problem (no sensitivities, residuals zero by default).\n",
    "\n",
    "Instead, we parse out the active constraints and bounds, and require that the residuals associated with those be exactly zero. From this perspective the KKT residual is just:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}} =\n",
    "  \\begin{bmatrix}\n",
    "  \\nabla_x f + \\nabla_x \\bar{g}^T \\bar\\lambda + \\nabla_x \\bar{h}^T_\\mathcal{A} \\bar\\mu_\\mathcal{A} + \\bar\\nu_\\mathcal{A}\n",
    "  \\end{bmatrix} &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Lumping all active constraints and bounds into a single pseudo-constraint vector $\\mathcal{G}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "  \\nabla_x \\mathcal{G}^T \\begin{bmatrix}\\bar\\lambda \\\\ \\bar\\mu \\\\ \\bar\\nu \\end{bmatrix} = -\\nabla_x f\n",
    "\\end{align*}\n",
    "\n",
    "For which we can find the multipliers using a least squares method in order to allow a little slack due to numerical issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
