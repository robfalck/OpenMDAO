{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "  MathJax.Hub.Config({\n",
    "    TeX: {\n",
    "      equationNumbers: {\n",
    "        autoNumber: \"AMS\",\n",
    "        useLabelIds: true\n",
    "      }\n",
    "    },\n",
    "    displayAlign: \"center\"\n",
    "  });\n",
    "</script>\n",
    "\n",
    "# Post-Optimality Sensitivities\n",
    "\n",
    "Post-optimality sensitivities refer to the derivative of an optimization output wrt an optimization input.\n",
    "These sensitivities are intended to inform the user of the cost of a design choice.\n",
    "**These sensitivities only apply in the context of gradient-based optimization.**\n",
    "\n",
    "## Viewing optimization as a nonlinear solve\n",
    "\n",
    "From a MAUD perspective, one could view a gradient-based optimization as a nonlinear solve.\n",
    "\n",
    "The implicit outputs in this context are the design varaible values, and the residual\n",
    "equations come from the Karush-Kuhn-Tucker conditions for optimization.\n",
    "\n",
    "### The Unconstrained Case\n",
    "\n",
    "Consider the optimization of some function. Here is the standard form of the OpenMDAO \n",
    "paraboloid where we've replaced the constants with some parameters $p_0$, $p_1$, and $p_2$.\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{x,\\, y} \\quad \n",
    "    & f(x, y; \\mathbf{p}) = (x - p_0)^2 + x y + (y + p_1)^2 - p_2 \\\\\n",
    "    \\text{where} \\quad \n",
    "    & \\mathbf{p} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "    & x, y \\in \\mathbb{R}\n",
    "\\end{align}\n",
    "\n",
    "This system has 3 outputs ($f$, $x$, $y$) and 3 inputs ($\\bar{p}$). So we may be interested in\n",
    "in how those outputs change at the next converged solution subject to small changes in $\\bar{p}$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{d x^*}{d \\bar{p}} \\\\\n",
    "    \\frac{d y^*}{d \\bar{p}} \\\\\n",
    "    \\frac{d f^*}{d \\bar{p}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "In an unconstrained problem, the optimizer is finding the \"bottom\" of a multidimensional bowl:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}}(\\bar\\theta, \\bar{p}) = \\nabla_\\theta f(\\bar{\\theta}, \\bar{p}) + \\cancel{\\lambda \\nabla_\\theta g(\\bar{\\theta}, \\bar{p})} &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\theta$ here represents the design variables instead of the typical $x$ in order to not confuse it with\n",
    "the inputs the to the paraboloid above.\n",
    "With two design variables, the residual vector $\\mathcal{R_{KKT}}$ has two elements.\n",
    "\n",
    "Our residual system has the following structure.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  -\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "      \\frac{df}{d\\bar{\\theta}} \\\\\n",
    "    \\end{bmatrix}\n",
    "  }\n",
    "  \\begin{bmatrix}\n",
    "     \\frac{d \\nabla_{\\theta} f}{d\\bar{\\theta}}  \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d \\nabla_{\\theta} f}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "or, expanding terms...\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df^*}{dp_0} & \\frac{df^*}{dp_1} & \\frac{df^*}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{df}{dp_0} & \\frac{df}{dp_1} & \\frac{df}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "  -\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "      \\frac{df}{dx} & \\frac{df}{dy} \\\\\n",
    "    \\end{bmatrix}\n",
    "  }\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_x f}{dx} & \\frac{d\\nabla_x f}{dy} \\\\\n",
    "      \\frac{d\\nabla_y f}{dx} & \\frac{d\\nabla_y f}{dy} \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_x f}{dp_0} & \\frac{d\\nabla_x f}{dp_1} & \\frac{d\\nabla_x f}{dp_2} \\\\\n",
    "      \\frac{d\\nabla_y f}{dp_0} & \\frac{d\\nabla_y f}{dp_1} & \\frac{d\\nabla_y f}{dp_2} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "In an unconstrained optimization such as this, the gradient $\\left[\\frac{df}{d\\bar{\\theta}}\\right]$ is zero, so the post-optimality sensitivity of the objective wrt the parameters is identical to the total derivative of the objective wrt the parameters.\n",
    "\n",
    "The following code generates plots that sweep the value of each parameter and plot the resulting objective value, overlaid by vectors indicating the calculated derivative $\\frac{df^*}{dp_i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[GRLAL0222090115:30937] shmem: mmap: an error occurred while determining whether or not /var/folders/zm/r34jq9lj0sx2qdt4pj65sfd00000gp/T//ompi.GRLAL0222090115.502/jf.0/127467520/sm_segment.GRLAL0222090115.502.7990000.0 could be created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominal Solution:\n",
      "   f*: [-27.33333333]\n",
      "   x*: [6.66666667]\n",
      "   y*: [-7.33333333]\n",
      "   df*/dp0: [[-7.33333333]]\n",
      "   df*/dp1: [[-6.66666667]]\n",
      "   df*/dp2: [[-1.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 76\u001b[0m\n\u001b[1;32m     73\u001b[0m prob\u001b[38;5;241m.\u001b[39mrun_driver()\n\u001b[1;32m     74\u001b[0m f_stars[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcopy(prob[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf_xy\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m est_multipliers, _ \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39m_get_lagrange_multipliers(driver_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, feas_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0E-6\u001b[39m)\n\u001b[1;32m     78\u001b[0m totals \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39mcompute_totals(of\u001b[38;5;241m=\u001b[39mofs, wrt\u001b[38;5;241m=\u001b[39mwrts, driver_scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# dfstar_dg = est_multipliers['g']\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# dg_dp = totals['g', 'p0']\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('parab', om.ExecComp('f_xy = (x-p0)**2 + x*y + (y+p1)**2 - p2'),\n",
    "                            promotes_inputs=['x', 'y', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f_xy'])\n",
    "\n",
    "# Design variables 'x' and 'y' span components, so we need to provide a common initial\n",
    "# value for them.\n",
    "prob.model.set_input_defaults('x', 3.0)\n",
    "prob.model.set_input_defaults('y', -4.0)\n",
    "prob.model.set_input_defaults('p0', 3.0)\n",
    "prob.model.set_input_defaults('p1', 4.0)\n",
    "prob.model.set_input_defaults('p2', 3.0)\n",
    "\n",
    "# setup the optimization\n",
    "prob.driver = om.pyOptSparseDriver()\n",
    "prob.driver.options['print_results'] = False\n",
    "prob.driver.options['optimizer'] = 'SLSQP'\n",
    "prob.driver.options['singular_jac_behavior'] = 'ignore'\n",
    "\n",
    "prob.model.add_design_var('x', lower=-50, upper=50)\n",
    "prob.model.add_design_var('y', lower=-50, upper=50)\n",
    "prob.model.add_objective('f_xy')\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "prob.run_model()\n",
    "driver_vars = prob.list_driver_vars(out_stream=None)\n",
    "des_vars = [dv for dv, _ in driver_vars['design_vars']]\n",
    "constraints = [dv for dv, _ in driver_vars['constraints']]\n",
    "objs = [dv for dv, _ in driver_vars['objectives']]\n",
    "other_ofs = []\n",
    "other_wrts = ['p0', 'p1', 'p2']\n",
    "\n",
    "ofs = objs + constraints + other_ofs\n",
    "wrts = des_vars + other_wrts\n",
    "\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "ax1 = axs[0]\n",
    "ax2 = axs[1]\n",
    "ax3 = axs[2]\n",
    "\n",
    "n_pts = 10\n",
    "ps = np.linspace(0, 10, n_pts)\n",
    "f_stars = np.zeros(n_pts)\n",
    "df_stars_dps = np.zeros((3, n_pts))\n",
    "\n",
    "prob.run_driver()\n",
    "# No constraints, no lagrange multipliers\n",
    "totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "print('Nominal Solution:')\n",
    "print('   f*:', prob.get_val('f_xy'))\n",
    "print('   x*:', prob.get_val('x'))\n",
    "print('   y*:', prob.get_val('y'))\n",
    "print('   df*/dp0:', totals['f_xy', 'p0'])\n",
    "print('   df*/dp1:', totals['f_xy', 'p1'])\n",
    "print('   df*/dp2:', totals['f_xy', 'p2'])\n",
    "\n",
    "\n",
    "for j in range(3):\n",
    "\n",
    "    prob.set_val('p0', 3)\n",
    "    prob.set_val('p1', 4)\n",
    "    prob.set_val('p2', 3)\n",
    "\n",
    "    for i, p in enumerate(ps):\n",
    "        prob[f'p{j}'] = p\n",
    "        prob.run_driver()\n",
    "        f_stars[i] = np.copy(prob['f_xy'])[0]\n",
    "\n",
    "        est_multipliers, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6)\n",
    "\n",
    "        totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "        # dfstar_dg = est_multipliers['g']\n",
    "        # dg_dp = totals['g', 'p0']\n",
    "        dfstar_dpj= totals['f_xy', f'p{j}']\n",
    "\n",
    "        # df_stars_dps[i] = np.copy(est_multipliers['g'] * (prob['x'] + prob['y']))\n",
    "        df_stars_dps[j, i] = np.copy(dfstar_dpj).ravel()[0]\n",
    "\n",
    "\n",
    "\n",
    "    uv = np.column_stack([np.ones_like(df_stars_dps[j]),\n",
    "                            df_stars_dps[j]])\n",
    "\n",
    "    uv_norm = np.linalg.norm(uv, axis=1)\n",
    "    uv /= uv_norm[:, np.newaxis]\n",
    "\n",
    "    axs[j].plot(ps,\n",
    "                f_stars,\n",
    "                zorder=0,\n",
    "                linewidth=2.0)\n",
    "    axs[j].quiver(ps,\n",
    "                f_stars,\n",
    "                uv[:,0],\n",
    "                uv[:,1],\n",
    "                angles=\"xy\",\n",
    "                pivot=\"mid\",\n",
    "                color=\"red\",\n",
    "                width=0.01,\n",
    "                headwidth=2,\n",
    "                headlength=1.0,\n",
    "                headaxislength=1.0,\n",
    "                minshaft=0.1,\n",
    "                minlength=0.5)\n",
    "\n",
    "    axs[j].set(xlabel=f'$p_{j}$', ylabel=r'$f^*$')\n",
    "    axs[j].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal design variable sensitivities for unconstrained optimization\n",
    "\n",
    "In the section above, we demonstrate the ability to get the post-optimality sensitivity of the objective with respect to some parameters of the problem.\n",
    "\n",
    "Now we want to know, how would the optimal values of $x$ and $y$ change as the values of $\\bar{p}$ are changed?\n",
    "The KKT residual is still the same for the unconstrained case.\n",
    "Viewing the problem as an implicit solve, we have the following linear system for the residuals\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\bar{\\theta}}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\cancel{\n",
    "    \\begin{bmatrix}\n",
    "      \\frac{d{\\bar{\\theta}}}{d{\\bar{p}}} \\\\\n",
    "    \\end{bmatrix}\n",
    "  }\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\bar{\\theta}}}{d{\\bar{\\theta}}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d{\\bar{\\theta}}} \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The term $\\frac{d{\\bar{\\theta}}}{d{\\bar{p}}}$ drops to zero because the unoptimized design variables are not impacted by the choice of the parameters.\n",
    "The term $\\frac{d{\\bar{\\theta}}}{d{\\bar{\\theta}}}$ is just an identity matrix, so we're left with:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\bar{\\bar{\\theta}}}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d{\\bar{\\theta}}} \\\\\n",
    "  \\end{bmatrix}^{-1}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{align*}\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d{\\bar{\\theta}}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d{\\bar{\\theta}}^*}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  -\n",
    "  \\begin{bmatrix}\n",
    "      \\frac{d\\nabla_{\\bar{\\theta}} {f}}{d\\bar{p}} \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "So given $\\frac{d\\nabla_{\\bar{\\theta}} {f}}{d{\\bar{\\theta}}}$ and $\\frac{d\\nabla_{\\bar{\\theta}} {f}}{d\\bar{p}}$, a linear solve provides the sensitivities $\\frac{d{\\bar{\\theta}}^*}{d\\bar{p}}$.\n",
    "\n",
    "But how do we obtain these?\n",
    "\n",
    "A derivative of a gradient is a second derivative, and OpenMDAO currently does not provide second derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the Lagrange Multipliers\n",
    "\n",
    "In a constrained optimization problem, Lagrange multiplier terms are included in the KKT residual for each active constraint.\n",
    "The presence of active constraints means that the optimization will not find the \"bottom of the bowl\", but rather be limited to finding some minimal value on the surface of the bowl where the active constraint residual is zero.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{R_{KKT}} =\n",
    "  \\begin{bmatrix}\n",
    "  \\nabla_{\\bar{\\theta}} f + \\nabla_{\\bar{\\theta}} \\bar{g}^T \\bar\\lambda + \\nabla_{\\bar{\\theta}} \\bar{h}^T_\\mathcal{A} \\bar\\mu_\\mathcal{A} + \\bar\\nu_\\mathcal{A}\n",
    "  \\end{bmatrix} &= \\bar{0}\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\bar{g}$ are the equality constraints, $\\bar{h}_\\mathcal{A}$ are the active inequality constraints.\n",
    "\n",
    "Lumping all active constraints and bounds into a single pseudo-equality-constraint vector $\\mathcal{G}$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "  \\nabla_{\\bar{\\theta}} \\mathcal{G}^T \\begin{bmatrix}\\bar\\lambda \\\\ \\bar\\mu \\\\ \\bar\\nu \\end{bmatrix} = -\\nabla_{\\bar{\\theta}} f\n",
    "\\end{align*}\n",
    "\n",
    "The gradients in the above equation are obtained by a call to OpenMDAO's `problem.compute_totals` method.\n",
    "\n",
    "From this we can find the multipliers using a least squares method in order to allow a little slack due to numerical issues.\n",
    "These multipliers give the _post-optimality sensitivities_ of the objective $f$ with respect to the active constraint bound values for the equality constraints, inequality constraints, and design variables, respectively.\n",
    "\n",
    "Now suppose, having already found the optimal solution that minimizes $f$, we treat the design variable vector $\\bar\\theta$ as if it were equality-constrained to those optimal values:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathcal{G} = \\begin{bmatrix}x - x* \\\\ y - y* \\end{bmatrix} &= \\left[ 0 \\right]\n",
    "\\end{align*}\n",
    "\n",
    "If we were to perturb the design variables $x$ and $y$ away from their optimal values and solve the KKT system, the resulting multpliers would approximate the sensitivities of the objective with respect to the design variables, $\\frac{d\\nabla_{{\\bar{\\theta}}} {f}}{d{{\\bar{\\theta}}}}$.\n",
    "\n",
    "Similarly, a perturbation of the parameter values and treating them as if they were constraints would give us $\\frac{d\\nabla_{{\\bar{\\theta}}} {f}}{d{\\bar{p}}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f* [-27.33333333]\n",
      "x* [6.66666667]\n",
      "y* [-7.33333333]\n",
      "\n",
      "{}\n",
      "\n",
      "perturbed x sensitivities via lagrange multipliers\n",
      "[2.]\n",
      "[1.]\n",
      "\n",
      "perturbed y sensitivities via lagrange multipliers\n",
      "[1.00000001]\n",
      "[2.]\n",
      "[[2.         1.00000001]\n",
      " [1.         2.        ]]\n",
      "\n",
      "perturbed p0 sensitivities via lagrange multipliers\n",
      "[-2.]\n",
      "[-8.15663058e-09]\n",
      "\n",
      "perturbed p1 sensitivities via lagrange multipliers\n",
      "-0.0\n",
      "1.9999999956073102\n",
      "\n",
      "perturbed p2 sensitivities via lagrange multipliers\n",
      "[-0.]\n",
      "[-8.15663058e-09]\n",
      "[[ 1.33333333e+00  6.66666670e-01 -2.71887688e-09]\n",
      " [-6.66666659e-01 -1.33333334e+00  5.43775374e-09]]\n",
      "df*/dp0 [[-7.33333333]]\n",
      "df*/dp1 [[-6.66666667]]\n",
      "df*/dp2 [[-1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('parab', om.ExecComp('f = (x-p0)**2 + x*y + (y+p1)**2 - p2'),\n",
    "                            promotes_inputs=['x', 'y', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f'])\n",
    "\n",
    "# Design variables 'x' and 'y' span components, so we need to provide a common initial\n",
    "# value for them.\n",
    "prob.model.set_input_defaults('x', 3.0)\n",
    "prob.model.set_input_defaults('y', -4.0)\n",
    "prob.model.set_input_defaults('p0', 3.0)\n",
    "prob.model.set_input_defaults('p1', 4.0)\n",
    "prob.model.set_input_defaults('p2', 3.0)\n",
    "\n",
    "# setup the optimization\n",
    "prob.driver = om.pyOptSparseDriver()\n",
    "prob.driver.options['print_results'] = False\n",
    "prob.driver.options['optimizer'] = 'SLSQP'\n",
    "prob.driver.options['singular_jac_behavior'] = 'ignore'\n",
    "\n",
    "prob.model.add_design_var('x', lower=-50, upper=50)\n",
    "prob.model.add_design_var('y', lower=-50, upper=50)\n",
    "prob.model.add_objective('f')\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "PERTURB_H = 1.0E-7\n",
    "\n",
    "p0 = 3.\n",
    "p1 = 4.\n",
    "p2 = 3.\n",
    "prob.set_val('p0', p0)\n",
    "prob.set_val('p1', p1)\n",
    "prob.set_val('p2', p2)\n",
    "\n",
    "prob.run_driver()\n",
    "\n",
    "fstar = np.copy(prob.get_val('f'))\n",
    "xstar = np.copy(prob.get_val('x'))\n",
    "ystar = np.copy(prob.get_val('y'))\n",
    "\n",
    "# # NOTE: We get the wrong answers if we don't use np.copy here.\n",
    "# fstar = prob.get_val('f')\n",
    "# xstar = prob.get_val('x')\n",
    "# ystar = prob.get_val('y')\n",
    "\n",
    "print('f*', fstar)\n",
    "print('x*', xstar)\n",
    "print('y*', ystar)\n",
    "\n",
    "print()\n",
    "\n",
    "# The nominal multipliers\n",
    "nom_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6)\n",
    "\n",
    "print(nom_mult)\n",
    "\n",
    "driver_vars = prob.list_driver_vars(out_stream=None)\n",
    "des_vars = [dv for dv, _ in driver_vars['design_vars']]\n",
    "constraints = [dv for dv, _ in driver_vars['constraints']]\n",
    "objs = [dv for dv, _ in driver_vars['objectives']]\n",
    "other_ofs = []\n",
    "other_wrts = ['p0', 'p1', 'p2']\n",
    "\n",
    "ofs = objs + constraints + other_ofs\n",
    "wrts = des_vars + other_wrts\n",
    "\n",
    "totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "# Perturb the design variables to obtain dgradf/dtheta\n",
    "# First do x\n",
    "\n",
    "prob.set_val('x', xstar + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "dR_dtheta = np.zeros((2, 2), dtype=float)\n",
    "\n",
    "pert_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "print('\\nperturbed x sensitivities via lagrange multipliers')\n",
    "print(-pert_mult['x'] / PERTURB_H)\n",
    "print(-pert_mult['y'] / PERTURB_H)\n",
    "dR_dtheta[0, 0] = -pert_mult['x'][0] / PERTURB_H\n",
    "dR_dtheta[1, 0] = -pert_mult['y'][0] / PERTURB_H\n",
    "# Now do y\n",
    "prob.set_val('x', xstar)\n",
    "prob.set_val('y', ystar + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "pert_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "print('\\nperturbed y sensitivities via lagrange multipliers')\n",
    "print(-pert_mult['x'] / PERTURB_H)\n",
    "print(-pert_mult['y'] / PERTURB_H)\n",
    "# dR_dtheta[:, 1] = np.ravel(-pert_mult['x'] / PERTURB_H)\n",
    "dR_dtheta[0, 1] = -pert_mult['x'][0] / PERTURB_H\n",
    "dR_dtheta[1, 1] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "# dR/dtheta\n",
    "print(dR_dtheta)\n",
    "\n",
    "# Reset back to optimal values\n",
    "prob.set_val('x', xstar)\n",
    "prob.set_val('y', ystar)\n",
    "prob.run_model()\n",
    "\n",
    "dR_dp = np.zeros((2, 3), dtype=float)\n",
    "\n",
    "# THE PERTURBED P0 GRADIENTS\n",
    "\n",
    "prob.set_val('p0', p0 + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "pert_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "print('\\nperturbed p0 sensitivities via lagrange multipliers')\n",
    "print(-pert_mult['x'] / PERTURB_H)\n",
    "print(-pert_mult['y'] / PERTURB_H)\n",
    "\n",
    "dR_dp[0, 0] = -pert_mult['x'][0] / PERTURB_H\n",
    "dR_dp[1, 0] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "prob.set_val('p0', p0)\n",
    "prob.set_val('p1', p1 + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "pert_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "print('\\nperturbed p1 sensitivities via lagrange multipliers')\n",
    "print(-pert_mult['x'][0] / PERTURB_H)\n",
    "print(-pert_mult['y'][0] / PERTURB_H)\n",
    "\n",
    "dR_dp[0, 1] = -pert_mult['x'][0] / PERTURB_H\n",
    "dR_dp[1, 1] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "prob.set_val('p1', p1)\n",
    "prob.set_val('p2', p2 + PERTURB_H)\n",
    "prob.run_model()\n",
    "\n",
    "pert_mult, active_bounds, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "print('\\nperturbed p2 sensitivities via lagrange multipliers')\n",
    "print(-pert_mult['x'] / PERTURB_H)\n",
    "print(-pert_mult['y'] / PERTURB_H)\n",
    "\n",
    "dR_dp[0, 2] = -pert_mult['x'][0] / PERTURB_H\n",
    "dR_dp[1, 2] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "dthetastar_dp, optimality_squared, rank, singular_vals = np.linalg.lstsq(dR_dtheta, -dR_dp, rcond=None)\n",
    "\n",
    "print(dthetastar_dp)\n",
    "\n",
    "dfstar_dp = np.hstack((totals['f', 'p0'], totals['f', 'p1'], totals['f', 'p2']))\n",
    "\n",
    "print('df*/dp0', totals['f', 'p0'])\n",
    "print('df*/dp1', totals['f', 'p1'])\n",
    "print('df*/dp2', totals['f', 'p2'])\n",
    "\n",
    "sensitivity_matrix = np.vstack((dfstar_dp, dthetastar_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import (\n",
    "    ColumnDataSource, ColorBar, LinearColorMapper, BasicTicker,\n",
    "    PrintfTickFormatter, LabelSet\n",
    ")\n",
    "from bokeh.transform import transform\n",
    "from seaborn import diverging_palette\n",
    "\n",
    "# Sensitivity matrix\n",
    "matrix = sensitivity_matrix[::-1, :]\n",
    "original_matrix = matrix.copy()\n",
    "\n",
    "row_labels = ['f', 'x', 'y']\n",
    "col_labels = ['p0', 'p1', 'p2']\n",
    "nrows, ncols = matrix.shape\n",
    "\n",
    "# Flatten for plotting\n",
    "x = np.tile(col_labels, nrows)\n",
    "y = np.repeat(row_labels[::-1], ncols)  # Reverse so 'f' is at the top\n",
    "values = matrix.flatten()\n",
    "\n",
    "# Normalize to get text contrast\n",
    "norm_vals = (values - matrix.min()) / (matrix.max() - matrix.min())\n",
    "text_colors = ['black' if val > 0.5 else 'white' for val in norm_vals]\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    value=values,\n",
    "    text_labels=[f\"{v:.2f}\" for v in values],\n",
    "    text_color=text_colors\n",
    "))\n",
    "\n",
    "# Color mapper\n",
    "cmap_min = np.min((matrix.min(), -matrix.max()))\n",
    "cmap_max = np.max((matrix.max(), -matrix.min()))\n",
    "\n",
    "purple_green = [270, 120]\n",
    "blue_orange = [240, 30]\n",
    "blue_red = [240, 10]\n",
    "teal_rust = [200, 10]\n",
    "slate_yellow = [220, 60]\n",
    "\n",
    "pal = diverging_palette(*purple_green, center='light', n=max(matrix.size, 256))\n",
    "# hex_palette = [seaborn.utils.rgb2hex(rgb) for rgb in pal]\n",
    "\n",
    "# Now use this in Bokeh's color mapper\n",
    "mapper = LinearColorMapper(palette=pal.as_hex(), low=cmap_min, high=cmap_max)\n",
    "\n",
    "p = figure(title=\"OpenMDAO Sensitivity Heatmap\",\n",
    "           x_range=col_labels, y_range=list(reversed(row_labels)),\n",
    "           x_axis_location=\"above\", width=600, height=600,\n",
    "           tools=\"\", toolbar_location=None)\n",
    "\n",
    "p.rect(x=\"x\", y=\"y\", width=1, height=1, source=source,\n",
    "       fill_color=transform('value', mapper), line_color=None)\n",
    "\n",
    "labels = LabelSet(x='x', y='y', text='text_labels', text_color='text_color',\n",
    "                  source=source, text_align='center', text_baseline='middle')\n",
    "p.add_layout(labels)\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, location=(0, 0),\n",
    "                     ticker=BasicTicker(desired_num_ticks=10),\n",
    "                     formatter=PrintfTickFormatter(format=\"%.2f\"))\n",
    "p.add_layout(color_bar, 'right')\n",
    "\n",
    "# TextInput widgets for scaling\n",
    "# inputs = [TextInput(value=\"1.0\", title=f\"Scale for {label}\") for label in col_labels]\n",
    "\n",
    "# JavaScript callback to apply scaling\n",
    "# callback = CustomJS(args=dict(source=source, inputs=inputs), code=f\"\"\"\n",
    "#     const data = source.data;\n",
    "#     const scales = inputs.map(inp => parseFloat(inp.value) || 1.0);\n",
    "#     const orig = {sensitivity_matrix};\n",
    "#     const x = data['x'];\n",
    "#     const y = data['y'];\n",
    "#     const new_vals = [];\n",
    "#     const new_labels = [];\n",
    "#     const new_colors = [];\n",
    "#     \"\"\"\n",
    "#     +\n",
    "#     \"\"\"\n",
    "#     function getTextColor(v, vmin, vmax) {\n",
    "#         const norm = (v - vmin) / (vmax - vmin);\n",
    "#         return norm > 0.5 ? 'black' : 'white';\n",
    "#     }\n",
    "\n",
    "#     let vmin = Infinity, vmax = -Infinity;\n",
    "\n",
    "#     for (let i = 0; i < 3; i++) {         // rows\n",
    "#         for (let j = 0; j < 3; j++) {     // cols\n",
    "#             const v = orig[i][j] * scales[j];\n",
    "#             new_vals.push(v);\n",
    "#             if (v < vmin) vmin = v;\n",
    "#             if (v > vmax) vmax = v;\n",
    "#         }\n",
    "#     }\n",
    "\n",
    "#     for (let k = 0; k < new_vals.length; k++) {\n",
    "#         new_labels[k] = new_vals[k].toFixed(2);\n",
    "#     }\n",
    "\n",
    "#     for (let k = 0; k < new_vals.length; k++) {\n",
    "#         new_colors[k] = getTextColor(new_vals[k], vmin, vmax);\n",
    "#     }\n",
    "\n",
    "#     data['value'] = new_vals;\n",
    "#     data['text_labels'] = new_labels;\n",
    "#     data['text_color'] = new_colors;\n",
    "#     source.change.emit();\n",
    "# \"\"\")\n",
    "\n",
    "# # Connect widgets to callback\n",
    "# for inp in inputs:\n",
    "#     inp.js_on_change(\"value\", callback)\n",
    "\n",
    "# Layout\n",
    "# layout = column(p, row(*inputs))\n",
    "# curdoc().add_root(layout)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimization\n",
    "\n",
    "For constrained optimization (and the more general case), the nominal Lagrange multipliers are no longer zero, so when perturbing each design variable and parameter we need to take the sensitivity to be the finite difference between the nominal value and the perturbed value.\n",
    "\n",
    "We were technically doing this before, but the nominal multiplier values were always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f* [-7.08333333]\n",
      "x* [2.16666667]\n",
      "y* [-2.83333333]\n",
      "\n",
      "['h']\n",
      "{'h': array([-4.5])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rfalck/Codes/OpenMDAO.git/openmdao/utils/coloring.py:448: DerivativesWarning:'parab' <class ExecComp>: Coloring was deactivated.  Improvement of 0.0% was less than min allowed (5.0%).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('parab', om.ExecComp(['f = (x-p0)**2 + x*y + (y+p1)**2 - p2',\n",
    "                                               'h = y - x + 5']),\n",
    "                            promotes_inputs=['x', 'y', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f', 'h'])\n",
    "\n",
    "# Design variables 'x' and 'y' span components, so we need to provide a common initial\n",
    "# value for them.\n",
    "prob.model.set_input_defaults('x', 3.0)\n",
    "prob.model.set_input_defaults('y', -4.0)\n",
    "prob.model.set_input_defaults('p0', 3.0)\n",
    "prob.model.set_input_defaults('p1', 4.0)\n",
    "prob.model.set_input_defaults('p2', 3.0)\n",
    "\n",
    "# setup the optimization\n",
    "prob.driver = om.pyOptSparseDriver()\n",
    "prob.driver.options['print_results'] = False\n",
    "prob.driver.options['optimizer'] = 'SLSQP'\n",
    "prob.driver.options['singular_jac_behavior'] = 'ignore'\n",
    "\n",
    "prob.model.add_design_var('x', lower=-50, upper=50)\n",
    "prob.model.add_design_var('y', lower=-50, upper=50)\n",
    "prob.model.add_objective('f')\n",
    "prob.model.add_constraint('h', lower=0.0)\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "PERTURB_H = 1.0E-7\n",
    "\n",
    "p0 = 3.\n",
    "p1 = 4.\n",
    "p2 = 3.\n",
    "prob.set_val('p0', p0)\n",
    "prob.set_val('p1', p1)\n",
    "prob.set_val('p2', p2)\n",
    "\n",
    "prob.run_driver()\n",
    "\n",
    "fstar = np.copy(prob.get_val('f'))\n",
    "xstar = np.copy(prob.get_val('x'))\n",
    "ystar = np.copy(prob.get_val('y'))\n",
    "\n",
    "# # NOTE: We get the wrong answers if we don't use np.copy here.\n",
    "# fstar = prob.get_val('f')\n",
    "# xstar = prob.get_val('x')\n",
    "# ystar = prob.get_val('y')\n",
    "\n",
    "print('f*', fstar)\n",
    "print('x*', xstar)\n",
    "print('y*', ystar)\n",
    "\n",
    "print()\n",
    "\n",
    "# The nominal multipliers\n",
    "nom_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6)\n",
    "\n",
    "print(nom_mult)\n",
    "\n",
    "# driver_vars = prob.list_driver_vars(out_stream=None)\n",
    "# des_vars = [dv for dv, _ in driver_vars['design_vars']]\n",
    "# constraints = [dv for dv, _ in driver_vars['constraints']]\n",
    "# objs = [dv for dv, _ in driver_vars['objectives']]\n",
    "# other_ofs = []\n",
    "# other_wrts = ['p0', 'p1', 'p2']\n",
    "\n",
    "# ofs = objs + constraints + other_ofs\n",
    "# wrts = des_vars + other_wrts\n",
    "\n",
    "# totals = prob.compute_totals(of=ofs, wrt=wrts, driver_scaling=False)\n",
    "\n",
    "# # Perturb the design variables to obtain dgradf/dtheta\n",
    "# # First do x\n",
    "\n",
    "# prob.set_val('x', xstar + PERTURB_H)\n",
    "# prob.run_model()\n",
    "\n",
    "# dR_dtheta = np.zeros((2, 2), dtype=float)\n",
    "\n",
    "# pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "# print('\\nperturbed x sensitivities via lagrange multipliers')\n",
    "# print(-pert_mult['x'] / PERTURB_H)\n",
    "# print(-pert_mult['y'] / PERTURB_H)\n",
    "# dR_dtheta[0, 0] = -pert_mult['x'][0] / PERTURB_H\n",
    "# dR_dtheta[1, 0] = -pert_mult['y'][0] / PERTURB_H\n",
    "# # Now do y\n",
    "# prob.set_val('x', xstar)\n",
    "# prob.set_val('y', ystar + PERTURB_H)\n",
    "# prob.run_model()\n",
    "\n",
    "# pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "# print('\\nperturbed y sensitivities via lagrange multipliers')\n",
    "# print(-pert_mult['x'] / PERTURB_H)\n",
    "# print(-pert_mult['y'] / PERTURB_H)\n",
    "# # dR_dtheta[:, 1] = np.ravel(-pert_mult['x'] / PERTURB_H)\n",
    "# dR_dtheta[0, 1] = -pert_mult['x'][0] / PERTURB_H\n",
    "# dR_dtheta[1, 1] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "# # dR/dtheta\n",
    "# print(dR_dtheta)\n",
    "\n",
    "# # Reset back to optimal values\n",
    "# prob.set_val('x', xstar)\n",
    "# prob.set_val('y', ystar)\n",
    "# prob.run_model()\n",
    "\n",
    "# dR_dp = np.zeros((2, 3), dtype=float)\n",
    "\n",
    "# # THE PERTURBED P0 GRADIENTS\n",
    "\n",
    "# prob.set_val('p0', p0 + PERTURB_H)\n",
    "# prob.run_model()\n",
    "\n",
    "# pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "# print('\\nperturbed p0 sensitivities via lagrange multipliers')\n",
    "# print(-pert_mult['x'] / PERTURB_H)\n",
    "# print(-pert_mult['y'] / PERTURB_H)\n",
    "\n",
    "# dR_dp[0, 0] = -pert_mult['x'][0] / PERTURB_H\n",
    "# dR_dp[1, 0] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "# prob.set_val('p0', p0)\n",
    "# prob.set_val('p1', p1 + PERTURB_H)\n",
    "# prob.run_model()\n",
    "\n",
    "# pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "# print('\\nperturbed p1 sensitivities via lagrange multipliers')\n",
    "# print(-pert_mult['x'] / PERTURB_H)\n",
    "# print(-pert_mult['y'] / PERTURB_H)\n",
    "\n",
    "# dR_dp[0, 1] = -pert_mult['x'][0] / PERTURB_H\n",
    "# dR_dp[1, 1] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "# prob.set_val('p1', p1)\n",
    "# prob.set_val('p2', p2 + PERTURB_H)\n",
    "# prob.run_model()\n",
    "\n",
    "# pert_mult, _ = prob.driver._get_lagrange_multipliers(driver_scaling=False, feas_tol=1.0E-6, assume_dvs_active=True)\n",
    "# print('\\nperturbed p2 sensitivities via lagrange multipliers')\n",
    "# print(-pert_mult['x'] / PERTURB_H)\n",
    "# print(-pert_mult['y'] / PERTURB_H)\n",
    "\n",
    "# dR_dp[0, 2] = -pert_mult['x'][0] / PERTURB_H\n",
    "# dR_dp[1, 2] = -pert_mult['y'][0] / PERTURB_H\n",
    "\n",
    "# dthetastar_dp, optimality_squared, rank, singular_vals = np.linalg.lstsq(dR_dtheta, -dR_dp, rcond=None)\n",
    "\n",
    "# print(dthetastar_dp)\n",
    "\n",
    "# dfstar_dp = np.hstack((totals['f', 'p0'], totals['f', 'p1'], totals['f', 'p2']))\n",
    "\n",
    "# print(f'df*/dp0', totals['f', 'p0'])\n",
    "# print(f'df*/dp1', totals['f', 'p1'])\n",
    "# print(f'df*/dp2', totals['f', 'p2'])\n",
    "\n",
    "# sensitivity_matrix = np.vstack((dfstar_dp, dthetastar_dp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Under' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mUnder\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Under' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
