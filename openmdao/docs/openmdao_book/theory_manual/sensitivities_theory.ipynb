{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Post-Optimality Sensitivities of a Constrained Optimization Problem\n",
    "\n",
    "Lets consider a problem such that we have an active bound and an active inequality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta_0,\\, \\theta_1} \\quad & f(\\theta_0, \\theta_1; \\mathbf{p}) = (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\\\\n",
    "\\text{where} \\quad \\mathbf{p} &= \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "\\text{bounds:} \\quad \\theta_0 &\\le 6 \\\\\n",
    "\\text{equality constraints:} \\quad \\theta_1 &= -\\theta_0\n",
    "\\end{align*}\n",
    "\n",
    "Then, if we want to know the sensitivity of the optimization to the value of $\\theta_0^{ub}$, as if is another parameter to the problem, we can assume $\\theta_0^{ub}$ is just another element in our parameter vector:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{p} &= \\begin{bmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\theta_0^{ub} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "If active, we can treat the bound on $\\theta_0$ as just another equality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{\\mathcal{G}}(\\bar{\\theta}, \\bar{p}) &= \\begin{bmatrix}\n",
    "                                   \\theta_0 + \\theta_1 \\\\\n",
    "                                   \\theta_0 - p_3\n",
    "                                \\end{bmatrix} = \\bar 0\n",
    "\\end{align*}\n",
    "\n",
    "**How will my system design ($\\bar{\\theta}^*$) respond to changes in my assumptions and system inputs ($\\bar{p}$)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Derivatives Equation\n",
    "\n",
    "The UDE is:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right] \\left[ \\frac{d u}{d \\mathcal{R}} \\right]\n",
    "  &=\n",
    "  \\left[ I \\right]\n",
    "  =\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right]^T \\left[ \\frac{d u}{d \\mathcal{R}} \\right]^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, the residuals are the primal and dual residuals of the optimization process, given above.\n",
    "\n",
    "There's a lot of nomenclature collisions, so let's define the following going forward.\n",
    "\n",
    "- Post-optimization, the unknowns ($\\bar{u}$) in our system are the resulting design variable values ($\\bar{\\theta}$).\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{u} &= \\bar{\\theta}\n",
    "\\end{align*}\n",
    "\n",
    "- The independent variables ($\\bar{x}$) of this system are those inputs for which we want to determine the sensitivity of the ouputs: the bounding values of the active constraints as well as any other parameters that are inputs to the model but not design variables controlled by the optimizer.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{x} &= \\bar{p}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the UDE to solving post-optimality sensitivities\n",
    "\n",
    "In our case, the unknowns vector consists of\n",
    "- the optimization parameters which includes the bounding values of active constraints ($\\bar{p}$)\n",
    "- the design variables of the optimization ($\\bar{\\theta}$)\n",
    "- the Lagrange multipliers of the optimization ($\\bar{\\lambda}$)\n",
    "- the objective value **as well as** any other outputs for which we want the sensitivities ($f$)\n",
    "\n",
    "The total size of the unknowns vector is $N_p + N_{\\theta} + N_{\\lambda} + N_{f}$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{u} &=\n",
    "  \\begin{bmatrix}\n",
    "    \\hat{p} \\\\\n",
    "    \\bar{\\theta} \\\\\n",
    "    \\bar{\\lambda} \\\\\n",
    "    \\bar{f}\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Under the UDE, the corresponding residual equations for these unknowns are\n",
    "- the implicit form of the independent variable values\n",
    "- the stationarity condition\n",
    "- the active constraints\n",
    "- the implicit form of the explicit calculations of $f$ and $y$\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{\\mathcal{R}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathcal{R}}_p \\\\\n",
    "\\bar{\\mathcal{R}}_{\\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{g} \\\\\n",
    "\\bar{\\mathcal{R}}_{f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\bar{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ -\\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{ab} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda} - \\check{g}_{ab} \\left( \\bar{\\theta}, \\bar{p} \\right) \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  f - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  p_0 - \\check{p}_0 \\\\[1.1ex]\n",
    "  p_1 - \\check{p}_1 \\\\[1.1ex]\n",
    "  p_2 - \\check{p}_2 \\\\[1.1ex]\n",
    "  p_3 - \\check{p}_3 \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ -\\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{ab} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  r_{\\lambda_0} - \\left[ \\theta_0 + \\theta_1 \\right] \\\\[1.1ex]\n",
    "  r_{\\lambda_1} - \\left[\\theta_0 - p_4 \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  f - \\left[ (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\right]\n",
    "\\end{bmatrix}\n",
    "&= \\bar 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the total derivatives that we seek ($\\frac{d f^*}{d \\bar{p}}$ and $\\frac{d \\bar{\\theta}^*}{d \\bar{p}}$), we need $\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}$.\n",
    "\n",
    "The optimizer has served as the nonlinear solver in this case which has computed the values in the unknowns vector: $\\bar{\\theta}$, $\\bar{\\lambda}$, and $\\bar{f}$ such that the residuals are satisfied.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_p}}{\\partial \\bar{p}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_g}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_g}}{\\partial \\bar{\\theta}} & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{\\theta}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\check{\\mathcal{L}}}{\\partial \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\frac{\\partial \\check g}{\\partial \\bar{\\theta}} & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This nomenclature can be a bit confusing.\n",
    "\n",
    "**The _partial_ derivatives of the post-optimality residuals are the _total_ derivatives of the analysis.**\n",
    "\n",
    "In this case of the stationarity residuals $\\mathcal{R}_{\\bar{\\theta}}$, which already include _total_ derivatives of the analysis for the objective and constraint gradients, second derivatives are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding total derivaties which we need to solve for are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\bar{u}}{d \\bar{\\mathcal{R}}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d f}{d \\bar{\\mathcal{R}_p}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{\\mathcal{R}_f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{p}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d f}{d \\bar{p}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we can solve them with four linear solves of the forward system, or three solves of the reverse system.\n",
    "\n",
    "TODO: Need to explain how du/dRf becomes du/df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDE for this problem, in forward form, is\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\nabla \\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    0 & \\left[ I_\\theta \\right] & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_\\lambda \\right] & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we have four parameters and thus four columns for which we need to solve the system.\n",
    "\n",
    "Alternatively, we have three rows of interest in this system...two for the design variables $\\theta_0$ and $\\theta_1$ and one for the objective $f$.\n",
    "\n",
    "Taking the transpose and solving this system using the reverse form would require three linear system solves.\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\nabla\\check{\\mathcal{L}}}{\\partial \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{\\partial \\check g}{\\partial \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{\\partial \\check f}{\\partial \\bar{p}} & -\\frac{\\partial \\check f}{\\partial \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix} ^T\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{f}}\n",
    "\\end{bmatrix} ^T\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    0 & \\left[ I_\\theta \\right] & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_\\lambda \\right] & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix} ^T\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets use OpenMDAO to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -25.999999999999993\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization Complete\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Problem: problem14\n",
       "Driver:  ScipyOptimizeDriver\n",
       "  success     : True\n",
       "  iterations  : 3\n",
       "  runtime     : 1.2067E-01 s\n",
       "  model_evals : 3\n",
       "  model_time  : 2.2514E-02 s\n",
       "  deriv_evals : 2\n",
       "  deriv_time  : 9.4140E-02 s\n",
       "  exit_status : SUCCESS"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import openmdao.api as om\n",
    "\n",
    "\n",
    "class ObjComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('f', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "        return np.array([f])\n",
    "\n",
    "class ConComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('g', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        g = Θ[0] + Θ[1]\n",
    "        return np.array([g])\n",
    "\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('f_comp', ObjComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "prob.model.add_subsystem('g_comp', ConComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "\n",
    "prob.model.add_design_var('Θ', upper=[6., None])\n",
    "prob.model.add_constraint('g', equals=0.)\n",
    "prob.model.add_objective('f')\n",
    "\n",
    "prob.driver = om.ScipyOptimizeDriver()\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "prob.set_val('p', [3, 4, 3, 6])\n",
    "\n",
    "prob.run_driver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Variables(s) in 'model'\n",
      "\n",
      "varname  val                  io      prom_name\n",
      "-------  -------------------  ------  ---------\n",
      "f_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  f      [-26.]               output  f        \n",
      "g_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  g      [1.77635684e-15]     output  g        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob.model.list_vars(print_arrays=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dvs, active_cons = prob.driver.compute_lagrange_multipliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets form the UDE system and compute the sensitivities, outside of OpenMDAO first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert OpenMDAO values to jax arrays\n",
    "\n",
    "f_opt = np.array(prob.get_val('f'))\n",
    "Θ_opt = np.array(prob.get_val('Θ'))\n",
    "p = np.array(prob.get_val('p'))\n",
    "\n",
    "# The lagrange multipliers of the active constraints are\n",
    "λ_opt = np.array([active_dvs['Θ']['multipliers'][active_dvs['Θ']['indices']],\n",
    "              active_cons['g']['multipliers'][active_cons['g']['indices']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our objective and active constraint (and bounds) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(Θ, p):\n",
    "    f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return np.array([f])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    return np.array([Θ[0] + Θ[1],\n",
    "                     Θ[0] - p[3]])\n",
    "\n",
    "# def df_dΘ(Θ, p):\n",
    "#     df_dΘ = [[2 * (Θ[0] - p[0]) + Θ[1]],\n",
    "#              [Θ[0] + 2 * (Θ[1] + p[1])]]\n",
    "#     return np.array(df_dΘ)\n",
    "\n",
    "# def df_dp(Θ, p):\n",
    "#     df_dp = [[-2 * (Θ[0] - p[0])],\n",
    "#              [2 * (Θ[1] + p[1])],\n",
    "#              [-1],\n",
    "#              [0]]\n",
    "#     return np.array(df_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Θ*:\n",
      "[ 6. -6.]\n",
      "\n",
      "λ*:\n",
      "[[ 2.]\n",
      " [-2.]]\n",
      "\n",
      "∂f/∂Θ:\n",
      "[[1.77635684e-15 2.00000000e+00]]\n",
      "\n",
      "∂f/∂p:\n",
      "[[-6. -4. -1.  0.]]\n",
      "\n",
      "∂g/∂Θ:\n",
      "[[1. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "∂g/∂p:\n",
      "[[ 0.  0.  0. -0.]\n",
      " [ 0.  0.  0. -1.]]\n",
      "\n",
      "∇L:\n",
      "[[-3.10862447e-15]\n",
      " [-1.77635684e-15]]\n",
      "\n",
      "∇²f (via jacobian of jacobian):\n",
      "[[[2. 1.]\n",
      "  [1. 2.]]]\n",
      "\n",
      "∂∇f/∂p (via jacobian of jacobian):\n",
      "[[[-2.  0.  0.  0.]\n",
      "  [ 0.  2.  0.  0.]]]\n",
      "\n",
      "∇²g (via jacobian of jacobian):\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n",
      "\n",
      "∂∇g/∂p (via jacobian of jacobian):\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "I_p:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "I_f:\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# The design vars, from OpenMDAO\n",
    "print('\\nΘ*:')\n",
    "print(Θ_opt)\n",
    "\n",
    "# The Lagrange multipliers, from OpenMDAO\n",
    "print(\"\\nλ*:\")\n",
    "print(λ_opt)\n",
    "\n",
    "# Jacobian of f with respect to Θ\n",
    "df_dΘ = jax.jacobian(f, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂Θ:\")\n",
    "print(df_dΘ)\n",
    "\n",
    "# Jacobian of f with respect to p\n",
    "df_dp = jax.jacobian(f, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂p:\")\n",
    "print(df_dp)\n",
    "\n",
    "# Jacobian of g_active with respect to Θ\n",
    "dg_dΘ = jax.jacobian(g_active, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂Θ:\")\n",
    "print(dg_dΘ)\n",
    "\n",
    "# Jacobian of g_active with respect to p\n",
    "dg_dp = jax.jacobian(g_active, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂p:\")\n",
    "print(dg_dp)\n",
    "\n",
    "# Lagrangian\n",
    "dL_dΘ = -df_dΘ.T + np.matmul(dg_dΘ.T, λ_opt)\n",
    "print(\"\\n∇L:\")\n",
    "print(dL_dΘ)\n",
    "\n",
    "# Hessian of the objective\n",
    "d2f_dΘ2 = jax.jacobian(jax.jacobian(f, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²f (via jacobian of jacobian):\")\n",
    "print(d2f_dΘ2)\n",
    "\n",
    "d2f_dΘdp = jax.jacobian(jax.jacobian(f, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇f/∂p (via jacobian of jacobian):\")\n",
    "print(d2f_dΘdp)\n",
    "\n",
    "# Hessian of the constraints\n",
    "d2g_dΘ2 = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²g (via jacobian of jacobian):\")\n",
    "print(d2g_dΘ2)\n",
    "\n",
    "d2g_dΘdp = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇g/∂p (via jacobian of jacobian):\")\n",
    "print(d2g_dΘdp)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘ2 = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘdp = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "I_p = np.eye(4)\n",
    "print(\"\\nI_p:\")\n",
    "print(I_p)\n",
    "\n",
    "I_f = np.eye(1)\n",
    "print(\"\\nI_f:\")\n",
    "print(I_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def assemble_ude_matrix_scipy_sparse(nabla2_L, nabla_g, dg_dp, df_dtheta, df_dp, Np, Nx, Ng):\n",
    "    \"\"\"\n",
    "    Assemble the UDE matrix using SciPy sparse matrices.\n",
    "    This is more memory efficient for large problems.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert JAX arrays to numpy for SciPy\n",
    "    nabla2_L_np = np.array(nabla2_L).reshape((Nx, Nx))\n",
    "    nabla_g_np = np.array(nabla_g)\n",
    "    dg_dp_np = np.array(dg_dp)\n",
    "    df_dtheta_np = np.array(df_dtheta)\n",
    "    df_dp_np = np.array(df_dp)\n",
    "\n",
    "    # Create sparse blocks\n",
    "\n",
    "    # Row 1: [I_p, 0, 0, 0]\n",
    "    row1 = [\n",
    "        sp.eye(Np),                                    # I_p\n",
    "        sp.csr_matrix((Np, Nx)),                      # 0\n",
    "        sp.csr_matrix((Np, Ng)),                      # 0\n",
    "        sp.csr_matrix((Np, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 2: [∂∇L/∂p, ∇²L, ∇g^T, 0]\n",
    "    row2 = [\n",
    "        sp.csr_matrix((Nx, Np)),                      # ∂∇L/∂p (placeholder)\n",
    "        sp.csr_matrix(nabla2_L_np),                   # ∇²L\n",
    "        sp.csr_matrix(nabla_g_np.T),                  # ∇g^T\n",
    "        sp.csr_matrix((Nx, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 3: [∂g/∂p, ∇g, 0, 0]\n",
    "    row3 = [\n",
    "        sp.csr_matrix(dg_dp_np),                      # ∂g/∂p\n",
    "        sp.csr_matrix(nabla_g_np),                    # ∇g\n",
    "        sp.csr_matrix((Ng, Ng)),                      # 0\n",
    "        sp.csr_matrix((Ng, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 4: [-∂f/∂p, -∂f/∂θ, 0, I_f]\n",
    "    row4 = [\n",
    "        sp.csr_matrix(-df_dp_np),                     # -∂f/∂p\n",
    "        sp.csr_matrix(-df_dtheta_np),                 # -∂f/∂θ\n",
    "        sp.csr_matrix((1, Ng)),                       # 0\n",
    "        sp.eye(1)                                     # I_f\n",
    "    ]\n",
    "\n",
    "    # Assemble using bmat\n",
    "    partial_R_partial_u = sp.bmat([row1, row2, row3, row4], format='csr')\n",
    "\n",
    "    return partial_R_partial_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_R_partial_u = assemble_ude_matrix_scipy_sparse(d2L_dΘ2, dg_dΘ, dg_dp, df_dΘ, df_dp, Np=4, Nx=2, Ng=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -1  1  1  0]\n",
      " [ 0  0  0  0 -1 -2  1  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 6  3  1  0  0 -2  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(linewidth=1024, precision=1):\n",
    "    print(np.asarray(partial_R_partial_u.todense(), dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the sensitivities of the objective with respect to the parameters, the final row of $\\frac{d u}{d \\mathcal{R}}$, we transpose $\\frac{\\partial \\mathcal{R}}{\\partial u}$ and seed the right hand side with a 1 in the last row.\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{bmatrix}\n",
    "    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-2 &-1 & 1 & 1 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-1 &-2 & 1 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 &-1 & 1 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    6 & 3 & 1 & 0 & 0 &-2 & 0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  ^T\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d f^*}{d p_0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_2} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_3} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d f^*}\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    1\n",
    "  \\end{bmatrix}  \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 0, 0, 0, 1]]).T\n",
    "dfstar_du = spsolve(partial_R_partial_u.T, rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2., -0., -0.,  2., -2.,  1.])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstar_dp = dfstar_du[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2.])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "Recall that the optimal objective value $f^*$ was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.999999999999993"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_opt.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the sensitivities by perturbing each element in p and reoptimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_f_sensitivity(h=1.0E-8):\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity               {\"UDE Result\":20s}       {\"FD Result\":20s}          {\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        prob.set_val('p', p_nom + dp)\n",
    "        prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        prob.run_driver()\n",
    "        prob.set_val('p', p_nom)\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dfstar_dpi_fd = (prob.get_val('f') - f_opt) / h\n",
    "\n",
    "        print(f'   df*/dp_{p_idx}     {dfstar_dp[p_idx]:20.12f}      {dfstar_dpi_fd[0]:20.12f}      {dfstar_dp[p_idx]-dfstar_dpi_fd[p_idx]:20.12f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity               UDE Result                 FD Result                     Error               \n",
      "   df*/dp_0          -6.000000000000           -6.000000496442            0.000000496442\n",
      "   df*/dp_1          -4.000000000000           -3.999999975690           -0.000000024310\n",
      "   df*/dp_2          -1.000000000000           -1.000000082740            0.000000082740\n",
      "   df*/dp_3          -2.000000000000           -2.000000876023            0.000000876023\n"
     ]
    }
   ],
   "source": [
    "check_f_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the sensitivities of $\\theta^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "dthetastar0_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 1, 0, 0, 0]]).T\n",
    "dthetastar1_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "dthetastar_du = np.vstack((dthetastar0_du, dthetastar1_du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dthetastar_dp = dthetastar_du[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., -1.]], dtype=float64)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dthetastar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, at the optimum $\\theta_0$ is on its upper bound, so modifying this upper bound will necessarily change $\\theta_0$ since we assume the bound remains active.\n",
    "\n",
    "Since $\\theta_1$ is constrained to be equal and opposite to $\\theta_0$, the increase in $\\theta_0$ will result in a equal and opposite change in $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Θ_sensitivity(h=1.0E-8):\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity                                       '\n",
    "          f'{\"UDE Result\":20s}                      '\n",
    "          f'{\"FD Result\":20s}                         '\n",
    "          f'{\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        prob.set_val('p', p_nom + dp)\n",
    "        prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        prob.run_driver()\n",
    "        prob.set_val('p', p_nom)\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dthetastar_dpi_fd = (prob.get_val('Θ') - Θ_opt) / h\n",
    "\n",
    "        # print(prob.get_val('Θ'), Θ_opt, dthetastar_dpi_fd)\n",
    "\n",
    "        with np.printoptions(precision=4, formatter={'all':lambda x: f'{x:16.12f}'}):\n",
    "            print(f'   dΘ*/dp_{p_idx}              {dthetastar_dp[:, p_idx]}      {dthetastar_dpi_fd}      {dthetastar_dp[:, p_idx]-dthetastar_dpi_fd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity                                       UDE Result                                FD Result                                    Error               \n",
      "   dΘ*/dp_0              [  0.000000000000   0.000000000000]      [  0.000000000000  -0.000000177636]      [  0.000000000000   0.000000177636]\n",
      "   dΘ*/dp_1              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_2              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_3              [  1.000000000000  -1.000000000000]      [  0.999999993923  -1.000000171558]      [  0.000000006077   0.000000171558]\n"
     ]
    }
   ],
   "source": [
    "check_Θ_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A matrix-free approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running Optimization\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Optimization Results\n",
      "============================================================\n",
      "θ₀* = 6.000000\n",
      "θ₁* = -6.000000\n",
      "f* = -26.000000\n",
      "g₁ = 1.776357e-15 (should be ~0)\n",
      "g₂ = 0.000000\n",
      "Bound active: True\n",
      "\n",
      "============================================================\n",
      "Computing Lagrange Multipliers\n",
      "============================================================\n",
      "{'g1': {'indices': array([0]), 'active_bounds': array([0]), 'multipliers': array([-2.])}}\n",
      "{'theta0': {'indices': array([0]), 'active_bounds': array([1]), 'multipliers': array([2.])}}\n",
      "[-2.  2.]\n",
      "Lagrange multipliers: [ 2. -2.]\n",
      "[ 2. -2.]\n",
      "\n",
      "============================================================\n",
      "Computing Post-Optimality Sensitivities\n",
      "============================================================\n",
      "Solving UDE system (9x9)...\n",
      "Parameters: ['p0', 'p1', 'p2', 'theta0_ub']\n",
      "Design vars: ['theta0', 'theta1']\n",
      "Constraints: ['g1', 'g2']\n",
      "Outputs: ['f']\n",
      "  Solving for parameter p0...\n",
      "  Solving for parameter p1...\n",
      "  Solving for parameter p2...\n",
      "  Solving for parameter theta0_ub...\n",
      "  Solving for design var sensitivity...\n",
      "  Solving for design var sensitivity...\n",
      "  Solving for output sensitivity...\n",
      "\n",
      "============================================================\n",
      "Sensitivity Results\n",
      "============================================================\n",
      "\n",
      "Design Variable Sensitivities (dθ/dp):\n",
      "----------------------------------------\n",
      "θ₀:\n",
      "  dθ₀/dp₀ = +0.000000\n",
      "  dθ₀/dp₁ = +0.000000\n",
      "  dθ₀/dp₂ = +0.000000\n",
      "  dθ₀/dθ₀_ub = +1.000000\n",
      "θ₁:\n",
      "  dθ₁/dp₀ = +0.000000\n",
      "  dθ₁/dp₁ = +0.000000\n",
      "  dθ₁/dp₂ = +0.000000\n",
      "  dθ₁/dθ₀_ub = -1.000000\n",
      "\n",
      "Objective Function Sensitivity (df*/dp):\n",
      "----------------------------------------\n",
      "  df*/dp₀ = -6.000000\n",
      "  df*/dp₁ = -4.000000\n",
      "  df*/dp₂ = -1.000000\n",
      "  df*/dθ₀_ub = -2.000000\n",
      "\n",
      "Most sensitive parameter: p₀ (df*/dp₀ = -6.000000)\n",
      "\n",
      "============================================================\n",
      "Verification with Finite Differences\n",
      "============================================================\n",
      "df*/dp₀:\n",
      "  UDE:    -6.000000\n",
      "  FD:     -6.000000\n",
      "  Error:  2.27e-10\n",
      "df*/dp₁:\n",
      "  UDE:    -4.000000\n",
      "  FD:     -4.000000\n",
      "  Error:  5.07e-10\n",
      "df*/dp₂:\n",
      "  UDE:    -1.000000\n",
      "  FD:     -1.000000\n",
      "  Error:  3.79e-11\n",
      "df*/dθ₀_ub:\n",
      "  UDE:    -2.000000\n",
      "  FD:     -2.000000\n",
      "  Error:  1.57e-08\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Post-optimality sensitivity analysis using OpenMDAO with matrix-free UDE approach.\n",
    "This implementation uses Hessian-vector products to avoid explicitly forming the Hessian.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import openmdao.api as om\n",
    "from scipy.sparse.linalg import gmres, LinearOperator\n",
    "\n",
    "\n",
    "class OptimizationProblem(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Component that defines the optimization problem:\n",
    "    min f(θ₀, θ₁; p) = (θ₀ - p₀)² + θ₀θ₁ + (θ₁ + p₁)² - p₂\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        # Inputs\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta1', val=1.0)\n",
    "        self.add_input('p0', val=3.0)\n",
    "        self.add_input('p1', val=4.0)\n",
    "        self.add_input('p2', val=3.0)\n",
    "\n",
    "        # Outputs\n",
    "        self.add_output('f', val=1.0)\n",
    "\n",
    "        # Declare partials\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        theta0 = inputs['theta0']\n",
    "        theta1 = inputs['theta1']\n",
    "        p0 = inputs['p0']\n",
    "        p1 = inputs['p1']\n",
    "        p2 = inputs['p2']\n",
    "\n",
    "        outputs['f'] = (theta0 - p0)**2 + theta0*theta1 + (theta1 + p1)**2 - p2\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        theta0 = inputs['theta0']\n",
    "        theta1 = inputs['theta1']\n",
    "        p0 = inputs['p0']\n",
    "        p1 = inputs['p1']\n",
    "\n",
    "        # Derivatives of f\n",
    "        partials['f', 'theta0'] = 2*(theta0 - p0) + theta1\n",
    "        partials['f', 'theta1'] = theta0 + 2*(theta1 + p1)\n",
    "        partials['f', 'p0'] = -2*(theta0 - p0)\n",
    "        partials['f', 'p1'] = 2*(theta1 + p1)\n",
    "        partials['f', 'p2'] = -1.0\n",
    "\n",
    "\n",
    "class EqualityConstraint(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Equality constraint: θ₁ = -θ₀\n",
    "    Formulated as g₁ = θ₀ + θ₁ = 0\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta1', val=1.0)\n",
    "        self.add_output('g1', val=0.0)\n",
    "\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['g1'] = inputs['theta0'] + inputs['theta1']\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        partials['g1', 'theta0'] = 1.0\n",
    "        partials['g1', 'theta1'] = 1.0\n",
    "\n",
    "\n",
    "class BoundConstraint(om.ExplicitComponent):\n",
    "    \"\"\"\n",
    "    Upper bound constraint: θ₀ ≤ θ₀_ub\n",
    "    Formulated as g₂ = θ₀ - θ₀_ub ≤ 0\n",
    "    When active, treated as g₂ = θ₀ - θ₀_ub = 0\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('theta0', val=1.0)\n",
    "        self.add_input('theta0_ub', val=6.0)\n",
    "        self.add_output('g2', val=0.0)\n",
    "\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['g2'] = inputs['theta0'] - inputs['theta0_ub']\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        partials['g2', 'theta0'] = 1.0\n",
    "        partials['g2', 'theta0_ub'] = -1.0\n",
    "\n",
    "\n",
    "class SensitivitySolver:\n",
    "    \"\"\"\n",
    "    Matrix-free UDE solver for post-optimality sensitivity analysis.\n",
    "    Uses Hessian-vector products to avoid forming the full Hessian.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prob, design_vars, constraints, parameters, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the UDE solver.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        prob : OpenMDAO Problem\n",
    "            The optimized problem\n",
    "        design_vars : list\n",
    "            Names of design variables ['theta0', 'theta1']\n",
    "        constraints : list\n",
    "            Names of active constraints ['g1'] or ['g1', 'g2']\n",
    "        parameters : list\n",
    "            Names of parameters ['p0', 'p1', 'p2', 'theta0_ub']\n",
    "        outputs : list\n",
    "            Names of outputs to track ['f']\n",
    "        \"\"\"\n",
    "        self.prob = prob\n",
    "        self.design_vars = design_vars\n",
    "        self.constraints = constraints\n",
    "        self.parameters = parameters\n",
    "        self.outputs = outputs\n",
    "\n",
    "        self.n_theta = len(design_vars)\n",
    "        self.n_lambda = len(constraints)\n",
    "        self.n_p = len(parameters)\n",
    "        self.n_f = len(outputs)\n",
    "\n",
    "        self.total_size = self.n_p + self.n_theta + self.n_lambda + self.n_f\n",
    "\n",
    "        # Store the optimal point\n",
    "        self.x_opt = np.array([prob[var] for var in design_vars]).ravel()\n",
    "\n",
    "    def gradient_lagrangian(self, x_perturbed=None, recompute=False):\n",
    "        \"\"\"\n",
    "        Compute gradient of Lagrangian: ∇L = ∇f - Σλᵢ∇gᵢ\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_perturbed : array, optional\n",
    "            Perturbed design variables for finite difference\n",
    "        recompute : bool\n",
    "            If True, recompute totals even if x hasn't changed\n",
    "        \"\"\"\n",
    "        if x_perturbed is not None:\n",
    "            # Set perturbed values\n",
    "            for i, var in enumerate(self.design_vars):\n",
    "                self.prob[var] = x_perturbed[i]\n",
    "            self.prob.run_model()\n",
    "\n",
    "        # Compute gradients using OpenMDAO's compute_totals\n",
    "        # This gets us ∇f and ∇g efficiently\n",
    "        grad_f = -self.prob.compute_totals(\n",
    "            of=self.outputs[0],\n",
    "            wrt=self.design_vars,\n",
    "            return_format='array'\n",
    "        ).flatten()\n",
    "\n",
    "        grad_L = grad_f.copy()\n",
    "\n",
    "        # Subtract constraint contributions\n",
    "        for i, con in enumerate(self.constraints):\n",
    "            # Get multiplier value (would come from optimizer)\n",
    "            # For now, compute from KKT conditions\n",
    "            grad_g = self.prob.compute_totals(\n",
    "                of=con,\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "\n",
    "            # In practice, lambda values come from the optimizer\n",
    "            # Here we compute them from stationarity conditions\n",
    "            if hasattr(self, 'lambda_vals'):\n",
    "                grad_L += self.lambda_vals[i] * grad_g\n",
    "\n",
    "        return grad_L\n",
    "\n",
    "    def hessian_vector_product(self, v, eps=1e-7):\n",
    "        \"\"\"\n",
    "        Compute Hessian-vector product ∇²L · v using finite differences.\n",
    "\n",
    "        H·v ≈ (∇L(x + εv) - ∇L(x - εv)) / (2ε)\n",
    "        \"\"\"\n",
    "        # Save current state\n",
    "        x_current = self.x_opt.copy()\n",
    "\n",
    "        # Compute gradient at x + eps*v\n",
    "        x_plus = x_current + eps * v\n",
    "        grad_L_plus = self.gradient_lagrangian(x_plus)\n",
    "\n",
    "        # # Compute gradient at x - eps*v\n",
    "        # x_minus = x_current - eps * v\n",
    "        # grad_L_minus = self.gradient_lagrangian(x_minus)\n",
    "\n",
    "        # Restore original state\n",
    "        for i, var in enumerate(self.design_vars):\n",
    "            self.prob[var] = x_current[i]\n",
    "        self.prob.run_model()\n",
    "\n",
    "        # Central finite difference approximation\n",
    "        # return (grad_L_plus - grad_L_minus) / (2 * eps)\n",
    "        return (grad_L_plus) / (eps)\n",
    "\n",
    "    def constraint_jacobian(self):\n",
    "        \"\"\"\n",
    "        Get Jacobian of constraints ∇g.\n",
    "        \"\"\"\n",
    "        jac = []\n",
    "        for con in self.constraints:\n",
    "            grad_g = self.prob.compute_totals(\n",
    "                of=con,\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            jac.append(grad_g)\n",
    "        return np.array(jac)\n",
    "\n",
    "    def ude_matvec(self, vec):\n",
    "        \"\"\"\n",
    "        Matrix-vector product for the UDE system matrix.\n",
    "\n",
    "        The UDE matrix is:\n",
    "        [I_p      0        0         0  ]\n",
    "        [∂∇L/∂p   ∇²L      (∇g)ᵀ     0  ]\n",
    "        [∂g/∂p    ∇g       0         0  ]\n",
    "        [-∂f/∂p   -∂f/∂θ   0         I_f]\n",
    "        \"\"\"\n",
    "        # Extract blocks from input vector\n",
    "        idx = 0\n",
    "        v_p = vec[idx:idx+self.n_p]\n",
    "        idx += self.n_p\n",
    "        v_theta = vec[idx:idx+self.n_theta]\n",
    "        idx += self.n_theta\n",
    "        v_lambda = vec[idx:idx+self.n_lambda]\n",
    "        idx += self.n_lambda\n",
    "        v_f = vec[idx:idx+self.n_f]\n",
    "\n",
    "        # Initialize result\n",
    "        result = np.zeros(self.total_size)\n",
    "        idx = 0\n",
    "\n",
    "        # First block row: [I_p, 0, 0, 0] @ v\n",
    "        result[idx:idx+self.n_p] = v_p\n",
    "        idx += self.n_p\n",
    "\n",
    "        # Second block row: [∂∇L/∂p, ∇²L, (∇g)ᵀ, 0] @ v\n",
    "        row2 = np.zeros(self.n_theta)\n",
    "\n",
    "        # ∇²L @ v_theta using Hessian-vector product\n",
    "        if np.any(v_theta):\n",
    "            row2 += self.hessian_vector_product(v_theta)\n",
    "\n",
    "        # (∇g)ᵀ @ v_lambda\n",
    "        if np.any(v_lambda):\n",
    "            jac_g = self.constraint_jacobian()\n",
    "            row2 += jac_g.T @ v_lambda\n",
    "\n",
    "        # ∂∇L/∂p @ v_p (often small, we'll approximate as zero for now)\n",
    "        # In practice, this could be computed via finite differences\n",
    "\n",
    "        result[idx:idx+self.n_theta] = row2\n",
    "        idx += self.n_theta\n",
    "\n",
    "        # Third block row: [∂g/∂p, ∇g, 0, 0] @ v\n",
    "        row3 = np.zeros(self.n_lambda)\n",
    "\n",
    "        # ∇g @ v_theta\n",
    "        if np.any(v_theta):\n",
    "            jac_g = self.constraint_jacobian()\n",
    "            row3 += jac_g @ v_theta\n",
    "\n",
    "        # ∂g/∂p @ v_p\n",
    "        if np.any(v_p):\n",
    "            for i, con in enumerate(self.constraints):\n",
    "                grad_g_p = self.prob.compute_totals(\n",
    "                    of=con,\n",
    "                    wrt=self.parameters,\n",
    "                    return_format='array'\n",
    "                ).flatten()\n",
    "                row3[i] += grad_g_p @ v_p\n",
    "\n",
    "        result[idx:idx+self.n_lambda] = row3\n",
    "        idx += self.n_lambda\n",
    "\n",
    "        # Fourth block row: [-∂f/∂p, -∂f/∂θ, 0, I_f] @ v\n",
    "        row4 = v_f.copy()  # I_f @ v_f\n",
    "\n",
    "        if np.any(v_p):\n",
    "            df_dp = self.prob.compute_totals(\n",
    "                of=self.outputs[0],\n",
    "                wrt=self.parameters,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            row4 -= df_dp @ v_p\n",
    "\n",
    "        if np.any(v_theta):\n",
    "            df_dtheta = self.prob.compute_totals(\n",
    "                of=self.outputs[0],\n",
    "                wrt=self.design_vars,\n",
    "                return_format='array'\n",
    "            ).flatten()\n",
    "            row4 -= df_dtheta @ v_theta\n",
    "\n",
    "        result[idx:idx+self.n_f] = row4\n",
    "\n",
    "        return result\n",
    "\n",
    "    def solve_sensitivities(self, tol=1e-6, maxiter=1000):\n",
    "        \"\"\"\n",
    "        Solve for post-optimality sensitivities using GMRES.\n",
    "        \"\"\"\n",
    "        # Create linear operator\n",
    "        A_op = LinearOperator(\n",
    "            (self.total_size, self.total_size),\n",
    "            matvec=self.ude_matvec\n",
    "        )\n",
    "\n",
    "        # Storage for solutions\n",
    "        X = np.zeros((self.total_size, self.total_size))\n",
    "\n",
    "        print(f\"Solving UDE system ({self.total_size}x{self.total_size})...\")\n",
    "        print(f\"Parameters: {self.parameters}\")\n",
    "        print(f\"Design vars: {self.design_vars}\")\n",
    "        print(f\"Constraints: {self.constraints}\")\n",
    "        print(f\"Outputs: {self.outputs}\")\n",
    "\n",
    "        # Solve for each column of the identity matrix\n",
    "        col = 0\n",
    "\n",
    "        # Columns for I_p\n",
    "        for i in range(self.n_p):\n",
    "            print(f\"  Solving for parameter {self.parameters[i]}...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[i] = 1.0\n",
    "            sol, info = gmres(A_op, rhs, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "            if info != 0:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Columns for I_theta\n",
    "        for i in range(self.n_theta):\n",
    "            print(\"  Solving for design var sensitivity...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[self.n_p + i] = 1.0\n",
    "            sol, info = gmres(A_op, rhs, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "            if info != 0:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Columns for I_lambda\n",
    "        # for i in range(self.n_lambda):\n",
    "        #     print(f\"  Solving for multiplier sensitivity...\")\n",
    "        #     rhs = np.zeros(self.total_size)\n",
    "        #     rhs[self.n_p + self.n_theta + i] = 1.0\n",
    "        #     sol, info = gmres(A_op, rhs, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "        #     if info != 0:\n",
    "        #         print(f\"    Warning: GMRES info={info}\")\n",
    "        #     X[:, col] = sol\n",
    "        #     col += 1\n",
    "\n",
    "        # Columns for I_f\n",
    "        for i in range(self.n_f):\n",
    "            print(\"  Solving for output sensitivity...\")\n",
    "            rhs = np.zeros(self.total_size)\n",
    "            rhs[self.n_p + self.n_theta + self.n_lambda + i] = 1.0\n",
    "            sol, info = gmres(A_op, rhs, atol=tol, rtol=tol, maxiter=maxiter)\n",
    "            if info != 0:\n",
    "                print(f\"    Warning: GMRES info={info}\")\n",
    "            X[:, col] = sol\n",
    "            col += 1\n",
    "\n",
    "        # Extract sensitivities\n",
    "        idx_theta = self.n_p\n",
    "        idx_lambda = self.n_p + self.n_theta\n",
    "        idx_f = self.n_p + self.n_theta + self.n_lambda\n",
    "\n",
    "        sensitivities = {\n",
    "            'dtheta_dp': X[idx_theta:idx_lambda, :self.n_p],\n",
    "            # 'dlambda_dp': X[idx_lambda:idx_f, :self.n_p],\n",
    "            'df_dp': X[idx_f:, :self.n_p]\n",
    "        }\n",
    "\n",
    "        return sensitivities\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the UDE sensitivity analysis.\n",
    "    \"\"\"\n",
    "    # Create OpenMDAO problem\n",
    "    prob = om.Problem()\n",
    "\n",
    "    # Add subsystems\n",
    "    prob.model.add_subsystem('obj', OptimizationProblem(),\n",
    "                            promotes_inputs=['theta0', 'theta1', 'p0', 'p1', 'p2'],\n",
    "                            promotes_outputs=['f'])\n",
    "\n",
    "    prob.model.add_subsystem('eq_con', EqualityConstraint(),\n",
    "                            promotes_inputs=['theta0', 'theta1'],\n",
    "                            promotes_outputs=['g1'])\n",
    "\n",
    "    prob.model.add_subsystem('bound_con', BoundConstraint(),\n",
    "                            promotes_inputs=['theta0', 'theta0_ub'],\n",
    "                            promotes_outputs=['g2'])\n",
    "\n",
    "    # Add driver (optimizer)\n",
    "    prob.driver = om.ScipyOptimizeDriver()\n",
    "    prob.driver.options['optimizer'] = 'SLSQP'\n",
    "    prob.driver.options['tol'] = 1e-8\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    # Define design variables\n",
    "    prob.model.add_design_var('theta0', lower=-50, upper=6.0)\n",
    "    prob.model.add_design_var('theta1', lower=-50, upper=50)\n",
    "\n",
    "    # Define objective\n",
    "    prob.model.add_objective('f')\n",
    "\n",
    "    # Define constraints\n",
    "    prob.model.add_constraint('g1', equals=0.0)\n",
    "    # Note: upper bound is handled by the design variable bounds\n",
    "\n",
    "    # Setup the problem\n",
    "    prob.setup()\n",
    "\n",
    "    # Set parameter values\n",
    "    prob.set_val('p0', 3.0)\n",
    "    prob.set_val('p1', 4.0)\n",
    "    prob.set_val('p2', 3.0)\n",
    "    prob.set_val('theta0_ub', 6.0)\n",
    "\n",
    "    # Initial guess\n",
    "    prob.set_val('theta0', 1.0)\n",
    "    prob.set_val('theta1', 1.0)\n",
    "\n",
    "    # Run optimization\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Running Optimization\")\n",
    "    print(\"=\"*60)\n",
    "    prob.run_driver()\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Optimization Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"θ₀* = {prob.get_val('theta0')[0]:.6f}\")\n",
    "    print(f\"θ₁* = {prob.get_val('theta1')[0]:.6f}\")\n",
    "    print(f\"f* = {prob.get_val('f')[0]:.6f}\")\n",
    "    print(f\"g₁ = {prob.get_val('g1')[0]:.6e} (should be ~0)\")\n",
    "    print(f\"g₂ = {prob.get_val('g2')[0]:.6f}\")\n",
    "\n",
    "    # Check if bound is active\n",
    "    bound_active = abs(prob.get_val('theta0')[0] - 6.0) < 1e-6\n",
    "    print(f\"Bound active: {bound_active}\")\n",
    "\n",
    "    # Determine active constraints\n",
    "    active_constraints = ['g1']  # Equality always active\n",
    "    if bound_active:\n",
    "        active_constraints.append('g2')\n",
    "\n",
    "    # Compute Lagrange multipliers from KKT conditions\n",
    "    # This is a simplification - normally these come from the optimizer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Computing Lagrange Multipliers\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # # Get gradients at optimum\n",
    "    grad_f = prob.compute_totals(of='f', wrt=['theta0', 'theta1'], return_format='array').flatten()\n",
    "\n",
    "    # # Build constraint Jacobian\n",
    "    jac_g = []\n",
    "    for con in active_constraints:\n",
    "        grad_g = prob.compute_totals(of=con, wrt=['theta0', 'theta1'], return_format='array').flatten()\n",
    "        jac_g.append(grad_g)\n",
    "    jac_g = np.array(jac_g)\n",
    "\n",
    "    # # Solve for multipliers: ∇f = Σλᵢ∇gᵢ\n",
    "    # # Using least squares for overdetermined system\n",
    "    lambda_vals = np.linalg.lstsq(jac_g.T, grad_f, rcond=None)[0]\n",
    "    active_dvs, active_cons = prob.driver.compute_lagrange_multipliers()\n",
    "    print(active_cons)\n",
    "    print(active_dvs)\n",
    "    # The lagrange multipliers of the active constraints are\n",
    "    lambda_vals2 = []\n",
    "    for active_con in active_cons.values():\n",
    "        lambda_vals2.extend(active_con['multipliers'][active_con['indices']])\n",
    "    for active_dv in active_dvs.values():\n",
    "        lambda_vals2.extend(active_dv['multipliers'][active_dv['indices']])\n",
    "    # lambda_vals = np.array([active_dvs['Θ']['multipliers'][active_dvs['Θ']['indices']],\n",
    "    #             active_cons['g']['multipliers'][active_cons['g']['indices']]])\n",
    "    lambda_vals2 = np.array(lambda_vals2)\n",
    "    print(lambda_vals2)\n",
    "    print(f\"Lagrange multipliers: {lambda_vals}\")\n",
    "\n",
    "    print(lambda_vals)\n",
    "\n",
    "    # Create UDE solver\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Computing Post-Optimality Sensitivities\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    ude_solver = SensitivitySolver(\n",
    "        prob=prob,\n",
    "        design_vars=['theta0', 'theta1'],\n",
    "        constraints=active_constraints,\n",
    "        parameters=['p0', 'p1', 'p2', 'theta0_ub'],\n",
    "        outputs=['f']\n",
    "    )\n",
    "\n",
    "    # Set Lagrange multipliers\n",
    "    ude_solver.lambda_vals = lambda_vals\n",
    "\n",
    "    # Compute sensitivities\n",
    "    sensitivities = ude_solver.solve_sensitivities()\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Sensitivity Results\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    param_names = ['p₀', 'p₁', 'p₂', 'θ₀_ub']\n",
    "\n",
    "    print(\"\\nDesign Variable Sensitivities (dθ/dp):\")\n",
    "    print(\"-\" * 40)\n",
    "    dtheta_dp = sensitivities['dtheta_dp']\n",
    "    for i, var in enumerate(['θ₀', 'θ₁']):\n",
    "        print(f\"{var}:\")\n",
    "        for j, param in enumerate(param_names):\n",
    "            print(f\"  d{var}/d{param} = {dtheta_dp[i, j]:+.6f}\")\n",
    "\n",
    "    print(\"\\nObjective Function Sensitivity (df*/dp):\")\n",
    "    print(\"-\" * 40)\n",
    "    df_dp = sensitivities['df_dp'].flatten()\n",
    "    for j, param in enumerate(param_names):\n",
    "        print(f\"  df*/d{param} = {df_dp[j]:+.6f}\")\n",
    "\n",
    "    # Identify most sensitive parameter\n",
    "    max_idx = np.argmax(np.abs(df_dp))\n",
    "    print(f\"\\nMost sensitive parameter: {param_names[max_idx]} \"\n",
    "          f\"(df*/d{param_names[max_idx]} = {df_dp[max_idx]:+.6f})\")\n",
    "\n",
    "    # Verify sensitivities with finite differences\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Verification with Finite Differences\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    eps = 1e-5\n",
    "    # f_orig = prob.get_val('f')[0].item()\n",
    "    for j, param in enumerate(['p0', 'p1', 'p2', 'theta0_ub']):\n",
    "        # Save original value\n",
    "        orig_val = prob.get_val(param)[0]\n",
    "\n",
    "        # Perturb parameter\n",
    "        prob.set_val(param, orig_val + eps)\n",
    "        prob.set_val('theta0', 1.0)\n",
    "        prob.set_val('theta1', 1.0)\n",
    "\n",
    "        if param == 'theta0_ub':\n",
    "            prob.model.set_design_var_options('theta0', upper=6. + eps)\n",
    "\n",
    "        # Re-optimize\n",
    "        prob.run_driver()\n",
    "        f_plus = prob.get_val('f')[0]\n",
    "\n",
    "        # # Restore and perturb in other direction\n",
    "        prob.set_val(param, orig_val - eps)\n",
    "        if param == 'theta0_ub':\n",
    "            prob.model.set_design_var_options('theta0', upper=6. - eps)\n",
    "        prob.set_val('theta0', 1.0)\n",
    "        prob.set_val('theta1', 1.0)\n",
    "        prob.run_driver()\n",
    "        f_minus = prob.get_val('f')[0]\n",
    "\n",
    "        # Compute finite difference\n",
    "        fd_sensitivity = (f_plus - f_minus) / (2 * eps)\n",
    "        # fd_sensitivity = (f_plus - f_orig) / (eps)\n",
    "\n",
    "        # Restore original\n",
    "        prob.model.set_design_var_options('theta0', upper=6.)\n",
    "        prob.set_val(param, orig_val)\n",
    "\n",
    "        print(f\"df*/d{param_names[j]}:\")\n",
    "        print(f\"  UDE:    {df_dp[j]:+.6f}\")\n",
    "        print(f\"  FD:     {fd_sensitivity:+.6f}\")\n",
    "        print(f\"  Error:  {abs(df_dp[j] - fd_sensitivity):.2e}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
