{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode  # noqa: F401\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Post-Optimality Sensitivities of a Constrained Optimization Problem\n",
    "\n",
    "Lets consider a problem such that we have an active bound and an active inequality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "\\min_{\\theta_0,\\, \\theta_1} \\quad & f(\\theta_0, \\theta_1; \\mathbf{p}) = (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\\\\n",
    "\\text{where} \\quad \\mathbf{p} &= \\begin{bmatrix} 3 \\\\ 4 \\\\ 3 \\end{bmatrix} \\in \\mathbb{R}^3 \\\\\n",
    "\\text{bounds:} \\quad \\theta_0 &\\le 6 \\\\\n",
    "\\text{equality constraints:} \\quad \\theta_1 &= -\\theta_0\n",
    "\\end{align*}\n",
    "\n",
    "Then, if we want to know the sensitivity of the optimization to the value of $\\theta_0^{ub}$, as if is another parameter to the problem, we can assume $\\theta_0^{ub}$ is just another element in our parameter vector:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\bar{p} &= \\begin{bmatrix} p_0 \\\\ p_1 \\\\ p_2 \\\\ \\theta_0^{ub} \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "If active, we can treat the bound on $\\theta_0$ as just another equality constraint.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{\\mathcal{G}}(\\bar{\\theta}, \\bar{p}) &= \\begin{bmatrix}\n",
    "                                   \\theta_0 + \\theta_1 \\\\\n",
    "                                   \\theta_0 - p_3\n",
    "                                \\end{bmatrix} = \\bar 0\n",
    "\\end{align*}\n",
    "\n",
    "**How will my system design ($\\bar{\\theta}^*$) respond to changes in my assumptions and system inputs ($\\bar{p}$)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Universal Derivatives Equation\n",
    "\n",
    "The UDE is:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right] \\left[ \\frac{d u}{d \\mathcal{R}} \\right]\n",
    "  &=\n",
    "  \\left[ I \\right]\n",
    "  =\n",
    "  \\left[ \\frac{\\partial \\mathcal{R}}{\\partial \\mathcal{u}} \\right]^T \\left[ \\frac{d u}{d \\mathcal{R}} \\right]^T\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Here, the residuals are the primal and dual residuals of the optimization process, given above.\n",
    "\n",
    "There's a lot of nomenclature collisions, so let's define the following going forward.\n",
    "\n",
    "- Post-optimization, the unknowns ($\\bar{u}$) in our system are the resulting design variable values ($\\bar{\\theta}$).\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{u} &= \\bar{\\theta}\n",
    "\\end{align*}\n",
    "\n",
    "- The independent variables ($\\bar{x}$) of this system are those inputs for which we want to determine the sensitivity of the ouputs: the bounding values of the active constraints as well as any other parameters that are inputs to the model but not design variables controlled by the optimizer.\n",
    "\n",
    "\\begin{align*}\n",
    "  \\bar{x} &= \\bar{p}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the UDE to solving post-optimality sensitivities\n",
    "\n",
    "In our case, the unknowns vector consists of\n",
    "- the optimization parameters which includes the bounding values of active constraints ($\\bar{p}$)\n",
    "- the design variables of the optimization ($\\bar{\\theta}$)\n",
    "- the Lagrange multipliers of the optimization ($\\bar{\\lambda}$)\n",
    "- the objective value **as well as** any other outputs for which we want the sensitivities ($f$)\n",
    "\n",
    "The total size of the unknowns vector is $N_p + N_{\\theta} + N_{\\lambda} + N_{f}$\n",
    "\n",
    "\\begin{align*}\n",
    "  \\hat{u} &=\n",
    "  \\begin{bmatrix}\n",
    "    \\hat{p} \\\\\n",
    "    \\bar{\\theta} \\\\\n",
    "    \\bar{\\lambda} \\\\\n",
    "    \\bar{f}\n",
    "  \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Under the UDE, the corresponding residual equations for these unknowns are\n",
    "- the implicit form of the independent variable values\n",
    "- the stationarity condition\n",
    "- the active constraints\n",
    "- the implicit form of the explicit calculations of $f$ and $y$\n",
    "\n",
    "\\begin{align}\n",
    "\\bar{\\mathcal{R}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\bar{\\mathcal{R}}_p \\\\\n",
    "\\bar{\\mathcal{R}}_{\\theta} \\\\\n",
    "\\bar{\\mathcal{R}}_{g} \\\\\n",
    "\\bar{\\mathcal{R}}_{f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  \\bar{p} - \\bar{p} \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ -\\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{ab} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\lambda} - \\check{g}_{ab} \\left( \\bar{\\theta}, \\bar{p} \\right) \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  f - \\check{f}\\left(\\bar{\\theta}, \\bar{p} \\right) \n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "  p_0 - \\check{p}_0 \\\\[1.1ex]\n",
    "  p_1 - \\check{p}_1 \\\\[1.1ex]\n",
    "  p_2 - \\check{p}_2 \\\\[1.1ex]\n",
    "  p_3 - \\check{p}_3 \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  \\bar{r}_{\\theta} - \\left[ -\\nabla_{\\bar{\\theta}} \\check{f} (\\bar{\\theta}, \\bar{p}) + \\nabla_{\\bar{\\theta}} \\check{g}_{ab} (\\bar{\\theta}, \\bar{p})^T \\bar{\\lambda} \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  r_{\\lambda_0} - \\left[ \\theta_0 + \\theta_1 \\right] \\\\[1.1ex]\n",
    "  r_{\\lambda_1} - \\left[\\theta_0 - p_4 \\right] \\\\[1.1ex]\n",
    "  \\hline \\\\\n",
    "  f - \\left[ (\\theta_0 - p_0)^2 + \\theta_0 \\theta_1 + (\\theta_1 + p_1)^2 - p_2 \\right]\n",
    "\\end{bmatrix}\n",
    "&= \\bar 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the total derivatives that we seek ($\\frac{d f^*}{d \\bar{p}}$ and $\\frac{d \\bar{\\theta}^*}{d \\bar{p}}$), we need $\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}$.\n",
    "\n",
    "The optimizer has served as the nonlinear solver in this case which has computed the values in the unknowns vector: $\\bar{\\theta}$, $\\bar{\\lambda}$, $\\bar{g}$, and $f$ such that the residuals are satisfied.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}}}{\\partial \\bar{u}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_p}}{\\partial \\bar{p}} & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\theta}} & \\frac{\\partial \\bar{\\mathcal{R}_{\\theta}}}{\\partial \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_g}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_g}}{\\partial \\bar{\\theta}} & 0 & 0 \\\\[1.1ex]\n",
    "\\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{p}} & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial \\bar{\\theta}} & 0 & \\frac{\\partial \\bar{\\mathcal{R}_f}}{\\partial f}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check{\\mathcal{L}}}{d \\bar{p}} & \\frac{d \\nabla \\check{\\mathcal{L}}}{d \\bar{\\theta}} & \\frac{d \\check{\\mathcal{L}}}{d \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check g}{d \\bar{p}} & \\frac{d \\check g}{d \\bar{\\theta}} & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{d \\check f}{d \\bar{p}} & -\\frac{d \\check f}{d \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check{\\mathcal{L}}}{d \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check g}{d \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{d \\check f}{d \\bar{p}} & -\\frac{d \\check f}{d \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "This nomenclature can be a bit confusing.\n",
    "\n",
    "**The _partial_ derivatives of the post-optimality residuals are the _total_ derivatives of the analysis.**\n",
    "\n",
    "In this case of the stationarity residuals $\\mathcal{R}_{\\bar{\\theta}}$, which already include _total_ derivatives of the analysis for the objective and constraint gradients, second derivatives are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding total derivaties which we need to solve for are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d \\bar{u}}{d \\bar{\\mathcal{R}}}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{p}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d f}{d \\bar{\\mathcal{R}_p}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d f}{d \\bar{\\mathcal{R}_f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\theta}} & \\frac{d \\bar{p}}{d \\bar{\\lambda}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\theta}} & \\frac{d \\bar{\\theta}}{d \\bar{\\lambda}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\theta}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\lambda}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\lambda}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\theta}} & \\frac{d f}{d \\bar{\\lambda}} & \\frac{d \\bar{f}}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\mathcal{R}_f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\theta}}} & \\frac{d f}{d \\bar{\\mathcal{R}_{\\lambda}}} & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\theta}} & \\frac{d \\bar{\\theta}}{d \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\theta}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\lambda}} & 0 \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\theta}} & \\frac{d f}{d \\bar{\\lambda}} & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we can solve them with four linear solves of the forward system, or three solves of the reverse system.\n",
    "\n",
    "TODO: Need to explain how du/dRf becomes du/df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDE for this problem, in forward form, is\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check{\\mathcal{L}}}{d \\bar{p}} & \\nabla^2 \\check{\\mathcal{L}} & \\nabla \\check g ^T & 0 \\\\[1.1ex]\n",
    "    \\frac{d \\check g}{d \\bar{p}} & \\nabla \\check g & 0 & 0 \\\\[1.1ex]\n",
    "    -\\frac{d \\check f}{d \\bar{p}} & -\\frac{d \\check f}{d \\bar{\\theta}} & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{d \\bar{p}}{d \\bar{p}} & \\frac{d \\bar{p}}{d \\bar{\\theta}} & \\frac{d \\bar{p}}{d \\bar{\\lambda}} & \\frac{d \\bar{p}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d \\bar{\\theta}}{d \\bar{p}}} & \\frac{d \\bar{\\theta}}{d \\bar{\\theta}} & \\frac{d \\bar{\\theta}}{d \\bar{\\lambda}} & \\frac{d \\bar{\\theta}}{d \\bar{f}} \\\\[1.1ex]\n",
    "\\frac{d \\bar{\\lambda}}{d \\bar{p}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\theta}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\lambda}} & \\frac{d \\bar{\\lambda}}{d \\bar{\\lambda}} \\\\[1.1ex]\n",
    "\\mathbf{\\frac{d f}{d \\bar{p}}} & \\frac{d f}{d \\bar{\\theta}} & \\frac{d f}{d \\bar{\\lambda}} & \\frac{d \\bar{f}}{d \\bar{f}}\n",
    "\\end{bmatrix}\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "    \\left[ I_p \\right] & 0 & 0 & 0 \\\\[1.1ex]\n",
    "    0 & \\left[ I_\\theta \\right] & 0 & 0 \\\\[1.1ex]\n",
    "    0 & 0 & \\left[ I_\\lambda \\right] & 0 \\\\[1.1ex]\n",
    "    0 & 0 & 0 & \\left[ I_f \\right]\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "The sensitivities of the objective and the design variable values with respect to the parameters of the optimization are highlighted.\n",
    "\n",
    "In this case, we have four parameters and thus four columns for which we need to solve the system.\n",
    "Alternatively, we have three rows of interest in this system...two for the design variables $\\theta_0$ and $\\theta_1$ and one for the objective $f$. Taking the transpose and solving this system using the reverse form would require three linear system solves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working through the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets use OpenMDAO to find the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -25.999999999999993\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization Complete\n",
      "-----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Problem: problem14\n",
       "Driver:  ScipyOptimizeDriver\n",
       "  success     : True\n",
       "  iterations  : 3\n",
       "  runtime     : 1.2067E-01 s\n",
       "  model_evals : 3\n",
       "  model_time  : 2.2514E-02 s\n",
       "  deriv_evals : 2\n",
       "  deriv_time  : 9.4140E-02 s\n",
       "  exit_status : SUCCESS"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import openmdao.api as om\n",
    "\n",
    "\n",
    "class ObjComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('f', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "        return np.array([f])\n",
    "\n",
    "class ConComp(om.JaxExplicitComponent):\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_input('Θ', shape=(2,))\n",
    "        self.add_input('p', shape=(4,))\n",
    "        self.add_output('g', shape=(1,))\n",
    "\n",
    "    def compute_primal(self, Θ, p):\n",
    "        g = Θ[0] + Θ[1]\n",
    "        return np.array([g])\n",
    "\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model.add_subsystem('f_comp', ObjComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "prob.model.add_subsystem('g_comp', ConComp(), promotes_inputs=['*'], promotes_outputs=['*'])\n",
    "\n",
    "prob.model.add_design_var('Θ', upper=[6., None])\n",
    "prob.model.add_constraint('g', equals=0.)\n",
    "prob.model.add_objective('f')\n",
    "\n",
    "prob.driver = om.ScipyOptimizeDriver()\n",
    "\n",
    "prob.setup()\n",
    "\n",
    "prob.set_val('p', [3, 4, 3, 6])\n",
    "\n",
    "prob.run_driver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Variables(s) in 'model'\n",
      "\n",
      "varname  val                  io      prom_name\n",
      "-------  -------------------  ------  ---------\n",
      "f_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  f      [-26.]               output  f        \n",
      "g_comp\n",
      "  Θ      |8.48528137|         input   Θ        \n",
      "         val:\n",
      "         array([ 6., -6.])\n",
      "  p      |8.36660027|         input   p        \n",
      "         val:\n",
      "         array([3., 4., 3., 6.])\n",
      "  g      [1.77635684e-15]     output  g        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob.model.list_vars(print_arrays=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_dvs, active_cons = prob.driver.compute_lagrange_multipliers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets form the UDE system and compute the sensitivities, outside of OpenMDAO first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert OpenMDAO values to jax arrays\n",
    "\n",
    "f_opt = np.array(prob.get_val('f'))\n",
    "Θ_opt = np.array(prob.get_val('Θ'))\n",
    "p = np.array(prob.get_val('p'))\n",
    "\n",
    "# The lagrange multipliers of the active constraints are\n",
    "λ_opt = np.array([active_dvs['Θ']['multipliers'][active_dvs['Θ']['indices']],\n",
    "              active_cons['g']['multipliers'][active_cons['g']['indices']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our objective and active constraint (and bounds) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(Θ, p):\n",
    "    f = (Θ[0] - p[0])**2 + Θ[0] * Θ[1] + (Θ[1] + p[1])**2 - p[2]\n",
    "    return np.array([f])\n",
    "\n",
    "def g_active(Θ, p):\n",
    "    return np.array([Θ[0] + Θ[1],\n",
    "                     Θ[0] - p[3]])\n",
    "\n",
    "# def df_dΘ(Θ, p):\n",
    "#     df_dΘ = [[2 * (Θ[0] - p[0]) + Θ[1]],\n",
    "#              [Θ[0] + 2 * (Θ[1] + p[1])]]\n",
    "#     return np.array(df_dΘ)\n",
    "\n",
    "# def df_dp(Θ, p):\n",
    "#     df_dp = [[-2 * (Θ[0] - p[0])],\n",
    "#              [2 * (Θ[1] + p[1])],\n",
    "#              [-1],\n",
    "#              [0]]\n",
    "#     return np.array(df_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Θ*:\n",
      "[ 6. -6.]\n",
      "\n",
      "λ*:\n",
      "[[ 2.]\n",
      " [-2.]]\n",
      "\n",
      "∂f/∂Θ:\n",
      "[[1.77635684e-15 2.00000000e+00]]\n",
      "\n",
      "∂f/∂p:\n",
      "[[-6. -4. -1.  0.]]\n",
      "\n",
      "∂g/∂Θ:\n",
      "[[1. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "∂g/∂p:\n",
      "[[ 0.  0.  0. -0.]\n",
      " [ 0.  0.  0. -1.]]\n",
      "\n",
      "∇L:\n",
      "[[-3.10862447e-15]\n",
      " [-1.77635684e-15]]\n",
      "\n",
      "∇²f (via jacobian of jacobian):\n",
      "[[[2. 1.]\n",
      "  [1. 2.]]]\n",
      "\n",
      "∂∇f/∂p (via jacobian of jacobian):\n",
      "[[[-2.  0.  0.  0.]\n",
      "  [ 0.  2.  0.  0.]]]\n",
      "\n",
      "∇²g (via jacobian of jacobian):\n",
      "[[[0. 0.]\n",
      "  [0. 0.]]\n",
      "\n",
      " [[0. 0.]\n",
      "  [0. 0.]]]\n",
      "\n",
      "∂∇g/∂p (via jacobian of jacobian):\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "∇²L:\n",
      "[[[-2.]\n",
      "  [-1.]]\n",
      "\n",
      " [[-1.]\n",
      "  [-2.]]]\n",
      "\n",
      "I_p:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "I_f:\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# The design vars, from OpenMDAO\n",
    "print('\\nΘ*:')\n",
    "print(Θ_opt)\n",
    "\n",
    "# The Lagrange multipliers, from OpenMDAO\n",
    "print(\"\\nλ*:\")\n",
    "print(λ_opt)\n",
    "\n",
    "# Jacobian of f with respect to Θ\n",
    "df_dΘ = jax.jacobian(f, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂Θ:\")\n",
    "print(df_dΘ)\n",
    "\n",
    "# Jacobian of f with respect to p\n",
    "df_dp = jax.jacobian(f, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂f/∂p:\")\n",
    "print(df_dp)\n",
    "\n",
    "# Jacobian of g_active with respect to Θ\n",
    "dg_dΘ = jax.jacobian(g_active, argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂Θ:\")\n",
    "print(dg_dΘ)\n",
    "\n",
    "# Jacobian of g_active with respect to p\n",
    "dg_dp = jax.jacobian(g_active, argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂g/∂p:\")\n",
    "print(dg_dp)\n",
    "\n",
    "# Lagrangian\n",
    "dL_dΘ = -df_dΘ.T + np.matmul(dg_dΘ.T, λ_opt)\n",
    "print(\"\\n∇L:\")\n",
    "print(dL_dΘ)\n",
    "\n",
    "# Hessian of the objective\n",
    "d2f_dΘ2 = jax.jacobian(jax.jacobian(f, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²f (via jacobian of jacobian):\")\n",
    "print(d2f_dΘ2)\n",
    "\n",
    "d2f_dΘdp = jax.jacobian(jax.jacobian(f, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇f/∂p (via jacobian of jacobian):\")\n",
    "print(d2f_dΘdp)\n",
    "\n",
    "# Hessian of the constraints\n",
    "d2g_dΘ2 = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=0)(Θ_opt, p)\n",
    "print(\"\\n∇²g (via jacobian of jacobian):\")\n",
    "print(d2g_dΘ2)\n",
    "\n",
    "d2g_dΘdp = jax.jacobian(jax.jacobian(g_active, argnums=0), argnums=1)(Θ_opt, p)\n",
    "print(\"\\n∂∇g/∂p (via jacobian of jacobian):\")\n",
    "print(d2g_dΘdp)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘ2 = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "# # Hessian of the lagrangian\n",
    "d2L_dΘdp = -d2f_dΘ2.T + np.dot(d2g_dΘ2.T, λ_opt)\n",
    "print(\"\\n∇²L:\")\n",
    "print(d2L_dΘ2)\n",
    "\n",
    "I_p = np.eye(4)\n",
    "print(\"\\nI_p:\")\n",
    "print(I_p)\n",
    "\n",
    "I_f = np.eye(1)\n",
    "print(\"\\nI_f:\")\n",
    "print(I_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def assemble_ude_matrix_scipy_sparse(nabla2_L, nabla_g, dg_dp, df_dtheta, df_dp, Np, Nx, Ng):\n",
    "    \"\"\"\n",
    "    Assemble the UDE matrix using SciPy sparse matrices.\n",
    "    This is more memory efficient for large problems.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert JAX arrays to numpy for SciPy\n",
    "    nabla2_L_np = np.array(nabla2_L).reshape((Nx, Nx))\n",
    "    nabla_g_np = np.array(nabla_g)\n",
    "    dg_dp_np = np.array(dg_dp)\n",
    "    df_dtheta_np = np.array(df_dtheta)\n",
    "    df_dp_np = np.array(df_dp)\n",
    "\n",
    "    # Create sparse blocks\n",
    "\n",
    "    # Row 1: [I_p, 0, 0, 0]\n",
    "    row1 = [\n",
    "        sp.eye(Np),                                    # I_p\n",
    "        sp.csr_matrix((Np, Nx)),                      # 0\n",
    "        sp.csr_matrix((Np, Ng)),                      # 0\n",
    "        sp.csr_matrix((Np, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 2: [∂∇L/∂p, ∇²L, ∇g^T, 0]\n",
    "    row2 = [\n",
    "        sp.csr_matrix((Nx, Np)),                      # ∂∇L/∂p (placeholder)\n",
    "        sp.csr_matrix(nabla2_L_np),                   # ∇²L\n",
    "        sp.csr_matrix(nabla_g_np.T),                  # ∇g^T\n",
    "        sp.csr_matrix((Nx, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 3: [∂g/∂p, ∇g, 0, 0]\n",
    "    row3 = [\n",
    "        sp.csr_matrix(dg_dp_np),                      # ∂g/∂p\n",
    "        sp.csr_matrix(nabla_g_np),                    # ∇g\n",
    "        sp.csr_matrix((Ng, Ng)),                      # 0\n",
    "        sp.csr_matrix((Ng, 1))                        # 0\n",
    "    ]\n",
    "\n",
    "    # Row 4: [-∂f/∂p, -∂f/∂θ, 0, I_f]\n",
    "    row4 = [\n",
    "        sp.csr_matrix(-df_dp_np),                     # -∂f/∂p\n",
    "        sp.csr_matrix(-df_dtheta_np),                 # -∂f/∂θ\n",
    "        sp.csr_matrix((1, Ng)),                       # 0\n",
    "        sp.eye(1)                                     # I_f\n",
    "    ]\n",
    "\n",
    "    # Assemble using bmat\n",
    "    partial_R_partial_u = sp.bmat([row1, row2, row3, row4], format='csr')\n",
    "\n",
    "    return partial_R_partial_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_R_partial_u = assemble_ude_matrix_scipy_sparse(d2L_dΘ2, dg_dΘ, dg_dp, df_dΘ, df_dp, Np=4, Nx=2, Ng=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0 -2 -1  1  1  0]\n",
      " [ 0  0  0  0 -1 -2  1  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0 -1  1  0  0  0  0]\n",
      " [ 6  3  1  0  0 -2  0  0  1]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(linewidth=1024, precision=1):\n",
    "    print(np.asarray(partial_R_partial_u.todense(), dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the sensitivities of the objective with respect to the parameters, the final row of $\\frac{d u}{d \\mathcal{R}}$, we transpose $\\frac{\\partial \\mathcal{R}}{\\partial u}$ and seed the right hand side with a 1 in the last row.\n",
    "\n",
    "\\begin{align}\n",
    "  \\begin{bmatrix}\n",
    "    1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-2 &-1 & 1 & 1 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 &-1 &-2 & 1 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    0 & 0 & 0 &-1 & 1 & 0 & 0 & 0 & 0 \\\\[1.5ex]\n",
    "    6 & 3 & 1 & 0 & 0 &-2 & 0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  ^T\n",
    "  \\begin{bmatrix}\n",
    "    \\frac{d f^*}{d p_0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_2} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d p_3} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\theta 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 0} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d r_\\lambda 1} \\\\[1.3ex]\n",
    "    \\frac{d f^*}{d f^*}\n",
    "  \\end{bmatrix}\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    0 \\\\[1.5ex]\n",
    "    1\n",
    "  \\end{bmatrix}  \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import spsolve\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 0, 0, 0, 1]]).T\n",
    "dfstar_du = spsolve(partial_R_partial_u.T, rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2., -0., -0.,  2., -2.,  1.])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstar_dp = dfstar_du[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6., -4., -1., -2.])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfstar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results\n",
    "\n",
    "Recall that the optimal objective value $f^*$ was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.999999999999993"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_opt.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the sensitivities by perturbing each element in p and reoptimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_f_sensitivity(h=1.0E-8):\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity               {\"UDE Result\":20s}       {\"FD Result\":20s}          {\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        prob.set_val('p', p_nom + dp)\n",
    "        prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        prob.run_driver()\n",
    "        prob.set_val('p', p_nom)\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dfstar_dpi_fd = (prob.get_val('f') - f_opt) / h\n",
    "\n",
    "        print(f'   df*/dp_{p_idx}     {dfstar_dp[p_idx]:20.12f}      {dfstar_dpi_fd[0]:20.12f}      {dfstar_dp[p_idx]-dfstar_dpi_fd[p_idx]:20.12f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity               UDE Result                 FD Result                     Error               \n",
      "   df*/dp_0          -6.000000000000           -6.000000496442            0.000000496442\n",
      "   df*/dp_1          -4.000000000000           -3.999999975690           -0.000000024310\n",
      "   df*/dp_2          -1.000000000000           -1.000000082740            0.000000082740\n",
      "   df*/dp_3          -2.000000000000           -2.000000876023            0.000000876023\n"
     ]
    }
   ],
   "source": [
    "check_f_sensitivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the sensitivities of $\\theta^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 1, 0, 0, 0, 0]]).T\n",
    "dthetastar0_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "rhs = sp.csr_matrix([[0, 0, 0, 0, 0, 1, 0, 0, 0]]).T\n",
    "dthetastar1_du = spsolve(partial_R_partial_u.T, rhs)\n",
    "dthetastar_du = np.vstack((dthetastar0_du, dthetastar1_du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "dthetastar_dp = dthetastar_du[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0., -1.]], dtype=float64)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dthetastar_dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, at the optimum $\\theta_0$ is on its upper bound, so modifying this upper bound will necessarily change $\\theta_0$ since we assume the bound remains active.\n",
    "\n",
    "Since $\\theta_1$ is constrained to be equal and opposite to $\\theta_0$, the increase in $\\theta_0$ will result in a equal and opposite change in $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Θ_sensitivity(h=1.0E-8):\n",
    "    prob.driver.options['disp'] = False\n",
    "\n",
    "    print(f'Sensitivity                                       '\n",
    "          f'{\"UDE Result\":20s}                      '\n",
    "          f'{\"FD Result\":20s}                         '\n",
    "          f'{\"Error\":20s}')\n",
    "\n",
    "    for p_idx in range(4):\n",
    "        p_nom = np.array([3, 4, 3, 6])\n",
    "        ub_nom = np.array([6., 1.0E16])\n",
    "\n",
    "        dp = np.zeros(4)\n",
    "        dp = dp.at[p_idx].set(h)\n",
    "\n",
    "        dub = np.zeros(2)\n",
    "        if p_idx == 3:\n",
    "            # To test p3 we need to change the upper bound on θ\n",
    "            dub = dub.at[0].set(h)\n",
    "\n",
    "        prob.set_val('p', p_nom + dp)\n",
    "        prob.set_val('Θ', [1, 1]) # Start away from the optimum\n",
    "\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom + dub)\n",
    "\n",
    "        prob.run_driver()\n",
    "        prob.set_val('p', p_nom)\n",
    "        prob.model.set_design_var_options('Θ', upper=ub_nom)\n",
    "\n",
    "        dthetastar_dpi_fd = (prob.get_val('Θ') - Θ_opt) / h\n",
    "\n",
    "        # print(prob.get_val('Θ'), Θ_opt, dthetastar_dpi_fd)\n",
    "\n",
    "        with np.printoptions(precision=4, formatter={'all':lambda x: f'{x:16.12f}'}):\n",
    "            print(f'   dΘ*/dp_{p_idx}              {dthetastar_dp[:, p_idx]}      {dthetastar_dpi_fd}      {dthetastar_dp[:, p_idx]-dthetastar_dpi_fd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity                                       UDE Result                                FD Result                                    Error               \n",
      "   dΘ*/dp_0              [  0.000000000000   0.000000000000]      [  0.000000000000  -0.000000177636]      [  0.000000000000   0.000000177636]\n",
      "   dΘ*/dp_1              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_2              [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]      [  0.000000000000   0.000000000000]\n",
      "   dΘ*/dp_3              [  1.000000000000  -1.000000000000]      [  0.999999993923  -1.000000171558]      [  0.000000006077   0.000000171558]\n"
     ]
    }
   ],
   "source": [
    "check_Θ_sensitivity()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
